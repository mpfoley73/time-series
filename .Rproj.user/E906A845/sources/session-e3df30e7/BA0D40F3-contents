# Exponential Smoothing (ETS) {#exponential}

```{r include=FALSE}
library(tidyverse)
library(tsibble)
library(feasts) # feature extraction and statistics
library(fable) # forecasting
# library(patchwork) # arranging plots
# library(flextable)
# library(kableExtra)

knitr::opts_chunk$set(
  ft.align="left"
)

flextable::set_flextable_defaults(
  # font.family = "Arial", 
  font.size = 10, 
  big.mark = ",",
  theme_fun = "theme_vanilla"
)

ggplot2::theme_set(
  theme_light() +
    theme(
      strip.background = element_rect(fill = "gray90", color = "gray60"),
      strip.text = element_text(color = "gray20", margin = margin(5, 5, 5, 5))
    )
)
```

Exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get more remote. Exponential smoothing is a family of methods that vary by their *trend* and *seasonal* components.

```{r echo=FALSE}
tibble(
  `Trend Component` = c("None (N)", "Additive (A)", "Additive Damped (Ad)"),
  `None (N)` = c("(N, N)\nSimple Exponential Smoothing",
                 "(A, N)\nHoltâ€™s Linear", 
                 "(Ad, N)\nAdditive damped trend"),
  `Additive (A)` = c("(N, A)", 
                     "(A, A)\nAdditive Holt-Winters", 
                     "(Ad, A)"),
  `Multiplicative (M)` = c("(N, M)", 
                           "(A, M)\nMultiplicative Holt-Winters", 
                           "(Ad, M)\nHolt-Winters Damped")
) %>%
  flextable::flextable() %>%
  flextable::add_header_row(values = c("", "Seasonal Component"), colwidths = c(1, 3)) %>%
  flextable::merge_h(part = "header") %>%
  flextable::border(j = 1, border.right = officer::fp_border(color = "gray80"), part = "all") %>%
  flextable::align(i = 1, part = "header") %>%
  flextable::valign(valign = "top") %>%
  flextable::set_caption("Exponential smoothing taxonomy.") %>%
  flextable::autofit()
```

* There can be no trend (N), an additive (A) linear trend from the forecast horizon, or a damped additive (A<sub>d</sub>) trend leveling off from the forecast horizon.^[The trend can also be multiplicative (M) or multiplicative damped (M<sub>d</sub>), but [Hyndman](https://otexts.com/fpp3/taxonomy.html) explains that they do not produce good forecasts.]

* There can be no seasonality (N), or it can be additive (A) or multiplicative (M).

The trend and seasonal combinations produce 3 x 3 = 9 possible exponential smoothing methods. ETS (Error, Trend, and Seasonality) models double the number of possible *state space models* to 18 by treating the error variances as either additive (A) or multiplicative (M). ETS models do not just extend the exponential smoothing models; they also estimate their parameters differently, using maximum likelihood estimation. For models with additive errors, this is equivalent to minimizing the sum of squared errors (SSE). The great advantage of using ETS models is that you can optimize the parameter settings by minimizing the Akaike Information Criterion (AIC<sub>c</sub>).

`fable::ETS()` fits ETS models:

```{r eval=FALSE}
ETS(y ~ error(c("A", "M")) + trend(c("N", "A", "Ad")) + season(c("N", "A", "M"))
```

## Simple Exponential Smoothing (SES)

Simple exponential smoothing models (SES) have no seasonal or trend components. They are of the form $\hat{y}_{T+h|T} = \alpha(1-\alpha)^0 y_{T-0} + \alpha(1-\alpha)^1y_{T-1} + \alpha(1-\alpha)^2y_{T-2} \dots$ where $0 < \alpha < 1$ is a weighting parameter. On the one extreme, $\alpha$ = 1 is the same as a naive model. On the other extreme $\alpha \approx$ 0 is the average model. 

ETS models are commonly expressed in component form as a recursive model. The component form of SES is

$$
\begin{align}
\hat{y}_{t+h|t} &= l_t \\
l_t &= \alpha y_t + (1 - \alpha)l_{t-1}
\end{align}
$$

The first component, $\hat{y}_{t+h|t}$, is the forecast. It equals the last value of the estimated level. The second component, $l_t$, is the level (or smoothed value) of the series at time $t$. It describes how the level changes over time, kind of like a slope. ETS uses nonlinear optimization to estimate two parameters for SES, $\alpha$ and $l_0$.

#### Example {-}

Data set `tsibbledata::global_economy` contains annual country-level economic indicators, including `Exports`. This time series has no trend or seasonality, so it is a good candidate for SES.

```{r}
tsibbledata::global_economy %>%
  filter(Country == "Algeria") %>%
  ggplot(aes(x = Year, y = Exports)) +
  geom_line() +
  labs(title = "Algerian Exports (% of GDP)")
```

`fable::ETS()` is the exponential smoothing function. With additive errors, this is an ETS(A, N, N) model.

```{r}
ses_fit <- tsibbledata::global_economy %>%
  filter(Country == "Algeria") %>%
  model(ETS(Exports ~ error("A") + trend("N") + season("N")))

ses_fit %>% report()
```

Exports were $\hat{l}_0$ = `r ses_fit %>% tidy() %>% filter(term == "l[0]") %>% pull(estimate) %>% scales::number(accuracy = .1)`% of GDP at period 0 (1960). $\hat{\alpha}$ = `r ses_fit %>% tidy() %>% filter(term == "alpha") %>% pull(estimate) %>% scales::number(accuracy = .001)`, a high weight on recent values. Check the model assumptions with residuals plots. 

```{r}
gg_tsresiduals(ses_fit)
```

Residual heteroscedasticity compromises prediction intervals. The innovation residuals vs time plot shows no heteroscedasticity.^[Innovation residuals are residuals on the transformed scale if the outcome variable was transformed.] It does show a potential outlier around 1962 which might be a concern. Autocorrelation increases prediction intervals. The autocorrelation function plot shows a barely significant negative spike at lag 12 years. Non-normal residuals also compromise prediction intervals. The residual distribution in the histogram is slightly left-skewed. 

Use the fitted model to forecast the response variable for five periods. `hilo()` attaches a prediction interval to the tsibble.

```{r}
ses_fc <- ses_fit %>%
  forecast(h = 5) %>%
  hilo(80) %>%
  mutate(
    lpi = map_dbl(`80%`, ~.$lower),
    upi = map_dbl(`80%`, ~.$upper)
  )

ses_fit %>%
  augment() %>%
  ggplot(aes(x = Year)) +
  geom_line(aes(y = Exports)) +
  geom_line(aes(y = .fitted), color = "goldenrod") +
  geom_line(data = ses_fc, aes(y = .mean), color = "goldenrod") +
  geom_ribbon(data = ses_fc, aes(ymin = lpi, ymax = upi), alpha = 0.2, fill = "goldenrod") +
  labs(title = "Simple Exponential Smoothing, ETS(A, N, N)")
```

## Holt Linear

Holt's linear method extends SES with a trend component, $b_t$.

$$
\begin{align}
\hat{y}_{t+h|t} &= l_t + hb_t \\
l_t &= \alpha y_t + (1 - \alpha)(l_{t-1} + hb_{t-1}) \\
b_t &= \beta^*(l_t - l_{t-1}) + (1 - \beta^*)b_{t-1}
\end{align}
$$

The level equation, $l_t$, is like SES except for a trend adjustment. The trend equation, $b_t$, describes how the slope changes over time. The parameter $\beta^*$ describes how quickly the slope can change. Now there are four parameter to estimate, $\alpha$, $l_0$, $\beta^*$, and $b_0$.

#### Example {-}

Data set `tsibbledata::global_economy` contains annual country-level economic indicators, including `Population` size. This time series has a trend, so it is a good candidate for Holt's linear trend method.

```{r}
tsibbledata::global_economy %>%
  filter(Country == "Australia") %>%
  ggplot(aes(x = Year, y = Population)) +
  geom_line() +
  labs(title = "Australian Population.")
```

Fit the model with `ETS()` specifying an additive trend, ETS(A, A, N).

```{r}
holt_fit <- tsibbledata::global_economy %>%
  filter(Country == "Australia") %>%
  model(ETS(Population ~ error("A") + trend("A") + season("N")))

holt_fit %>% report()
```

$\hat{l}_0$ = `r holt_fit %>% tidy() %>% filter(term == "l[0]") %>% pull(estimate) %>% scales::comma(1)` people at period 0 (1960). $\alpha$ is high when the trend increases rapidly, assigning more weight to recent values. $\hat{\alpha}$ = `r holt_fit %>% tidy() %>% filter(term == "alpha") %>% pull(estimate) %>% scales::number(accuracy = .0001)`, a very high weighting. Population initially grows at $\beta_0$ = `r holt_fit %>% tidy() %>% filter(term == "b[0]") %>% pull(estimate) %>% scales::comma()` people per year. $\hat{\beta}$ = `r holt_fit %>% tidy() %>% filter(term == "beta") %>% pull(estimate) %>% scales::number(accuracy = .001)`, a fairly large value, meaning the trend changes often. 

Check the model assumptions with residuals plots. There is no heteroscedasticity in the residuals vs time plot and no skew in the residual distribution plot, so the prediction intervals are reliable. The autocorrelation plot has no significant spikes, so the prediction intervals will not be unduly large.

```{r}
gg_tsresiduals(holt_fit)
```

Use the fitted model to forecast the response variable for ten periods.

```{r}
holt_fc <- holt_fit %>%
  forecast(h = 10) %>%
  hilo(80) %>%
  mutate(
    lpi = map_dbl(`80%`, ~.$lower),
    upi = map_dbl(`80%`, ~.$upper)
  )

holt_fit %>%
  augment() %>%
  ggplot(aes(x = Year)) +
  geom_line(aes(y = Population)) +
  geom_line(aes(y = .fitted), color = "goldenrod") +
  geom_line(data = holt_fc, aes(y = .mean), color = "goldenrod") +
  geom_ribbon(data = holt_fc, aes(ymin = lpi, ymax = upi), alpha = 0.2, fill = "goldenrod") +
  labs(title = "Holt's Linear Method, ETS(A, A, N)")
```

## Additive Damped Trend

Holt's linear trend produces a sloped, but straight, line. Research shows that constant trends tend to overshoot. The additive damped trend model introduces a damping parameter, $\phi$, to reduce the forecasted trend to a flat line over time. The forecast equation replaces $h$ with the series $\phi^1 + \phi^2 + \cdots + \phi^h$. The trend equation adds $\phi$ as a multiplier to the second term.

$$
\begin{align}
\hat{y}_{t+h|t} &= l_t + (\phi^1 + \phi^2 + \cdots + \phi^h)b_t \\
l_t &= \alpha y_t + (1 - \alpha)(l_{t-1} + \phi b_{t-1}) \\
b_t &= \beta^*(l_t - l_{t-1}) + (1 - \beta^*) \phi b_{t-1}
\end{align}
$$

Now there are five parameters to estimate, $\alpha$, $\beta^*$, $l_0$, $b_0$, and $\phi$ (although you can supply a $\phi$ value to the `trend()` equation. Expect $\phi$ to between .8 and .998. 

#### Example {-}

Return to the Australian population data and include an additive damped trend model in a fit. The new model is an ETS(A, A<sub>d</sub>, N). 

```{r}
dholt_fit <- tsibbledata::global_economy %>%
  filter(Country == "Australia") %>%
  model(
    `Holt's Linear` = ETS(Population ~ error("A") + trend("A") + season("N")),
    `Damped Holt's Linear` = ETS(Population ~ error("A") + trend("Ad") + season("N"))
  )

# Just report on the new model.
dholt_fit %>% select(`Damped Holt's Linear`) %>% report()
```

$\hat{\phi}$ = `r dholt_fit %>% tidy() %>% filter(term == "phi") %>% pull(estimate) %>% scales::number(accuracy = .001)`, a modest amount of damping. $\hat{\beta}$ increased from `r holt_fit %>% tidy() %>% filter(term == "beta") %>% pull(estimate) %>% scales::number(accuracy = .001)` in the linear model to `r dholt_fit %>% tidy() %>% filter(.model == "Damped Holt's Linear", term == "beta") %>% pull(estimate) %>% scales::number(accuracy = .001)` here, so the slope is changing more frequently.

```{r}
dholt_fc <- dholt_fit %>%
  forecast(h = 10) %>%
  hilo(80) %>%
  mutate(
    lpi = map_dbl(`80%`, ~.$lower),
    upi = map_dbl(`80%`, ~.$upper)
  )

palette_dholt <- c(`Holt's Linear` = "goldenrod", `Damped Holt's Linear` = "seagreen")

dholt_fit %>%
  augment() %>%
  ggplot(aes(x = Year)) +
  geom_line(aes(y = Population)) +
  geom_line(aes(y = .fitted, color = .model)) +
  geom_line(data = dholt_fc, aes(y = .mean, color = .model)) +
  geom_ribbon(data = dholt_fc, aes(ymin = lpi, ymax = upi, color = .model, fill = .model), 
              alpha = 0.2) +
  scale_fill_manual(values = palette_dholt) +
  scale_color_manual(values = palette_dholt) +
  theme(legend.position = "top") +
  labs(color = NULL, fill = NULL, 
       title = "Holt's Linear Method, ETS(A, A, N), and Additive Damped, ETS(A, Ad, N)")
```

## Holt-Winters

The Holt-Winters method extends Holt's method with a seasonality component, $s_t$, for $m$ seasons per period. There are two versions of this model, the *additive* and the *multiplicative*. The additive method assumes the error variance is constant, and the seasonal component sums to approximately zero over the course of the year. The multiplicative version assumes the error variance scales with the level, and the seasonal component sums to approximately $m$ over the course of the year. 

**Additive Holt-Winters** introduces the seasonality component as an additive element. 

$$
\begin{align}
\hat{y}_{t+h|t} &= l_t + hb_t + s_{t+h-m(k+1)} \\
l_t &= \alpha(y_t - s_{t-m}) + (1 - \alpha)(l_{t-1} + b_{t-1}) \\
b_t &= \beta^*(l_t - l_{t-1}) + (1 - \beta^*)b_{t-1} \\
s_t &= \gamma(y_t - l_{t-1} - b_{t-1}) + (1 - \gamma)s_{t-m}
\end{align}
$$

$k$ is the modulus of $(h - 1) / m$, so $s_{t+h-m(k+1)}$ is always based on the prior seasonal period. $l_t$ is a weighted average ($\alpha$ weighting) between the seasonally adjusted observation and the non-seasonal forecast. The trend component is unchanged. The seasonal component is a weighted average ($\gamma$ weighting) between the current seasonal index and the same season of the prior season period.  

The seasonality averages to one in **multiplicative Holt-Winters**. Use the multiplicative method if the seasonal variation increases with the level.

$$
\begin{align}
\hat{y}_{t+h|t} &= (l_t + hb_t) s_{t+h-m(k+1)} \\
l_t &= \alpha\frac{y_t}{s_{t-m}} + (1 - \alpha)(l_{t-1} + b_{t-1}) \\
b_t &= \beta^*(l_t - l_{t-1}) + (1-\beta*)b_{t-1} \\
s_t &= \gamma\frac{y_t}{(l_{t-1} - b_{t-1})} + (1 - \gamma)s_{t-m} \\
\end{align}
$$

Now there are five smoothing parameters to estimate: $\alpha$, $l_0$, $\beta^*$, $b_0$, and $\gamma$, plus an initial value for each season of the seasonal period.

#### Example {-}

Data set `tsibble::tourism` contains quarterly domestic tourist visit-nights in Australia. It's not clear whether the error variance increases with the series level, so either the additive or the multiplicative method may be appropriate. 

```{r}
tsibble::tourism %>%
  filter(Purpose == "Holiday") %>%
  summarize(Trips = sum(Trips) / 1000) %>%
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  labs(title = "Australian Domestic Tourist Visit-Nights")
```

Fit the model with `ETS()` specifying additive and multiplicative seasonality and error, ETS(A, A, A) and ETS(M, A, M).

```{r}
hw_fit <- tsibble::tourism %>%
  filter(Purpose == "Holiday") %>%
  summarize(Trips = sum(Trips) / 1000) %>%
  model(
    `Additive Holt-Winters` = ETS(Trips ~ error("A") + trend("A") + season("A")),
    `Multiplicative Holt-Winters` = ETS(Trips ~ error("M") + trend("A") + season("M"))
  )

hw_fit %>% tidy() %>% pivot_wider(names_from = .model, values_from = estimate)
```

Notice that the seasonal component estimates, `s[0]` to `s[-3]`, sum to ~0 for the additive model and ~4 for the multiplicative model, the number of seasons in the seasonal period. The small $\hat{\gamma}$ values mean the seasonal component hardly changes over time. The small $\hat{\beta}$ values mean the slope component hardly changes over time. You can see this more clearly in the fit decomposition (note the differing vertical scales for the slope and level components).

```{r warning=FALSE}
hw_fit %>% 
  components() %>%
  pivot_longer(cols = Trips:remainder) %>%
  mutate(name = factor(name, levels = c("Trips", "level", "slope", "season", "remainder"))) %>%
  ggplot(aes(x = Quarter, y = value)) +
  geom_line() +
  facet_grid(rows = vars(name), cols = vars(.model), scales = "free_y") +
  labs(x = NULL, y = NULL, title = "Holt-Winters Fit Decomposition")
```

The model fit metrics suggest Multiplicative Holt-Winters is the better model. It has lower AIC and MSE.

```{r}
hw_fit %>% glance()
```

Check the model assumptions of the multiplicative model. The residuals plot shows some heteroscedasticity in the middle periods, and higher variance the latter years. The histogram shows a normal distribution. The autocorrelation function (ACF) plot shows a single significant spike at t14. The Ljung-Box test fails to reject the null hypothesis of no autocorrelation of the residuals (*p* = 0.504).

```{r}
# Can't use an object containing multiple fits, so re-fit just the multiplicative model.
hwm_fit <- tsibble::tourism %>%
  filter(Purpose == "Holiday") %>%
  summarize(Trips = sum(Trips) / 1000) %>%
  model(ETS(Trips ~ error("M") + trend("A") + season("M")))

hwm_fit %>% 
  gg_tsresiduals()
```
```{r}
hwm_fit %>%
  augment() %>% 
  features(.var = .innov, features = ljung_box, lag = 14)
```

Use the fitted model to forecast the response variable for ten periods.

```{r}
hw_fc <- hw_fit %>%
  forecast(h = 10) %>%
  hilo(80) %>%
  mutate(
    lpi = map_dbl(`80%`, ~.$lower),
    upi = map_dbl(`80%`, ~.$upper)
  )

palette_hw <- c(`Additive Holt-Winters` = "goldenrod", 
                `Multiplicative Holt-Winters` = "seagreen")

hw_fit %>%
  augment() %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Trips)) +
  geom_line(aes(y = .fitted, color = .model)) +
  geom_line(data = hw_fc, aes(y = .mean, color = .model)) +
  geom_ribbon(data = hw_fc, aes(ymin = lpi, ymax = upi, color = .model, fill = .model), 
              alpha = 0.2) +
  scale_fill_manual(values = palette_hw) +
  scale_color_manual(values = palette_hw) +
  theme(legend.position = "top") +
  labs(color = NULL, fill = NULL, 
       title = "Holt Winters Additive, ETS(A, A, A), and Multiplicative, ETS(M, A, M)")
```

## Auto-fitting

If you specify an ETS model with no parameters, it will use maximum likelihood to select the model with the minimum AIC<sub>c</sub>. 

```{r}
auto_fit <- tsibble::tourism %>%
  filter(Purpose == "Holiday") %>%
  summarize(Trips = sum(Trips) / 1000) %>%
  model(ETS(Trips))

auto_fit %>% report()
```

ETS chose a multiplicative error, non-trended, additive error model. Compare this model fit with the two Holt-Winters fits from the previous section. The autofit had the largest MSE, but lowest AICc.

```{r}
bind_rows(
  hw_fit %>% glance(),
  auto_fit %>% glance()
)
```

