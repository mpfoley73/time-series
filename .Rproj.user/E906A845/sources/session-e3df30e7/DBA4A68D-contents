# Toolbox {#toolbox}

```{r include=FALSE}
library(tidyverse)
library(tidymodels)
library(tsibble)   # extends tibble to time-series data.
library(feasts)    # feature extraction and statistics
library(fable)     # forecasting tsibbles.
library(modeltime) # new time series forecasting framework.
```

This section covers fundamental concepts in time series analysis. I will keep three paradigms in mind as I build models: i) `ts`, `zoo`, and `xts` objects used in the `forecast` base R framework, ii) `tsibbles` used in the `fables` tidyverse framework, and iii) `tibbles` used in the `timetk` tidymodeling framework.

## Common Frameworks

Let's work with the `fpp3::us_employment` data of US monthly employment data. There is one row per month (969 months) and data series (~150 series).

```{r message=FALSE, collapse=TRUE}
us_employment_tibble <- fpp3::us_employment %>% as_tibble() %>% mutate(Month = ym(Month))

glimpse(us_employment_tibble)
```

**ts** is the base R time series package. The `ts` object is essentially a matrix of observations indexed by a chronological identifier. Because it is a matrix, any descriptive attributes need to enter as numeric, perhaps by one-hot encoding, or pivoting the data (yuck). A `ts` can only have *one row per time observation*, and it assumes the time series is regularly spaced. Define a `ts` object with `ts(x, start, frequency)` where `frequency` is the number of observations in the seasonal pattern: 7 for daily observations with a week cycle; 5 for weekday observations in a week cycle; 24 for hourly in a day cycle, 24x7 for hourly in a week cycle, etc. `us_employment_tibble` is monthly observations starting with Jan 1939. Had it series started in Feb, you would specify `start = c(1939, 2)`. I'll filter the data to comply with the one row per time observation rule.

```{r collapse=TRUE}
us_employment_ts <- 
  us_employment_tibble %>% 
  filter(Title == "Total Private") %>%
  arrange(Month) %>%
  select(Employed) %>%
  ts(start = c(1939, 1), frequency = 12)

head(us_employment_ts)
```

**zoo** (Zeileis's ordered observations) has functions similar to those in **ts**, but also supports irregular time series. A `zoo` object contains an array of data values and an index attribute to provide information about the data ordering. **zoo** was introduced in 2014.

**xts** (extensible time series) extends **zoo**. `xts` objects are more flexible than `ts` objects while imposing reasonable constraints to make them truly time-based. An `xts` object is essentially a matrix of observations indexed by a time *object*. Create an `xts` object with `xts(x, order.by)` where `order.by` is a vector of dates/times to index the data. You can also add metadata to the `xts` object by declaring name-value pairs such as `Title` below.

```{r collapse=TRUE, warning=FALSE, message=FALSE}
library(xts)

x <- 
  us_employment_tibble %>%
  filter(Title == "Total Private") %>%
  arrange(Month)

us_employment_xts <- xts(x$Employed, order.by = x$Month, Title = x$Title)

str(us_employment_xts)
```

A `tsibble`, from the package of the same name, is a time-series `tibble`. Unlike the `ts`, `zoo`, and `xts` objects, a `tsibble` preserves the time index, making heterogeneous data structures possible. For example, you can re-index a `tsibble` from monthly to yearly analysis, or include one or more features per time element. 

A `tsibble` object is a tibble uniquely defined by `key` columns plus a date `index` column. This structure accommodates multiple series, and descriptive attribute columns. The date index can be a `Date`, `period`, etc. Express weekly time series with `yearweek()`, monthly time series with `yearmonth()`, or quarterly with `yearquarter()`. 

```{r}
us_employment_tsibble <- us_employment_tibble %>% 
  mutate(Month = yearmonth(Month)) %>%
  tsibble(key = c(Title), index = Month)

us_employment_tsibble %>% filter(Title == "Total Private")
```

A `tsibble` behaves like a `tibble`, so you can use *tidyverse** verbs. The only thing that will trip you up is that `tsibble` objects are grouped by the index, so `group_by()` operations implicitly include the index. Use `index_by()` if you need to summarize at a new time level. 

```{r collapse=TRUE, eval=FALSE}
# Group by Title implicitly includes the Month index column.
us_employment_tsibble %>% group_by(Title) %>% summarize(S = sum(Employed, na.rm = T))

# But you can change the index aggregation level with index_by()
us_employment_tsibble %>%
  index_by(Q = ~ quarter(.)) %>%
  group_by(Title) %>%
  summarise(S = sum(Employed, na.rm = T))

# Add group_by_key() to retain the key
us_employment_tsibble %>% 
  group_by_key() %>% 
  index_by(Q = ~ quarter(.)) %>%
  summarize(S = sum(Employed, na.rm = T))
```

## Fitting Models

If you are fitting an explanatory model, the workflow will be fitting, verifying assumptions, then summarizing  the parameters. If your are fitting a predictive model, your workflow will be comparing models with cross-validation using a hold-out data set, then making predictions.

Let's continue working with the `us_employment` data set and use just the Total Private series. We'll split the data into training and testing for predictive modeling.

```{r}
us_employment_full <- us_employment_tsibble %>% 
  filter(Title == "Total Private", year(Month) >= 2000)

us_employment_train <- us_employment_full %>% filter(year(Month) <= 2015)

us_employment_test <- us_employment_full %>% filter(year(Month) > 2015)
```

Fit a *naive* model (projection of last value) to Google's `Close` from 2015, then predict values from Jan 2016. We'll create an 80:20 train-test split (`test` should be ~20% or at least as long as the anticipated forecast.).

```{r}
train_fit <- 
  us_employment_train %>%
  model(
    Mean = MEAN(Employed),
    Naive = NAIVE(Employed),
    SNaive = SNAIVE(Employed),
    Drift = RW(Employed ~ drift())
  )

train_fcast <-
  train_fit %>%
  forecast(new_data = us_employment_test) %>%
  # Make an 80% CI from the distribution. 
  mutate(
    mu = map_dbl(Employed, ~unlist(.) %>% .["mu"]),
    sigma = map_dbl(Employed, ~unlist(.) %>% .["sigma"]),
    lcl = qnorm(.10, mu, sigma),
    ucl = qnorm(.90, mu, sigma)
  ) %>% 
  as_tibble()
```

The `autoplot()` and `autolayer()` functions take a lot of the headache out of plotting the results, especially since `forecast()` tucks away the confidence intervals in a `distribution` list object.

```{r}
bind_rows(
  us_employment_train, 
  us_employment_test 
) %>%
  as_tibble() %>%
  ggplot(aes(x = Month)) +
  geom_line(aes(y = Employed), color = "black", size = 1) +
  geom_line(data = train_fcast, aes(y = mu, color = .model), size = 1) +
  geom_ribbon(data = train_fcast, 
              aes(ymin = lcl, ymax = ucl, fill = .model, color = .model), 
              alpha = .2) +
  labs(title = "Forecast with 80% CI", x = NULL, y = NULL)
```

## Evaluating Fit

Evaluate the model fit with residuals diagnostics.^[Residuals and errors are *not* the same thing. The *residual* is the difference between the observed and fitted value in the *training* data set. The *error* is the difference between the observed and fitted value in the *test* data set.] `broom::augment()` adds three columns to the model cols: `.fitted`, `.resid`, and `.innov`. `.innov` is the residual from the transformed data (if no transformation, it just equals `.resid`).

Innovation residuals should be independent random variables normally distributed with mean zero and constant variance (the normality and variance conditions are only required for inference and prediction intervals). Happily, `feasts` has just what you need.

```{r warning=FALSE}
train_fit %>% 
  select("Naive") %>% 
  gg_tsresiduals() +
  labs(title = "Residuals Analysis")
```

The autocorrelation plot violates the independence assumption. The histogram plot tests normality. The residuals plot tests mean zero and constant variance. You can carry out a  *portmanteau* test on the autocorrelation assumption. Two common tests are the Box-Pierce and the Ljung-Box. These tests check the likelihood of a combination of autocorrelations at once, without testing any one correlation, kind of like an ANOVA test. The Ljung-Box test statistic is a sum of squared $k$-lagged autocorrelations, $r_k^2$,

$$Q^* = T(T+2) \sum_{k=1}^l(T-k)^{-1}r_k^2.$$

The test statistic has a $\chi^2$ distribution with $l - K$ degrees of freedom (where $K$ is the number of parameters in the model). Use $l = 10$ for non-seasonal data and $l = 2m$ for seasonal data. If your model has no explanatory variables, $K = 0.$ Reject the no-autocorrelation (i.e., white noise) assumption if *p* < .05.

```{r}
train_fit %>% 
  broom::augment() %>% 
  features(.var = .innov, features = ljung_box, lag = 10, dof = 0)
```

## Evaluating Accuracy

Some forecasting methods are extremely simple and surprisingly effective. The **mean** method projects the historical average, $\hat{y}_{T+h|T} = \bar{y}.$ The **naive** method projects the last observation, $\hat{y}_{T+h|T} = y_T.$ The **seasonal naive** method projects the last seasonal observation, $\hat{y}_{T+h|T} = y_{T+h-m(k+1)}.$ The **drift** method projects the straight line from the first and last observation, $\hat{y}_{T+h|T} = y_T + h\left(\frac{y_T - y_1}{T-1}\right).$

Evaluate the forecast accuracy with the test data (aka, "hold-out set", and "out-of-sample data"). The **forecast error** is the difference between the observed and forecast value, $e_{T+h} = y_{T+h} - \hat{y}_{t+h|T}.$ Forecast errors differ from model residuals in that they come from the test data set and because forecast values are usually multi-step forecasts which include prior forecast values as inputs.

The major accuracy benchmarks are: 

* **MAE**.  Mean absolute error, $mean(|e_t|)$
* **RMSE**.  Root mean squared error, $\sqrt{mean(e_t^2)}$
* **MAPE**.  Mean absolute percentage error, $mean(|e_t / y_t|) \times 100$
* **MASE**.  Mean absolute scaled error, $MAE/Q$ where $Q$ is a scaling constant calculated as the average one-period change in the outcome variable (error from a one-step naive forecast).

The MAE and RMSE are on the same scale as the data, so they are only useful for comparing models fitted to the same series. MAPE is unitless, but does not work for $y_t = 0$, and it assumes a meaningful zero (ratio data). MASE is most useful for comparing data sets of different units. 

Use `accuracy()` to evaluate a model.

```{r}
train_fit %>% 
  forecast(new_data = us_employment_test) %>%
  fabletools::accuracy(data = us_employment_test) %>%
  select(.model, RMSE, MAE, MAPE, MASE)
```

Time series *cross-validation* is a better way to evaluating a model. It breaks the dataset into multiple training sets by setting a cutoff at varying points and setting the test set to a single step ahead of the horizon. Function `stretch_tsibble()` creates a tsibble of initial size `.init` and appends additional data sets of increasing size `.step`.

```{r warning=FALSE}
us_employment_full %>%
  stretch_tsibble(.init = 3, .step = 1) %>%
  # Fit a model for each key
  model(
    Mean = MEAN(Employed),
    Naive = NAIVE(Employed),
    SNaive = SNAIVE(Employed),
    Drift = RW(Employed ~ drift())
  ) %>%
  forecast(h = 12) %>%
  fabletools::accuracy(data = us_employment_test)
```
