[["index.html", "Time Series Analysis Preface", " Time Series Analysis Michael Foley 2023-09-08 Preface These notes are based on the Time Series with R skill track at DataCamp and Rob Hyndman’s Forecasting: Principles and Practice (Rob J Hyndman 2021). I organized them into a section on working with a tsibble (time series tibble) (chapter 1), a section on data exploration (chapter 2), and then four sections on models. Forecasts aren’t necessarily based on time series models - you can perform a cross-sectional regression analysis of features, possibly including time-related features such as month of year (chapter 3). Time series forecasts are a specific type of forecast based, at least in part, on the assumption that future outcomes are functionally dependent upon prior outcomes. In most cases the objective of a time series forecast is to project a time series. In these cases, the forecast either decomposes a time series into trend and seasonality components (exponential smoothing models, chapter 4) or describes the autocorrelation within the data (ARIMA models, chapter 5). There may also be cases where you include other predictor variables (dynamic models, chapter 6). In addition to the standard packages, these notes use the tsibble, feasts, fable, and tsibbledata packages. library(tidyverse) library(patchwork) # arranging plots library(glue) library(tsibble) library(feasts) # feature extraction and statistics library(fable) # forecasting library(tsibbledata) References "],["basics.html", "Chapter 1 Basics 1.1 Common Frameworks 1.2 Fitting Models 1.3 Evaluating Fit 1.4 Model Selection", " Chapter 1 Basics This section covers fundamental concepts in time series analysis. There seems to be three modeling paradigms: i) the base R framework using native ts, zoo, and xts objects to model with the forecast package, ii) the tidyverts framework using tsibble objects to model with the fable package, and iii) the tidymodels framework using tibble objects with the timetk package. The base R framework is clunky, so I avoid it. The tidymodels seems to be geared toward machine-learning workflows which are still unfamiliar to me. So most of these notes use tidyverts. library(tidyverse) library(tsibble) # extends tibble to time-series data. library(feasts) # feature extraction and statistics library(fable) # forecasting tsibbles. library(tidymodels) library(modeltime) # new time series forecasting framework. 1.1 Common Frameworks Let’s review those three frameworks briefly. I’ll work with the fpp3::us_employment data of US monthly employment data. There is one row per month and series (225 months x 150 series). us_employment_tibble &lt;- fpp3::us_employment %&gt;% as_tibble() %&gt;% mutate(Month = ym(Month)) %&gt;% filter(year(Month) &gt;= 2001) glimpse(us_employment_tibble) ## Rows: 33,300 ## Columns: 4 ## $ Month &lt;date&gt; 2001-01-01, 2001-02-01, 2001-03-01, 2001-04-01, 2001-05-01,… ## $ Series_ID &lt;chr&gt; &quot;CEU0500000001&quot;, &quot;CEU0500000001&quot;, &quot;CEU0500000001&quot;, &quot;CEU05000… ## $ Title &lt;chr&gt; &quot;Total Private&quot;, &quot;Total Private&quot;, &quot;Total Private&quot;, &quot;Total Pr… ## $ Employed &lt;dbl&gt; 109928, 110140, 110603, 110962, 111624, 112303, 111897, 1118… Base R ts is the base R time series package. The ts object is essentially a matrix of observations indexed by a chronological identifier. Because it is a matrix, any descriptive attributes need to enter as numeric, perhaps by one-hot encoding. A ts can only have one row per time observation, and the time series must be regularly spaced (no data gaps). Define a ts object with ts(x, start, frequency) where frequency is the number of observations in the seasonal pattern: 7 for daily observations with a week cycle; 5 for weekday observations in a week cycle; 24 for hourly in a day cycle, 24x7 for hourly in a week cycle, etc. us_employment_tibble is monthly observations starting with Jan 1939. Had the series started in Feb, you would specify start = c(1939, 2). I filtered the data to the Total Private series to get one row per time observation. us_employment_ts &lt;- us_employment_tibble %&gt;% filter(Title == &quot;Total Private&quot;) %&gt;% arrange(Month) %&gt;% select(Employed) %&gt;% ts(start = c(1939, 1), frequency = 12) glimpse(us_employment_ts) ## Time-Series [1:225, 1] from 1939 to 1958: 109928 110140 110603 110962 111624 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;Employed&quot; zoo and xts zoo (Zeileis’s ordered observations) has functions similar to those in ts, but also supports irregular time series. A zoo object contains an array of data values and an index attribute to provide information about the data ordering. zoo was introduced in 2014. xts (extensible time series) extends zoo. xts objects are more flexible than ts objects while imposing reasonable constraints to make them truly time-based. An xts object is essentially a matrix of observations indexed by a time object. Create an xts object with xts(x, order.by) where order.by is a vector of dates/times to index the data. You can also add metadata to the xts object by declaring name-value pairs such as Title below. library(xts) us_employment_xts &lt;- us_employment_tibble %&gt;% filter(Title == &quot;Total Private&quot;) %&gt;% arrange(Month) %&gt;% xts(.$Employed, order.by = .$Month, Title = .$Title) glimpse(us_employment_xts) ## An xts object on 2001-01-01 / 2019-09-01 containing: ## Data: character [225, 4] ## Columns: Month, Series_ID, Title, Employed ## Index: Date [225] (TZ: &quot;UTC&quot;) ## xts Attributes: ## $ Title: chr [1:225] &quot;Total Private&quot; &quot;Total Private&quot; &quot;Total Private&quot; &quot;Total Private&quot; ... tidyverts A tsibble, from the package of the same name, is a time-series tibble. Unlike the ts, zoo, and xts objects, a tsibble preserves the time index, making heterogeneous data structures possible. For example, you can re-index a tsibble from monthly to yearly analysis, or include one or more features per time element and fit a linear regression. A tsibble object is a tibble uniquely defined by key columns plus a date index column. This structure accommodates multiple series, and attribute columns. The date index can be a Date, period, etc. Express weekly time series with yearweek(), monthly time series with yearmonth(), or quarterly with yearquarter(). us_employment_tsibble &lt;- us_employment_tibble %&gt;% mutate(Month = yearmonth(Month)) %&gt;% tsibble(key = c(Title), index = Month) us_employment_tsibble %&gt;% filter(Title == &quot;Total Private&quot;) %&gt;% glimpse() ## Rows: 225 ## Columns: 4 ## Key: Title [1] ## $ Month &lt;mth&gt; 2001 Jan, 2001 Feb, 2001 Mar, 2001 Apr, 2001 May, 2001 Jun, … ## $ Series_ID &lt;chr&gt; &quot;CEU0500000001&quot;, &quot;CEU0500000001&quot;, &quot;CEU0500000001&quot;, &quot;CEU05000… ## $ Title &lt;chr&gt; &quot;Total Private&quot;, &quot;Total Private&quot;, &quot;Total Private&quot;, &quot;Total Pr… ## $ Employed &lt;dbl&gt; 109928, 110140, 110603, 110962, 111624, 112303, 111897, 1118… A tsibble behaves like a tibble, so you can use tidyverse verbs. The only thing that will trip you up is that tsibble objects are grouped by the index, so group_by() operations implicitly include the index. Use index_by() if you need to summarize at a new time level. # Group by Quarter implicitly includes the Month index column. Don&#39;t do this. us_employment_tsibble %&gt;% group_by(Quarter = quarter(Month), Title) %&gt;% summarize(Employed = sum(Employed, na.rm = TRUE)) %&gt;% tail(3) # Instead, change the index aggregation level with index_by() and group with # either group_by() or group_by_key() us_employment_tsibble %&gt;% index_by(Quarter = ~ quarter(.)) %&gt;% group_by_key(Title) %&gt;% summarise(Employed = sum(Employed, na.rm = TRUE)) %&gt;% tail(3) modeltime The modeltime framework requires no special data object - it works with tibbles. 1.2 Fitting Models The workflow for an explanatory model is: fit, verify assumptions, then summarize parameters. For a predictive model it’s: compare models with cross-validation, predict values. Let’s continue working with the us_employment data set and use just the Total Private series. We’ll split the data into training and testing to support predictive modeling. # Tidyverts (tv) framework with tsibbles. tv_full &lt;- us_employment_tibble %&gt;% filter(Title == &quot;Total Private&quot;) %&gt;% mutate(Month = yearmonth(Month)) %&gt;% tsibble(key = c(Title), index = Month) tv_train &lt;- tv_full %&gt;% filter(year(Month) &lt;= 2015) tv_test &lt;- tv_full %&gt;% filter(year(Month) &gt; 2015) # Modeltime (mt) with tibbles mt_split &lt;- us_employment_tibble %&gt;% filter(Title == &quot;Total Private&quot;) %&gt;% timetk::time_series_split(date_var = Month, initial = &quot;15 years&quot;, assess = &quot;45 months&quot;) Fit some simple benchmark models. Some forecasting methods are extremely simple and surprisingly effective. The mean method projects the historical average, \\(\\hat{y}_{T+h|T} = \\bar{y}.\\) The naive method projects the last observation, \\(\\hat{y}_{T+h|T} = y_T.\\) The seasonal naive method projects the last seasonal observation, \\(\\hat{y}_{T+h|T} = y_{T+h-m(k+1)}.\\) The drift method projects the straight line from the first and last observation, \\(\\hat{y}_{T+h|T} = y_T + h\\left(\\frac{y_T - y_1}{T-1}\\right).\\) Modeltime doesn’t appear to support the mean or random walk models. # Tidverts supports fitting multiple models at once. tv_fit &lt;- tv_train %&gt;% model( Mean = MEAN(Employed), Naive = NAIVE(Employed), SNaive = SNAIVE(Employed), Drift = RW(Employed ~ drift()) ) # Modeltime follows the tidymodels style. mt_fit_naive &lt;- modeltime::naive_reg() %&gt;% set_engine(&quot;naive&quot;) %&gt;% parsnip::fit(Employed ~ Month, data = rsample::training(mt_split)) mt_fit_snaive &lt;- modeltime::naive_reg() %&gt;% set_engine(&quot;snaive&quot;) %&gt;% parsnip::fit(Employed ~ Month, data = rsample::training(mt_split)) ## frequency = 12 observations per 1 year mt_fit &lt;- modeltime::modeltime_table( mt_fit_naive, mt_fit_snaive ) The autoplot() and autolayer() functions take a lot of the headache out of plotting the results, especially since forecast() tucks away the confidence intervals in a distribution list object. Nevertheless, I like extracting the results manually. # modeltime mt_fc &lt;- mt_fit %&gt;% modeltime::modeltime_calibrate(testing(mt_split)) %&gt;% modeltime::modeltime_forecast( new_data = testing(mt_split), actual_data = us_employment_tibble %&gt;% filter(Title == &quot;Total Private&quot;) ) # (not run) # mt_fc %&gt;% modeltime::plot_modeltime_forecast(.interactive = FALSE) # tidyverts tv_fc &lt;- tv_fit %&gt;% forecast(new_data = tv_test) %&gt;% hilo(80) %&gt;% mutate( lpi = map_dbl(`80%`, ~.$lower), upi = map_dbl(`80%`, ~.$upper) ) tv_fit %&gt;% augment() %&gt;% ggplot(aes(x = Month)) + geom_line(aes(y = Employed)) + geom_line(data = tv_fc, aes(y = .mean, color = .model)) + geom_ribbon(data = tv_fc, aes(ymin = lpi, ymax = upi, fill = .model), alpha = 0.2) + labs(title = &quot;Forecast with 80% CI&quot;) 1.3 Evaluating Fit Evaluate the model fit with residuals diagnostics.1 broom::augment() adds three columns to the model cols: .fitted, .resid, and .innov. .innov is the residual from the transformed data (if no transformation, it just equals .resid). Innovation residuals should be independent random variables normally distributed with mean zero and constant variance (the normality and variance conditions are only required for inference and prediction intervals). Happily, feasts has just what you need. tv_fit %&gt;% select(&quot;Naive&quot;) %&gt;% feasts::gg_tsresiduals() + labs(title = &quot;Residuals Analysis&quot;) The autocorrelation plot tests the independence assumption. The histogram plot tests normality. The residuals plot tests mean zero and constant variance. You can carry out a portmanteau test on the autocorrelation assumption. Two common tests are the Box-Pierce and the Ljung-Box. These tests check the likelihood of a combination of autocorrelations at once, without testing any one correlation, kind of like an ANOVA test. The Ljung-Box test statistic is a sum of squared \\(k\\)-lagged autocorrelations, \\(r_k^2\\), \\[Q^* = T(T+2) \\sum_{k=1}^l(T-k)^{-1}r_k^2.\\] The test statistic has a \\(\\chi^2\\) distribution with \\(l - K\\) degrees of freedom (where \\(K\\) is the number of parameters in the model). Use \\(l = 10\\) for non-seasonal data and \\(l = 2m\\) for seasonal data. If your model has no explanatory variables, \\(K = 0.\\) Reject the no-autocorrelation (i.e., white noise) assumption if p &lt; .05. tv_fit %&gt;% broom::augment() %&gt;% features(.var = .innov, features = ljung_box, lag = 10, dof = 0) ## # A tibble: 4 × 4 ## Title .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Total Private Drift 43.3 0.00000442 ## 2 Total Private Mean 1085. 0 ## 3 Total Private Naive 43.3 0.00000442 ## 4 Total Private SNaive 1170. 0 1.4 Model Selection Evaluate the forecast accuracy with the test data (aka, “hold-out set”, and “out-of-sample data”). The forecast error is the difference between the observed and forecast value, \\(e_{T+h} = y_{T+h} - \\hat{y}_{t+h|T}.\\) Forecast errors differ from model residuals in that they come from the test data set and because forecast values are usually multi-step forecasts which include prior forecast values as inputs. The major accuracy benchmarks are: MAE. Mean absolute error, \\(mean(|e_t|)\\) RMSE. Root mean squared error, \\(\\sqrt{mean(e_t^2)}\\) MAPE. Mean absolute percentage error, \\(mean(|e_t / y_t|) \\times 100\\) MASE. Mean absolute scaled error, \\(MAE/Q\\) where \\(Q\\) is a scaling constant calculated as the average one-period change in the outcome variable (error from a one-step naive forecast). The MAE and RMSE are on the same scale as the data, so they are only useful for comparing models fitted to the same series. MAPE is unitless, but does not work for \\(y_t = 0\\), and it assumes a meaningful zero (ratio data). MASE is most useful for comparing data sets of different units. Use accuracy() to evaluate a model. tv_fit %&gt;% fabletools::forecast(new_data = tv_test) %&gt;% fabletools::accuracy(data = tv_test) %&gt;% select(.model, RMSE, MAE, MAPE, MASE) ## # A tibble: 4 × 5 ## .model RMSE MAE MAPE MASE ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Drift 2888. 2439. 1.93 NaN ## 2 Mean 13018. 12713. 10.1 NaN ## 3 Naive 4514. 3831. 3.02 NaN ## 4 SNaive 5961. 5435. 4.31 NaN Time series cross-validation is a better way to evaluate a model. It breaks the dataset into multiple training sets by setting a cutoff at varying points and setting the test set to a single step ahead of the horizon. Function stretch_tsibble() creates a tsibble of initial size .init and appends additional data sets of increasing size .step. Normal cross-validation repeatedly fits a model to the data set with one of the rows left out. Since model() fits a separate model per index value, creating this long tsibble effectively accomplishes the same thing. Note the fundamental difference here though: time series CV does not leave out single values from points along the time series. It leaves out all points after a particular point along the time series - each sub-data set starts at the beginning and is uninterrupted until reaching the varying end points. us_employment_tsibble %&gt;% filter(Title == &quot;Total Private&quot;) %&gt;% stretch_tsibble(.init = 3, .step = 1) %&gt;% # Fit a model for each key model( Mean = MEAN(Employed), Naive = NAIVE(Employed), SNaive = SNAIVE(Employed), Drift = RW(Employed ~ drift()) ) %&gt;% fabletools::forecast(h = 12) %&gt;% fabletools::accuracy(data = tv_test) ## # A tibble: 4 × 11 ## .model Title .type ME RMSE MAE MPE MAPE MASE RMSSE ACF1 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Drift Total Private Test 712. 1749. 1420. 0.561 1.14 0.631 0.775 0.825 ## 2 Mean Total Private Test 11845. 12043. 11845. 9.43 9.43 5.27 5.33 0.822 ## 3 Naive Total Private Test 1169. 2023. 1685. 0.925 1.34 0.749 0.896 0.795 ## 4 SNaive Total Private Test 2267. 2274. 2267. 1.81 1.81 1.01 1.01 0.641 Residuals and errors are not the same thing. The residual is the difference between the observed and fitted value in the training data set. The error is the difference between the observed and fitted value in the test data set.↩︎ "],["exploration.html", "Chapter 2 Exploratory Analysis 2.1 Graphical Analysis 2.2 Transformations 2.3 Decomposition", " Chapter 2 Exploratory Analysis Start an analysis by viewing the data values and structure, then take some summary statistics. fabletools::features() is great for this. tsibbledata::aus_production %&gt;% features(Beer, list(mean = mean, quantile = quantile)) ## # A tibble: 1 × 6 ## mean `quantile_0%` `quantile_25%` `quantile_50%` `quantile_75%` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 415. 213 379. 422 466. ## # ℹ 1 more variable: `quantile_100%` &lt;dbl&gt; There are many autocorrelation features you might want to review. I don’t understand why you’d want to know all of these, but feat_acf has them. tsibbledata::aus_production %&gt;% features(Beer, feat_acf) ## # A tibble: 1 × 7 ## acf1 acf10 diff1_acf1 diff1_acf10 diff2_acf1 diff2_acf10 season_acf1 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.684 4.37 -0.221 2.72 -0.361 2.26 0.940 2.1 Graphical Analysis The next task is to plot the data to identify patterns, unusual observations, changes over time, and relationships between variables. Look for trend, cycles, and seasonality in your exploratory plots. These features inform the subsequent forecasting process. ggplot2::autoplot() does a great job picking out the right plot, but I feel more comfortable staying old-school for now. tsibbledata::ansett %&gt;% filter(Airports == &quot;MEL-SYD&quot;, Class == &quot;Economy&quot;) %&gt;% ggplot(aes(x = Week, y = Passengers)) + geom_line(color = &quot;goldenrod&quot;) + theme_light() + labs(title = &quot;Start with a simple time series plot.&quot;, subtitle = &quot;Weekly passenger volume.&quot;, x = NULL, y = NULL) What does this one reveal? There was a period in 1989 of zero passengers (strike). There was a period in 1992 where passenger load dropped (planes temporarily reconfigured). Volume increased during the second half of 1992. Several large post-holiday dips in volume. Some larger trends of increasing and decreasing volume (the data is cyclic). Also appears to be some missing observations. (common practice with missing observations is to impute values with the time series mean.) If a data series has trend and seasonality, highlight it with feasts::gg_season(). a10 &lt;- tsibbledata::PBS %&gt;% filter(ATC2 == &quot;A10&quot;) %&gt;% select(Month, Cost) %&gt;% summarize(Cost = sum(Cost)) p1 &lt;- a10 %&gt;% filter(year(Month) %in% c(1991, 1995, 2000, 2005)) %&gt;% ggplot(aes(x = month(Month, label = TRUE, abbr = TRUE), y = Cost, group = factor(year(Month)), color = factor(year(Month)), label = if_else(month(Month) %in% c(1, 12), year(Month), NA_real_))) + geom_line(show.legend = FALSE) + geom_text(show.legend = FALSE) + theme_light() + theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) + labs(subtitle = &quot;using gglot&quot;, x = NULL, y = NULL) p2 &lt;- a10 %&gt;% filter(year(Month) %in% c(1991, 1995, 2000, 2005)) %&gt;% fill_gaps() %&gt;% # otherwise, gg_season() barks. gg_season(Cost, labels = &quot;both&quot;) + theme_light() + theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) + labs(subtitle = &quot;using feasts&quot;, x = NULL, y = NULL) p1 + p2 + plot_annotation(title = &quot;Medicare script costs increase from Feb - following Jan.&quot;, subtitle = &quot;12-month seasonality plot, selected years.&quot;) Emphasize the seasonality further by faceting on the sub-series with feasts::gg_subseries(). yint &lt;- a10 %&gt;% as.tibble() %&gt;% group_by(month(Month, label = TRUE, abbr = TRUE)) %&gt;% mutate(mean_cost = mean(Cost) / 1e6) p1 &lt;- a10 %&gt;% ggplot(aes(x = year(Month), y = Cost / 1e6)) + geom_line(show.legend = FALSE, color = &quot;goldenrod&quot;) + geom_hline(data = yint, aes(yintercept = mean_cost), linetype = 2) + theme_light() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), panel.grid = element_blank()) + scale_y_continuous(labels = scales::dollar) + facet_wrap(vars(month(Month, label = TRUE, abbr = TRUE)), nrow = 1) + labs(subtitle = &quot;using gglot&quot;, x = NULL, y = NULL) p2 &lt;- a10 %&gt;% mutate(Cost = Cost / 1e6) %&gt;% fill_gaps() %&gt;% # otherwise, gg_subseries() barks. gg_subseries(Cost) + theme_light() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), panel.grid = element_blank()) + scale_y_continuous(labels = scales::dollar) + labs(subtitle = &quot;using feasts&quot;, x = NULL, y = NULL) p1 / p2 + plot_annotation(title = &quot;Medicare script costs follow monthly seasonality, inreasing annually.&quot;, subtitle = &quot;Monthly subseries plot of cost $MM, 1991 - 2009.&quot;) Explore the correlation between two quantitative variables with the Pearson correlation coefficient. Recall that the covariance between series \\(X\\) and \\(Y\\) is defined \\(Cov(X, Y) = E[(X - \\mu_X) (Y - \\mu_Y)]\\) which simplifies to \\(Cov(X, Y) = E[XY] - \\mu_X \\mu_Y\\). The covariance of \\(X\\) and \\(Y\\) is positive if \\(X\\) and \\(Y\\) increase together, negative if they move in opposite directions, and if \\(X\\) and \\(Y\\) are independent, \\(E[XY] = E[X]E[Y] = \\mu_X \\mu_Y\\), so \\(Cov(X, Y) = 0\\). Covariance is usually inconvenient because its values depend on the units of the series. Dividing \\(Cov(X, Y)\\) by the standard deviations \\(\\sigma_X \\sigma_Y\\) creates a unit-less variable with range [-1, 1], also known as the Pearson correlation.2 \\[\\rho = \\frac{\\sigma_{XY}} {\\sigma_X \\sigma_Y}.\\] PBS %&gt;% group_by(ATC1) %&gt;% summarize(.groups = &quot;drop&quot;, Cost = sum(Cost)) %&gt;% pivot_wider(names_from = ATC1, values_from = Cost) %&gt;% as_tibble() %&gt;% select(-Month) %&gt;% cor() %&gt;% ggcorrplot::ggcorrplot() + theme_light() + labs(title = &quot;Z and P indexes negatively correlated with others.&quot;, subtitle = &quot;Correlation plot of Medicare ATC1 indexes.&quot;, x = NULL, y = NULL) Autocorrelation is correlation with lagging observations. Lag plots of current period vs lags are a particular kind of correlation scatterplot useful for identifying seasonality. feasts::ACF() extends the base R acf() function to tsibbles. aus_production contains quarterly production levels. The 4-period lag is a year-over-year correlation and is strong because of seasonality in production. The 8-period lag is less strong. The 1-, 2-, and 3-period lags are not positively correlated. In fact, the lag-2 is negatively correlated. prod2k &lt;- tsibbledata::aus_production %&gt;% filter(year(Quarter) &gt;= 2000) prod2k %&gt;% gg_lag(Beer, geom = &quot;point&quot;) + theme_light() + labs(title = &quot;Production is seasonal with Q4 peak, Q1 second.&quot;, subtitle = &quot;Quarterly lag plot of beer production&quot;, x = NULL, y = NULL) p1 &lt;- prod2k %&gt;% ACF(Beer, lag_max = 16) %&gt;% ggplot(aes(x = lag)) + geom_linerange(aes(ymin = 0, ymax = acf), color = &quot;goldenrod&quot;) + geom_point(aes(y = acf), color = &quot;goldenrod&quot;) + geom_hline(yintercept = 0) + geom_hline(yintercept = -2 / sqrt(nrow(prod2k)), linetype = 2) + geom_hline(yintercept = +2 / sqrt(nrow(prod2k)), linetype = 2) + theme_light() + theme(panel.grid = element_blank()) + labs(subtitle = &quot;using ggplot&quot;, y = NULL) p2 &lt;- prod2k %&gt;% ACF(Beer, lag_max = 16) %&gt;% autoplot() + theme_light() + labs(subtitle = &quot;using autoplot&quot;, y = NULL) p1 + p2 + plot_annotation(title = &quot;Strongest correlation is 4-period lag, negative 2-period lag.&quot;, subtitle = &quot;Quarterly autocorrelation of beer production.&quot;, caption = &quot;Dashed lines are +/- square root of series length, the white noise bound.&quot;) Autocorrelation for trending data tends to be large and positive because observations nearby in time are also nearby in size. The ACF tends to have positive values that slowly decrease as the lags increase. Autocorrelation for seasonal data tends to be larger for the seasonal lags (at multiples of the seasonal frequency). The Quarterly Australian Beer Production ACF above shows seasonality. Autocorrelation for both trended and seasonal data has a combination of these effects. Time series that show no autocorrelation are called white noise. White noise series have near-zero autocorrelation. Stock price changes often exhibit white noise. Almost all lags have autocorrelation insignificantly different from zero. Expect 95% of spikes to lie withing \\(\\pm 2 / \\sqrt{T}\\) where \\(T\\) is the length of the data series. stock &lt;- tsibbledata::gafa_stock %&gt;% filter(Symbol == &quot;FB&quot;) %&gt;% mutate(l1 = Close - lag(Close)) stock %&gt;% ACF(l1, lag_max = 16) %&gt;% ggplot(aes(x = lag)) + geom_linerange(aes(ymin = 0, ymax = acf), color = &quot;goldenrod&quot;) + geom_point(aes(y = acf), color = &quot;goldenrod&quot;) + geom_hline(yintercept = 0) + geom_hline(yintercept = -2 / sqrt(nrow(stock)), linetype = 2) + geom_hline(yintercept = +2 / sqrt(nrow(stock)), linetype = 2) + theme_light() + theme(panel.grid = element_blank()) + labs(title = &quot;Stock price changes tend to exhibit white noise.&quot;, subtitle = &quot;Daily autocorrelation of stock price changes.&quot;, caption = &quot;Dashed lines are +/- square root of series length, the white noise bound.&quot;) The Ljung-Box test tests the randomness of a series; a p-value under 0.05 rejects the null hypothesis of white noise. The test reject white noise for the beer production, but not for stock price changes. Box.test(prod2k$Beer, lag = 16, fitdf = 0, type = &quot;Ljung&quot;) ## ## Box-Ljung test ## ## data: prod2k$Beer ## X-squared = 187.44, df = 16, p-value &lt; 2.2e-16 Box.test(stock$l1, lag = 16, fitdf = 0, type = &quot;Ljung&quot;) ## ## Box-Ljung test ## ## data: stock$l1 ## X-squared = 16.745, df = 16, p-value = 0.4023 2.2 Transformations Remove known sources of variation (e.g., days per month, population growth, inflation). E.g., monthly totals may vary due to differing month lengths. milk &lt;- fma::milk %&gt;% as_tsibble() %&gt;% # milk is a `ts` object mutate(daily_avg = value / lubridate::days_in_month(index)) p1 &lt;- milk %&gt;% ggplot(aes(x = index, y = value)) + geom_line(color = &quot;goldenrod&quot;) + theme_light() + theme(axis.text.x = element_blank()) + labs(subtitle = &quot;Original&quot;, x = NULL, y = NULL) p2 &lt;- milk %&gt;% ggplot(aes(x = index, y = daily_avg)) + geom_line(color = &quot;goldenrod&quot;) + theme_light() + labs(subtitle = &quot;Daily Average&quot;, x = NULL, y = NULL) p1 / p2 + plot_annotation(title = &quot;Convert monthly sums into daily averages per month.&quot;, subtitle = &quot;Average daily milk production by month.&quot;) Make patterns more consistent across the data set. Simpler patterns usually lead to more accurate forecasts. A Box-Cox transformation can equalize seasonal variation. \\[w_t = \\begin{cases} \\mathrm{log}(y_t), &amp; \\mbox{if } \\lambda\\mbox{ = 0} \\\\ \\left(\\mathrm{sign}(y_t) |y_t|^\\lambda - 1 \\right) / \\lambda, &amp; \\mbox{otherwise} \\end{cases}\\] \\(\\lambda\\) can take any value, but values near the following yield familiar transformations. \\(\\lambda = 1\\): no substantive transformation. \\(\\lambda = 0.5\\): square root plus linear transformation. \\(\\lambda = 0.333\\): cube root plus linear transformation. \\(\\lambda = 0\\): natural log. \\(\\lambda = -1\\): inverse. A good value of \\(\\lambda\\) is one which makes the size of the seasonal variation about the same across the whole series. fabletools::features() and BoxCox.lambda() optimize \\(\\lambda\\), but try to choose a simple value to make interpretation clearer. Note that while forecasts are not sensitive to \\(\\lambda\\), prediction intervals are. lambda &lt;- tsibbledata::aus_production %&gt;% # guerrero() applies Guerrero&#39;s method to select lambda that minimizes the # coefficient of variation: .12. features(Gas, features = guerrero) %&gt;% pull(lambda_guerrero) # supposedly the same method, but returns .10 instead. lambda_v2 &lt;- forecast::BoxCox.lambda(aus_production$Gas, method = &quot;guerrero&quot;) p1 &lt;- aus_production %&gt;% mutate(gas_xform = box_cox(Gas, lambda)) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Gas), color = &quot;goldenrod&quot;) + theme_light() + theme(axis.text.x = element_blank()) + labs(subtitle = &quot;Original&quot;, x = NULL) p2 &lt;- aus_production %&gt;% mutate(gas_xform = box_cox(Gas, lambda)) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = gas_xform), color = &quot;goldenrod&quot;) + theme_light() + labs(subtitle = glue(&quot;Box-Cox, lambda = {scales::comma(lambda, accuracy = 0.01)}&quot;), y = glue(&quot;Gas^{scales::comma(lambda, accuracy = 0.01)}&quot;)) p1 / p2 + plot_annotation(title = &quot;Box-Cox transformation equalizes the seasonal component.&quot;, subtitle = &quot;Box-Cox transformation (Guerrero&#39;s method) of quarterly gas production.&quot;) 2.3 Decomposition Time series data often has trending, seasonality, and cycles. It’s usually useful to lump trending and cycles into a trend-cycle components, or simply “trend”, and treat time series data as consisting of seasonality \\(S_t\\), trend \\(T_t\\), and a remainder \\(R_t\\). If the magnitude of the seasonal fluctuations and the variation in the trend cycle are constant, then these components are additive, \\(y_t = S_t + T_t + R_t\\); if they are proportional to the level, then these components are multiplicative, \\(y_t = S_t \\times T_t \\times R_t\\). 2.3.1 Classical Decomposition Classical decomposition was commonly used until the 1950s. It is still the basis of other methods, so it is good to understand. Classical decomposition is based on moving averages. An m-MA moving average of order \\(m = 2k + 1\\) averages the \\(k\\) observations before \\(t\\) through the \\(k\\) observations after \\(t\\). The slider package is great for this. \\(m\\) is usually an odd number so that the number of periods before and after are equal. \\[\\hat{T}_t = \\frac{1}{m}\\sum_{j = -k}^k y_{t+j}\\] library(slider) # sliding window functions tsibbledata::global_economy %&gt;% filter(Country == &quot;Australia&quot;) %&gt;% mutate( `3-MA` = slide_dbl(Exports, mean, .before = 1, .after = 1, .complete = TRUE), `5-MA` = slide_dbl(Exports, mean, .before = 2, .after = 2, .complete = TRUE), `7-MA` = slide_dbl(Exports, mean, .before = 3, .after = 3, .complete = TRUE), `9-MA` = slide_dbl(Exports, mean, .before = 4, .after = 4, .complete = TRUE) ) %&gt;% pivot_longer(cols = ends_with(&quot;-MA&quot;), names_to = &quot;MA_name&quot;, values_to = &quot;MA&quot;) %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = MA, color = MA_name), na.rm = TRUE) + geom_line(aes(y = Exports), size = 1.25, color = &quot;#000000&quot;, alpha = 0.4) + theme_light() + # guides(color = guide_legend(title = &quot;series&quot;)) + labs(title = &quot;m-MA simple moving averages smooth a time series.&quot;, subtitle = &quot;m-MA of annual export proportion of GDP for m = 3, 5, 7, 9.&quot;, x = NULL, y = &quot;Pct of GDP&quot;, color = NULL) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. But what if you have known seasonal periods in the data? For example, with quarterly seasonality it makes sense to take a moving average of the 4 periods at once. Do this with a moving average of a four-period moving average. tsibbledata::aus_production %&gt;% filter(year(Quarter) &gt;= 1992) %&gt;% mutate( `4-MA` = slide_dbl(Beer, mean, .before = 1, .after = 2, .complete = TRUE), `2x4-MA` = slide_dbl(`4-MA`, mean, .before = 1, .after = 0, .complete = TRUE) ) %&gt;% pivot_longer(cols = ends_with(&quot;-MA&quot;), names_to = &quot;MA_type&quot;, values_to = &quot;MA&quot;) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Beer), color = &quot;gray&quot;, size = 1) + geom_line(aes(y = MA, color = MA_type), na.rm = TRUE) + theme_light() + ggthemes::scale_color_few() + guides(color = guide_legend(title = &quot;series&quot;)) + labs(title = &quot;MAs of MAs smooth seasonal periods.&quot;, subtitle = &quot;2-MA of a 4-MA for quarterly data.&quot;, y = NULL) Classical decomposition calculates a trend-cycle component \\(\\hat{T}_t\\) with an m-MA (odd order) or \\(2 \\times m\\)-MA. The de-trended series is the difference, \\(y_t - \\hat{T}_t\\). The seasonal component \\(\\hat{S}_t\\) is the seasonal average (e.g., for monthly seasons, then \\(\\hat{S}_1\\) would be the January average). The remainder is the residual, \\(\\hat{R}_t = y_t - \\hat{T}_t - \\hat{S}_t.\\) If the cycle variation and seasonal magnitude increases with the observation level, then the same principles apply except the subtrations are replaced with divisions, \\(y_t / \\hat{T}_t\\) and remainder \\(\\hat{R}_t = y_t / (\\hat{T}_t \\hat{S}_t)\\). fpp3::us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(classical = classical_decomposition(Employed, type = &quot;additive&quot;)) %&gt;% components() %&gt;% pivot_longer(cols = Employed:random, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;seasonal&quot;, &quot;random&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;Classical additive decomposition of US retail employment&quot;, subtitle = &quot;Employed = trend + seasonal + random&quot;, y = NULL) Classical decomposition has weaknesses: the trend-cycle is unavailable for the first few and and last few periods; it assumes the seasonal component is stable over time; and it also tends to over-smooth the data. 2.3.2 X-11 and SEATS X-11 and Seasonal Extraction in ARIMA Time Series (SEATS) are commonly used by governmental agencies. X-11 overcomes some of classical decomposition’s drawbacks by adding extra steps. It creates trend-cycle estimates for all periods, and accommodates a slowly varying seasonal component. fpp3::us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(x11 = X_13ARIMA_SEATS(Employed ~ x11())) %&gt;% components() %&gt;% pivot_longer(cols = Employed:irregular, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;seasonal&quot;, &quot;irregular&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;X11 decomposition of US retail employment&quot;, subtitle = &quot;Employed = trend + seasonal + irregular&quot;, y = NULL) SEATS is another one (too complicated to discuss evidently!). fpp3::us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(x11 = X_13ARIMA_SEATS(Employed ~ seats())) %&gt;% components() %&gt;% pivot_longer(cols = Employed:irregular, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;seasonal&quot;, &quot;irregular&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;SEATS decomposition of US retail employment&quot;, subtitle = &quot;Employed = f(trend, seasonal, irregular)&quot;, y = NULL) 2.3.3 STL Seasonal and Trend decomposition using Loess (STL) has several advantages over classical decomposition, and the SEATS and X-11 methods. It handles any type of seasonality (not just monthly and quarterly); the seasonality component can change; the smoothness of the trend-cycle can be changed by the modeler; and it is robust to outliers. The widow settings inside model() control how rapidly the components change. fpp3::us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(STL = STL(Employed ~ trend(window = 7) + season(window = &quot;periodic&quot;), robust = TRUE)) %&gt;% components() %&gt;% pivot_longer(cols = Employed:remainder, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;season_year&quot;, &quot;remainder&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;STL decomposition of US retail employment&quot;, subtitle = &quot;Employed = f(trend, seasonal, irregular)&quot;, y = NULL) STL decomposition is the basis for other insights into the data series. You can measure the relative strength of trend and seasonality by the relative size of their variance: \\(F_T = 1 - \\mathrm{Var}(R_T) / \\mathrm{Var}(R_T + T_T)\\) and \\(F_S = 1 - \\mathrm{Var}(S_T) / \\mathrm{Var}(S_T + T_T)\\). us_employment_featues &lt;- fpp3::us_employment %&gt;% features(Employed, feat_stl) %&gt;% inner_join(fpp3::us_employment %&gt;% as_tibble() %&gt;% select(Series_ID, Title) %&gt;% unique(), by = &quot;Series_ID&quot;) us_1 &lt;- us_employment_featues %&gt;% filter(trend_strength &gt;= 0.995 &amp; seasonal_strength_year &gt; 0.9) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_2 &lt;- us_employment_featues %&gt;% filter(trend_strength &gt;= 0.995 &amp; seasonal_strength_year &lt; 0.5) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_3 &lt;- us_employment_featues %&gt;% filter(trend_strength &lt;= 0.985 &amp; seasonal_strength_year &lt; 0.5) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_4 &lt;- us_employment_featues %&gt;% filter(trend_strength &lt;= 0.985 &amp; seasonal_strength_year &gt; 0.9) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_employment_ids &lt;- c(us_1, us_2, us_3, us_4) us_employment_featues %&gt;% mutate(Series_lbl = if_else(Series_ID %in% us_employment_ids, Title, NA_character_)) %&gt;% ggplot(aes(x = trend_strength, y = seasonal_strength_year)) + geom_point(color = &quot;goldenrod&quot;) + geom_text(aes(label = Series_lbl), color = &quot;goldenrod4&quot;) + theme_light() + labs(title = &quot;Some sectors have seasonal employment, othes trend, others both.&quot;) Incidentally, \\(\\rho\\) is related to the slope of the linear regression line: \\(\\beta_1 = \\frac{\\sigma_{XY}}{\\sigma_X^2} = \\rho \\frac{\\sigma_Y}{\\sigma_X}\\).↩︎ "],["regression.html", "Chapter 3 Time Series Regression 3.1 Exploratory Analysis 3.2 Fit Model 3.3 Model Evaluation 3.4 Variable Selection 3.5 Predicting Values 3.6 Nonlinear Regression", " Chapter 3 Time Series Regression A time series regression forecasts a time series as a linear relationship with the independent variables. \\[y_t = X_t \\beta + \\epsilon_t\\] The linear regression model assumes there is a linear relationship between the forecast variable and the predictor variables. This implies that the errors must have mean zero, otherwise the forecasts are biased: \\(E(\\epsilon | X_j) = 0\\). The least squares method guarantees this condition is met. The residuals must not be autocorrelated, otherwise the forecasts are inefficient because there is more information in the data that can be exploited. To produce reliable inferences and prediction intervals, the residuals must be independent normal random variables with constant variance. Let’s learn by example. Data set tsibbledata::aus_production contains quarterly estimates of selected indicators of manufacturing production in Australia: Beer, Gas, Electricity, and Cement. (Tobacco and Bricks too, but they are incomplete data series). We’ll look at Beer production. 3.1 Exploratory Analysis The correlation matrix shows Beer is negatively correlated with Gas, Electricity, and Cement. tsibbledata::aus_production %&gt;% filter(year(Quarter) &gt;= 1992) %&gt;% as_tibble() %&gt;% select(-Quarter) %&gt;% cor() %&gt;% ggcorrplot::ggcorrplot(type = &quot;upper&quot;, lab = TRUE, lab_size = 3) + theme_light() + labs(title = &quot;Consumption is correlated with predictors&quot;, subtitle = &quot;Correlation plot of US economic indicators.&quot;, caption = &quot;Source: tsibbledata::aus_production.&quot;, x = NULL, y = NULL) You would probably want to explore the data further, but this is a minimal example. 3.2 Fit Model Consider this kitchen-sink model: \\[\\mathrm{Beer}_t = \\beta_0 + \\beta_1 \\mathrm{Gas}_t + \\beta_2 \\mathrm{Electricity}_t + \\beta_3 \\mathrm{Cement}_t + \\epsilon_t\\] Use fable::TSLM() to fit a linear regression model to tsibble time series data. TSLM() is similar to lm() with additional facilities for handling time series. TSLM() %&gt;% report() is identical to lm() %&gt;% summary(). fable is part of the tidyverts group of time series packages. One nice feature of the tidyverts is piping. ausprod_fmla &lt;- formula(Beer ~ Gas + Electricity + Cement) ausprod_gas &lt;- tsibbledata::aus_production %&gt;% filter(year(Quarter) &gt;= 1992) ausprod_lm &lt;- ausprod_gas %&gt;% model(TSLM(ausprod_fmla)) report(ausprod_lm) ## Series: Beer ## Model: TSLM ## ## Residuals: ## Min 1Q Median 3Q Max ## -61.47 -25.47 -11.00 21.05 76.03 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.748e+02 3.583e+01 13.250 &lt; 2e-16 *** ## Gas -1.216e+00 2.501e-01 -4.860 6.93e-06 *** ## Electricity 7.544e-04 1.469e-03 0.514 0.60919 ## Cement 7.404e-02 2.609e-02 2.838 0.00594 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 36.67 on 70 degrees of freedom ## Multiple R-squared: 0.3097, Adjusted R-squared: 0.2801 ## F-statistic: 10.47 on 3 and 70 DF, p-value: 9.003e-06 The modeled \\(R^2\\) is 0.310, the adjusted \\(R^2\\) is 0.280, and the standard error of the regression3, \\(\\hat{\\sigma}_\\epsilon,\\) is 36.7. The fitted values follow the observations okay. The fitted to actuals plot has a moderate linear relationship. plot_lm &lt;- function() { p1 &lt;- augment(ausprod_lm) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Beer), color = &quot;dark gray&quot;, size = 1) + geom_line(aes(y = .fitted), color = &quot;goldenrod&quot;, size = 1) + theme_light() + labs(subtitle = &quot;Time series&quot;) p2 &lt;- augment(ausprod_lm) %&gt;% ggplot(aes(x = Beer, y = .fitted)) + geom_point(color = &quot;goldenrod&quot;, size = 1) + geom_abline(intercept = 0, slope = 1, linetype = 2, size = 1, color = &quot;dark gray&quot;) + theme_light() + labs(subtitle = &quot;Fitted vs actuals&quot;) p3 &lt;- p1 + p2 + patchwork::plot_annotation(title = &quot;Fitted values plots&quot;, subtitle = ausprod_fmla) p3 } plot_lm() Time series regressions usually suffer from autocorrelation, so you need to control for trend and seasonality by adding time and seasonal dummy variables. Do this in TSLM() by including the trend() and season() helper functions in the formula. These are the tidyverts equivalent to introducing a row sequence and seasonal dummies as predictors in base R. ausprod_fmla &lt;- update(ausprod_fmla, . ~ . + trend() + season()) ausprod_lm &lt;- ausprod_gas %&gt;% model(TSLM(ausprod_fmla)) plot_lm() Much better. The modeled \\(R^2\\) is 0.926, the adjusted \\(R^2\\) is 0.919, and the standard error of the regression, \\(\\hat{\\sigma}_\\epsilon,\\) is 12.3. The fitted to actuals plot has a moderate linear relationship. Special Predictors We used two special predictor functions in our model, trend() and season(). Let’s look more closely at them. Adding the trend() special function is the same thing as adding a row number predictor. It captures the slope of the response variable associated with its sequence in the regression. ausprod_gas %&gt;% model(TSLM(Beer ~ trend())) %&gt;% augment() %&gt;% mutate(rownum = row_number()) %&gt;% ggplot(aes(x = rownum)) + geom_point(aes(y = .fitted), color = &quot;steelblue&quot;, size = 2) + geom_point(aes(y = Beer), color = &quot;goldenrod&quot;, size = 1) + geom_smooth(aes(y = Beer), method = &quot;lm&quot;, formula = &quot;y ~ x&quot;, size = .5) + theme_light() + labs(title = &quot;trend() is equivalent to adding time sequence to model.&quot;, subtitle = &quot;Beer ~ trend()&quot;) season() is the equivalent to adding seasonal dummy vars to the regression. tsibbledata::aus_production is defined with a quarterly index, so there are three associated dummies. x &lt;- ausprod_gas %&gt;% model(TSLM(Beer ~ season())) # Gets the number of dummies right, but why call them &quot;year&quot;? coef(x) ## # A tibble: 4 × 6 ## .model term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TSLM(Beer ~ season()) (Intercept) 429. 3.27 131. 1.71e-85 ## 2 TSLM(Beer ~ season()) season()year2 -35.0 4.63 -7.57 1.14e-10 ## 3 TSLM(Beer ~ season()) season()year3 -17.8 4.69 -3.80 3.05e- 4 ## 4 TSLM(Beer ~ season()) season()year4 72.5 4.69 15.5 2.97e-24 augment(x) %&gt;% mutate(season = quarter(Quarter)) %&gt;% ggplot(aes(x = season)) + geom_point(aes(y = .fitted), color = &quot;steelblue&quot;, size = 2) + geom_point(aes(y = Beer), color = &quot;goldenrod&quot;, size = 1) + theme_light() + labs(title = &quot;season() is equivalent to adding seasonal dummy variables to model.&quot;, subtitle = &quot;Beer ~ season()&quot;) There are other special variables you might consider. If an exogenous event has a one period effect or level effect, model it with an intervention dummy. If the intervention has a trend effect, use a piecewise linear trend. If you are modeling monthly totals, you might want to control for the number of days in the month (trading days, business days, etc.). You can do this by transforming your response variable, or by introducing a predictor with the normalizing factor. Some predictor variables may exert a lagged effect. A good example is advertising; its effect is most pronounced in the first month with a diminishing effect in subsequent months. If you expect a lag effect for a variable, include its lagged value in the model formula. \\[y = \\beta_0 + \\beta_1x_{l0} + \\beta_2x_{l1} + \\ldots\\] Fourier terms are an alternative to dummy variables. They are especially useful for long seasonal periods. Fourier showed that a series of alternating sine and cosine terms of the right frequencies can approximate any periodic function. A quarterly seasonal regression would have terms \\(\\sin\\left(\\frac{2\\pi}{m}\\right)\\), \\(\\cos\\left(\\frac{2\\pi}{m}\\right)\\), \\(\\sin\\left(\\frac{4\\pi}{m}\\right)\\). Use special function fourier(K) where K equals the number of sin and cos pairs to include, usually equal to half the number of seasonal periods. 3.3 Model Evaluation Evaluate the regression model with diagnostic plots. Use feasts::gg_tsresiduals() from the tidyverts. gg_tsresiduals(ausprod_lm) Time series observations are usually related to prior observations. That shows up in diagnostic plots as autocorrelation in the residuals. Autocorrelation in the residuals increases the prediction intervals, making forecasts less efficient (although still unbiased). The autocorrelation function plot (ACF) finds a significant negative spike at lag 1 and a positive spike at lag 12. Another test of autocorrelation in the residuals is the Breusch-Godfrey test for serial correlation up to a specified order. A small p-value indicates there is significant autocorrelation remaining in the residuals. The Breusch-Godfrey test is similar to the Ljung-Box test, but it is specifically designed for use with regression models. I cannot find support for the Breusch-Godfrey test. (ausprod_lm_lb &lt;- ausprod_lm %&gt;% augment() %&gt;% features(.innov, ljung_box, lag = 12, dof = 4)) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TSLM(ausprod_fmla) 22.0 0.00497 Using Ljung-Box, the spike at lag 1 is significant (p = 0.0050). The autocorrelation is not particularly large and is unlikely to have a noticeable impact on the forecasts or the prediction intervals. The residuals vs time diagnostic plot revealed no heteroscedasticity (although there might be an outlier). Heteroscedasticity can make prediction intervals inaccurate. The histogram shows that the residuals are slightly left-skewed. Non-normality of the residuals can also make the prediction intervals inaccurate. The residuals should be independent of each of the explanatory variables and independent of candidate variables not used in the model. In this case, the residuals have a random pattern in each of the plots. ausprod_gas %&gt;% left_join(residuals(ausprod_lm), by = &quot;Quarter&quot;) %&gt;% pivot_longer(Cement:Gas, names_to = &quot;regressor&quot;, values_to = &quot;x&quot;) %&gt;% ggplot(aes(x = x, y = .resid, color = regressor)) + geom_point(show.legend = FALSE) + facet_wrap(vars(regressor), scales = &quot;free_x&quot;) + labs(title = &quot;There is no relationship between residuals and individual regressors.&quot;, subtitle = &quot;otherwise the relationship may be nonlinear.&quot;, x = NULL) + theme_light() + ggthemes::scale_color_few() A second check on the homoscedastity assumption is a plot of the residuals against the fitted values. Again, there should be no pattern. augment(ausprod_lm) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + labs(title = &quot;There is no relationship between residuals and fitted values.&quot;, subtitle = &quot;otherwise the response variable may require transformation.&quot;, y = &quot;Residuals&quot;, x = &quot;Fitted&quot;) + theme_light() Outliers, Leverage Points, and Influential Points Check for outliers, leverage points, and influential points. An outlier is a point far from the others (in either the x or y direction); a leverage point is far from the others in the x direction, potentially affecting the measured slope; an influential point is a leverage point that does affect the slope. For multiple linear regression models there is no straight-forward visual diagnostic like the simple linear regression scatter plot. The “hat matrix” \\(H\\) identifies leverage points. Recall that in the linear regression model, \\(\\hat{y} = X \\hat{\\beta}\\), the slope coefficients are estimated by \\(\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y\\). Substituting, \\(\\hat{y} = X(X&#39;X)^{-1}X&#39;y\\), or \\(\\hat{y} = Hy\\), where \\[H = X(X&#39;X)^{-1}X&#39;.\\] \\(H\\) is called the hat matrix because \\(H\\) puts the hat on \\(y\\). \\(H\\) is an \\(n \\times n\\) matrix. The diagonal elements \\(H_{ii}\\) are a measure of the distances between each observation \\(i\\)’s predictor variables \\(X_i\\) and the average of the entire data set predictor variables \\(\\bar{X}\\). \\(H_{ii}\\) are the leverage that the observed responses \\(y_i\\) exert on the predicted responses \\(\\hat{y}_i\\). Each \\(H_{ii}\\) is in the unit interval [0, 1] and the values sum to the number of regression parameters (including the intercept) \\(\\sum{H_{ii}} = k + 1\\). A common rule is to research any observation whose leverage value is more than 3 times larger than the mean leverage value, which since the sum of the leverage values is \\(k + 1\\), equals \\[H_{ii} &gt; 3 \\frac{k + 1}{n}.\\] Identify influential points by their Cook’s distance. Cook’s distance for observation \\(i\\) is defined \\[D_i = \\frac{(y_i - \\hat{y}_i)^2}{p \\times MSE} \\frac{H_{ii}}{(1 - H_{ii})^2}.\\] \\(D_i\\) directly summarizes how much all of the fitted values change when the ith observation is deleted. A data point with \\(D_i &gt; 1\\) is probably influential. \\(D_i &gt; 0.5\\) is at least worth investigating. I cannot find any support for Cook’s distance in fable. 3.4 Variable Selection There are five common measures of predictive accuracy: \\(\\bar{R}^2\\), CV, AIC, AICc, and BIC. glance(ausprod_lm) %&gt;% t() ## [,1] ## .model &quot;TSLM(ausprod_fmla)&quot; ## r_squared &quot;0.9263632&quot; ## adj_r_squared &quot;0.9185533&quot; ## sigma2 &quot;152.123&quot; ## statistic &quot;118.613&quot; ## p_value &quot;7.315323e-35&quot; ## df &quot;8&quot; ## log_lik &quot;-286.6818&quot; ## AIC &quot;381.3606&quot; ## AICc &quot;384.1731&quot; ## BIC &quot;402.0972&quot; ## CV &quot;170.4503&quot; ## deviance &quot;10040.12&quot; ## df.residual &quot;66&quot; ## rank &quot;8&quot; \\(\\bar{R}^2\\) is common and well-established, but tends to select too many predictor variables, making it less suitable for forecasting. BIC has the feature that if there is a true underlying model, the BIC will select it given enough data. However, there is rarely a true underlying model, and even if there was one, that model would not necessarily produce the best forecasts because the parameter estimates may not be accurate. The AICc, AIC, and CV statistics are usually best because forecasting is their objective. If the value of time series size \\(T\\) is large enough, they all lead to the same model. \\(R^2 = 1 - \\frac{SSE}{SST}\\) and \\(\\bar{R}^2 = 1 - (1 - R^2) \\frac{T - 1}{T - k - 1}\\). Maximizing \\(\\bar{R}^2\\) is equivalent to minimizing the regression standard error \\(\\hat{\\sigma}\\). Classical leave-one-out cross-validation (CV) measures the predictive ability of a model. In concept, CV is calculated by fitting the model without observation \\(t\\) and measuring the predictive error on observation \\(t\\). Repeat for all \\(T\\) observations. CV is the mean squared error, and the model with the minimum CV is the best model for forecasting. In practice, you use the hat matrix instead of fitting the model repeatedly. \\[CV = MSE = \\frac{1}{T} \\sum_{t=1}^T \\left[\\frac{e_t}{1 - h_t}\\right]^2\\] where \\(h_t\\) are the diagonal values of the hat-matrix \\(H\\) from \\(\\hat{y} = X\\beta = X(X&#39;X)^{-1}X&#39;y = Hy\\) and \\(e_t\\) is the residual obtained from fitting the model to all \\(T\\) observations. Closely related to CV is Akaike’s Information Criterion (AIC), defined as \\[AIC = T \\log\\left(\\frac{SSE}{T}\\right) + 2(k + 2)\\] The measure penalizes the model by the number of parameters that need to be estimated. The model with the minimum AIC is the best model for forecasting. For large values of \\(T\\), minimizing the AIC is equivalent to minimizing the CV. For small values of \\(T\\), the AIC tends to select too many predictors, and so a bias-corrected version of the AIC has been developed, AICc. \\[AIC_c = AIC + \\frac{2(k+2)(k + 3)}{T - k - 3}\\] BIC is similar to AIC, but penalizes the number of parameters more heavily than the AIC. For large values of \\(T\\), minimizing BIC is similar to leave-v-out cross-validation when \\(v = T[1 − 1/\\log(T) - 1]\\). \\[BIC = T \\log\\left(\\frac{SSE}{T}\\right) + (k + 2)\\log(T)\\] There are many strategies to choose regression model predictors when there are many to choose from. Two common methods for using these measures are best subsets regression and stepwise regression. In best subsets regression, you fit all possible models then choose the one with the best metric value (e.g., lowest AIC). If there are too many candidate models (40 predictors would yield \\(2^{40}\\) models!), use stepwise regression. In backwards stepwise regression, include all candidate predictors initially, then check whether leaving any one predictor out improves the evaluation metric. If any leave-one-out model is better, then choose the best leave-one-out model. Repeat until no leave-one-out model is better. Let’s use the best subsets method to evaluate the possible models for the ausprod_gas data set. The 3 candidate predictors yield \\(2^3 = 8\\) possible models. I don’t know how to do this in caret, so I’ll just do it manually. x &lt;- formula(Beer ~ trend() + season()) fmla &lt;- tribble( ~Gas, ~Electricity, ~Cement, ~fmla, 1, 1, 1, update(x, ~ . + Gas + Electricity + Cement), 1, 1, 0, update(x, ~ . + Gas + Electricity), 1, 0, 1, update(x, ~ . + Gas + Cement), 0, 1, 1, update(x, ~ . + Electricity + Cement), 1, 0, 0, update(x, ~ . + Gas), 0, 1, 0, update(x, ~ . + Electricity), 0, 0, 1, update(x, ~ . + Cement), 0, 0, 0, x ) ausprod_best_subsets &lt;- fmla %&gt;% mutate(mdl = map(fmla, function(x) model(ausprod_gas, TSLM(x))), mdl_glance = map(mdl, glance)) %&gt;% unnest(mdl_glance) %&gt;% select(Gas, Electricity, Cement, AdjR2 = adj_r_squared, CV, AIC, AICc, BIC) ausprod_best_subsets %&gt;% flextable() %&gt;% flextable::theme_zebra() %&gt;% flextable::border(j = 3, border.right = officer::fp_border()) %&gt;% flextable::colformat_double(j = 4, digits = 3) %&gt;% flextable::colformat_double(j = 5:8, digits = 0) %&gt;% flextable::set_caption(&quot;Best subsets model is Beer ~ Gas + Cement&quot;) .cl-b3fc2d28{}.cl-b3f5b6be{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b3f5b6c8{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b3f83eac{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b3f84d5c{width:0.75in;background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b3f84d5d{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b3f84d66{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b3f84d70{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b3f84d71{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 3.1: Best subsets model is Beer ~ Gas + Cement GasElectricityCementAdjR2CVAICAICcBIC1110.9191703813844021100.9201673803823981010.9191653803823980110.9181693813833991000.9201623783803940100.9191653793813950010.9191643793813950000.920160377379391 All of the models produce about the same \\(\\bar{R}^2\\). The CV, AIC, AICc, and BIC metrics shrink somewhat with less predictors, optimizing on the intercept-only model. It seems that while beer production is correlated with other commodities, it is fully explained by the trend and seasonal variables. 3.5 Predicting Values Use the forecast() method to predict future periods. If there are no predictors in the model other than trend() and season(), forecast() is all you need. If there are predictors, construct “new data” using scenarios() and new_data(). # scenarios() creates data sets extending the key-index in `.data` `n` periods. future_dat &lt;- scenarios( `High Gas` = new_data(ausprod_gas, n = 4) %&gt;% mutate(Gas = max(ausprod_gas$Gas), Electricity = mean(ausprod_gas$Electricity), Cement = mean(ausprod_gas$Cement)), `Low Gas` = new_data(ausprod_gas, n = 4) %&gt;% mutate(Gas = min(ausprod_gas$Gas), Electricity = mean(ausprod_gas$Electricity), Cement = mean(ausprod_gas$Cement)) ) future_dat ## $`High Gas` ## # A tsibble: 4 x 4 [1Q] ## Quarter Gas Electricity Cement ## &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010 Q3 252 50230. 1948. ## 2 2010 Q4 252 50230. 1948. ## 3 2011 Q1 252 50230. 1948. ## 4 2011 Q2 252 50230. 1948. ## ## $`Low Gas` ## # A tsibble: 4 x 4 [1Q] ## Quarter Gas Electricity Cement ## &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010 Q3 116 50230. 1948. ## 2 2010 Q4 116 50230. 1948. ## 3 2011 Q1 116 50230. 1948. ## 4 2011 Q2 116 50230. 1948. ## ## attr(,&quot;names_to&quot;) ## [1] &quot;.scenario&quot; ausprod_fc &lt;- forecast(ausprod_lm, new_data = future_dat) ausprod_fc_2 &lt;- ausprod_fc %&gt;% mutate(mu = map_dbl(Beer, ~unlist(.) %&gt;% .[&quot;mu&quot;]), sigma = map_dbl(Beer, ~unlist(.) %&gt;% .[&quot;sigma&quot;]), ci_025 = qnorm(.025, mu, sigma), ci_100 = qnorm(.100, mu, sigma), ci_900 = qnorm(.900, mu, sigma), ci_975 = qnorm(.975, mu, sigma)) %&gt;% select(.scenario, Quarter, Beer, mu, sigma, ci_025:ci_975) ausprod_gas %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Beer), color = &quot;goldenrod&quot;) + geom_line(data = ausprod_fc_2, aes(y = mu, color = .scenario), size = 1) + geom_ribbon(data = ausprod_fc_2, aes(ymin = ci_100, ymax = ci_900, fill = .scenario), alpha = .2) + geom_ribbon(data = ausprod_fc_2, aes(ymin = ci_025, ymax = ci_975, fill = .scenario), alpha = .2) + scale_color_manual(values = c(&quot;High Gas&quot; = &quot;brown&quot;, &quot;Low Gas&quot; = &quot;darkolivegreen&quot;)) + scale_fill_manual(values = c(&quot;High Gas&quot; = &quot;brown&quot;, &quot;Low Gas&quot; = &quot;darkolivegreen&quot;)) + theme_light() + theme(legend.position = &quot;right&quot;) + labs(title = &quot;Four period linear model, high- and low-gas scenarios.&quot;, caption = &quot;Shaded area is 80%- and 95% confidence interval.&quot;, x = NULL, y = &quot;Beer&quot;, color = NULL, fill = NULL) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## Warning: Removed 8 rows containing missing values (`geom_line()`). ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf 3.6 Nonlinear Regression If there are kinks in the response variable trend, you can use piecewise linear regression by defining the knots in the series. boston_dat &lt;- fpp3::boston_marathon %&gt;% filter(Year &gt;= 1924) %&gt;% filter(Event == &quot;Men&#39;s open division&quot;) %&gt;% mutate(Minutes = as.numeric(Time)/60) boston_lm &lt;- boston_dat %&gt;% model(piecewise = TSLM(Minutes ~trend(knots = c(1950, 1980)))) boston_fc &lt;- boston_lm %&gt;% forecast(h = 10) %&gt;% mutate(mu = map_dbl(Minutes, ~unlist(.) %&gt;% .[&quot;mu&quot;]), sigma = map_dbl(Minutes, ~unlist(.) %&gt;% .[&quot;sigma&quot;]), ci_025 = qnorm(.025, mu, sigma), ci_100 = qnorm(.100, mu, sigma), ci_900 = qnorm(.900, mu, sigma), ci_975 = qnorm(.975, mu, sigma)) %&gt;% select(Year, Minutes, mu, sigma, ci_025:ci_975) augment(boston_lm) %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = Minutes), color = &quot;darkgray&quot;) + geom_line(aes(y = .fitted), color = &quot;goldenrod&quot;) + geom_line(aes(y = mu), data = boston_fc, color = &quot;goldenrod&quot;) + geom_ribbon(aes(ymin = ci_100, ymax = ci_900, fill = .scenario), data = boston_fc, fill = &quot;goldenrod&quot;, alpha = .2) + geom_ribbon(aes(ymin = ci_025, ymax = ci_975, fill = .scenario), data = boston_fc, fill = &quot;goldenrod&quot;, alpha = .2) + theme_light() + theme(legend.position = &quot;right&quot;) + labs(title = &quot;Piecewise linear regression with knots at 1950 and 1980.&quot;, subtitle = &quot;Boston Marathon winning times, Minutes ~trend(knots = c(1950, 1980))&quot;, caption = &quot;Shaded area is 80%- and 95% confidence interval.&quot;, x = NULL, y = &quot;Minutes&quot;, color = NULL, fill = NULL) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## Warning: Removed 10 rows containing missing values (`geom_line()`). ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf The residual standard error, \\(\\hat{\\sigma}\\), is actually the standard deviation of the residuals. See discussion on StackExchange. Is the name related to the fact that \\(\\hat{\\sigma}\\) is part of the formula for the standard error of the coefficient estimates, \\(SE\\left(\\hat{\\beta}\\right) = \\sqrt{\\hat{\\sigma}^2(X&#39;X)^{-1}}\\)? See my regression notes.↩︎ "],["exponential.html", "Chapter 4 Exponential Smoothing (ETS) 4.1 Simple Exponential Smoothing (SES) 4.2 Holt Linear 4.3 Additive Damped Trend 4.4 Holt-Winters 4.5 Auto-fitting", " Chapter 4 Exponential Smoothing (ETS) Exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get more remote. Exponential smoothing is a family of methods that vary by their trend and seasonal components. .cl-b4d51048{}.cl-b4cefe06{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b4cefe10{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b4d14f12{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b4d14f1c{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b4d14f26{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b4d14f27{margin:0;text-align:justify;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b4d14f30{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b4d15c6e{width:1.644in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(204, 204, 204, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15c78{width:2.177in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15c82{width:1.582in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15c83{width:1.883in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15c84{width:1.644in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(204, 204, 204, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15c8c{width:2.177in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15c8d{width:1.582in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15c8e{width:1.883in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15c96{width:1.644in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(204, 204, 204, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15ca0{width:2.177in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15ca1{width:1.582in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15caa{width:1.883in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15cab{width:1.644in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(204, 204, 204, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15cac{width:2.177in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15cb4{width:1.582in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15cb5{width:1.883in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15cb6{width:1.644in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(204, 204, 204, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15cbe{width:2.177in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15cbf{width:1.582in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b4d15cc8{width:1.883in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 4.1: Exponential smoothing taxonomy. Seasonal ComponentTrend ComponentNone (N)Additive (A)Multiplicative (M)None (N)(N, N)Simple Exponential Smoothing(N, A)(N, M)Additive (A)(A, N)Holt’s Linear(A, A)Additive Holt-Winters(A, M)Multiplicative Holt-WintersAdditive Damped (Ad)(Ad, N)Additive damped trend(Ad, A)(Ad, M)Holt-Winters Damped There can be no trend (N), an additive (A) linear trend from the forecast horizon, or a damped additive (Ad) trend leveling off from the forecast horizon.4 There can be no seasonality (N), or it can be additive (A) or multiplicative (M). The trend and seasonal combinations produce 3 x 3 = 9 possible exponential smoothing methods. ETS (Error, Trend, and Seasonality) models double the number of possible state space models to 18 by treating the error variances as either additive (A) or multiplicative (M). ETS models do not just extend the exponential smoothing models; they also estimate their parameters differently, using maximum likelihood estimation. For models with additive errors, this is equivalent to minimizing the sum of squared errors (SSE). The great advantage of using ETS models is that you can optimize the parameter settings by minimizing the Akaike Information Criterion (AICc). fable::ETS() fits ETS models: ETS(y ~ error(c(&quot;A&quot;, &quot;M&quot;)) + trend(c(&quot;N&quot;, &quot;A&quot;, &quot;Ad&quot;)) + season(c(&quot;N&quot;, &quot;A&quot;, &quot;M&quot;)) 4.1 Simple Exponential Smoothing (SES) Simple exponential smoothing models (SES) have no seasonal or trend components. They are of the form \\(\\hat{y}_{T+h|T} = \\alpha(1-\\alpha)^0 y_{T-0} + \\alpha(1-\\alpha)^1y_{T-1} + \\alpha(1-\\alpha)^2y_{T-2} \\dots\\) where \\(0 &lt; \\alpha &lt; 1\\) is a weighting parameter. On the one extreme, \\(\\alpha\\) = 1 is the same as a naive model. On the other extreme \\(\\alpha \\approx\\) 0 is the average model. ETS models are commonly expressed in component form as a recursive model. The component form of SES is \\[ \\begin{align} \\hat{y}_{t+h|t} &amp;= l_t \\\\ l_t &amp;= \\alpha y_t + (1 - \\alpha)l_{t-1} \\end{align} \\] The first component, \\(\\hat{y}_{t+h|t}\\), is the forecast. It equals the last value of the estimated level. The second component, \\(l_t\\), is the level (or smoothed value) of the series at time \\(t\\). It describes how the level changes over time, kind of like a slope. ETS uses nonlinear optimization to estimate two parameters for SES, \\(\\alpha\\) and \\(l_0\\). Example Data set tsibbledata::global_economy contains annual country-level economic indicators, including Exports. This time series has no trend or seasonality, so it is a good candidate for SES. tsibbledata::global_economy %&gt;% filter(Country == &quot;Algeria&quot;) %&gt;% ggplot(aes(x = Year, y = Exports)) + geom_line() + labs(title = &quot;Algerian Exports (% of GDP)&quot;) fable::ETS() is the exponential smoothing function. With additive errors, this is an ETS(A, N, N) model. ses_fit &lt;- tsibbledata::global_economy %&gt;% filter(Country == &quot;Algeria&quot;) %&gt;% model(ETS(Exports ~ error(&quot;A&quot;) + trend(&quot;N&quot;) + season(&quot;N&quot;))) ses_fit %&gt;% report() ## Series: Exports ## Model: ETS(A,N,N) ## Smoothing parameters: ## alpha = 0.8399875 ## ## Initial states: ## l[0] ## 39.539 ## ## sigma^2: 35.6301 ## ## AIC AICc BIC ## 446.7154 447.1599 452.8968 Exports were \\(\\hat{l}_0\\) = 39.5% of GDP at period 0 (1960). \\(\\hat{\\alpha}\\) = 0.840, a high weight on recent values. Check the model assumptions with residuals plots. gg_tsresiduals(ses_fit) Residual heteroscedasticity compromises prediction intervals. The innovation residuals vs time plot shows no heteroscedasticity.5 It does show a potential outlier around 1962 which might be a concern. Autocorrelation increases prediction intervals. The autocorrelation function plot shows a barely significant negative spike at lag 12 years. Non-normal residuals also compromise prediction intervals. The residual distribution in the histogram is slightly left-skewed. Use the fitted model to forecast the response variable for five periods. hilo() attaches a prediction interval to the tsibble. ses_fc &lt;- ses_fit %&gt;% forecast(h = 5) %&gt;% hilo(80) %&gt;% mutate( lpi = map_dbl(`80%`, ~.$lower), upi = map_dbl(`80%`, ~.$upper) ) ses_fit %&gt;% augment() %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = Exports)) + geom_line(aes(y = .fitted), color = &quot;goldenrod&quot;) + geom_line(data = ses_fc, aes(y = .mean), color = &quot;goldenrod&quot;) + geom_ribbon(data = ses_fc, aes(ymin = lpi, ymax = upi), alpha = 0.2, fill = &quot;goldenrod&quot;) + labs(title = &quot;Simple Exponential Smoothing, ETS(A, N, N)&quot;) 4.2 Holt Linear Holt’s linear method extends SES with a trend component, \\(b_t\\). \\[ \\begin{align} \\hat{y}_{t+h|t} &amp;= l_t + hb_t \\\\ l_t &amp;= \\alpha y_t + (1 - \\alpha)(l_{t-1} + hb_{t-1}) \\\\ b_t &amp;= \\beta^*(l_t - l_{t-1}) + (1 - \\beta^*)b_{t-1} \\end{align} \\] The level equation, \\(l_t\\), is like SES except for a trend adjustment. The trend equation, \\(b_t\\), describes how the slope changes over time. The parameter \\(\\beta^*\\) describes how quickly the slope can change. Now there are four parameter to estimate, \\(\\alpha\\), \\(l_0\\), \\(\\beta^*\\), and \\(b_0\\). Example Data set tsibbledata::global_economy contains annual country-level economic indicators, including Population size. This time series has a trend, so it is a good candidate for Holt’s linear trend method. tsibbledata::global_economy %&gt;% filter(Country == &quot;Australia&quot;) %&gt;% ggplot(aes(x = Year, y = Population)) + geom_line() + labs(title = &quot;Australian Population.&quot;) Fit the model with ETS() specifying an additive trend, ETS(A, A, N). holt_fit &lt;- tsibbledata::global_economy %&gt;% filter(Country == &quot;Australia&quot;) %&gt;% model(ETS(Population ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;N&quot;))) holt_fit %&gt;% report() ## Series: Population ## Model: ETS(A,A,N) ## Smoothing parameters: ## alpha = 0.9998992 ## beta = 0.3257153 ## ## Initial states: ## l[0] b[0] ## 10067191 228012.5 ## ## sigma^2: 4139605871 ## ## AIC AICc BIC ## 1525.705 1526.859 1536.008 \\(\\hat{l}_0\\) = 10,067,191 people at period 0 (1960). \\(\\alpha\\) is high when the trend increases rapidly, assigning more weight to recent values. \\(\\hat{\\alpha}\\) = 0.9999, a very high weighting. Population initially grows at \\(\\beta_0\\) = 228,013 people per year. \\(\\hat{\\beta}\\) = 0.326, a fairly large value, meaning the trend changes often. Check the model assumptions with residuals plots. There is no heteroscedasticity in the residuals vs time plot and no skew in the residual distribution plot, so the prediction intervals are reliable. The autocorrelation plot has no significant spikes, so the prediction intervals will not be unduly large. gg_tsresiduals(holt_fit) Use the fitted model to forecast the response variable for ten periods. holt_fc &lt;- holt_fit %&gt;% forecast(h = 10) %&gt;% hilo(80) %&gt;% mutate( lpi = map_dbl(`80%`, ~.$lower), upi = map_dbl(`80%`, ~.$upper) ) holt_fit %&gt;% augment() %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = Population)) + geom_line(aes(y = .fitted), color = &quot;goldenrod&quot;) + geom_line(data = holt_fc, aes(y = .mean), color = &quot;goldenrod&quot;) + geom_ribbon(data = holt_fc, aes(ymin = lpi, ymax = upi), alpha = 0.2, fill = &quot;goldenrod&quot;) + labs(title = &quot;Holt&#39;s Linear Method, ETS(A, A, N)&quot;) 4.3 Additive Damped Trend Holt’s linear trend produces a sloped, but straight, line. Research shows that constant trends tend to overshoot. The additive damped trend model introduces a damping parameter, \\(\\phi\\), to reduce the forecasted trend to a flat line over time. The forecast equation replaces \\(h\\) with the series \\(\\phi^1 + \\phi^2 + \\cdots + \\phi^h\\). The trend equation adds \\(\\phi\\) as a multiplier to the second term. \\[ \\begin{align} \\hat{y}_{t+h|t} &amp;= l_t + (\\phi^1 + \\phi^2 + \\cdots + \\phi^h)b_t \\\\ l_t &amp;= \\alpha y_t + (1 - \\alpha)(l_{t-1} + \\phi b_{t-1}) \\\\ b_t &amp;= \\beta^*(l_t - l_{t-1}) + (1 - \\beta^*) \\phi b_{t-1} \\end{align} \\] Now there are five parameters to estimate, \\(\\alpha\\), \\(\\beta^*\\), \\(l_0\\), \\(b_0\\), and \\(\\phi\\) (although you can supply a \\(\\phi\\) value to the trend() equation. Expect \\(\\phi\\) to between .8 and .998. Example Return to the Australian population data and include an additive damped trend model in a fit. The new model is an ETS(A, Ad, N). dholt_fit &lt;- tsibbledata::global_economy %&gt;% filter(Country == &quot;Australia&quot;) %&gt;% model( `Holt&#39;s Linear` = ETS(Population ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;N&quot;)), `Damped Holt&#39;s Linear` = ETS(Population ~ error(&quot;A&quot;) + trend(&quot;Ad&quot;) + season(&quot;N&quot;)) ) # Just report on the new model. dholt_fit %&gt;% select(`Damped Holt&#39;s Linear`) %&gt;% report() ## Series: Population ## Model: ETS(A,Ad,N) ## Smoothing parameters: ## alpha = 0.9998988 ## beta = 0.4392868 ## phi = 0.98 ## ## Initial states: ## l[0] b[0] ## 10067191 277729.2 ## ## sigma^2: 4584582964 ## ## AIC AICc BIC ## 1532.543 1534.190 1544.906 \\(\\hat{\\phi}\\) = 0.980, a modest amount of damping. \\(\\hat{\\beta}\\) increased from 0.326 in the linear model to 0.439 here, so the slope is changing more frequently. dholt_fc &lt;- dholt_fit %&gt;% forecast(h = 10) %&gt;% hilo(80) %&gt;% mutate( lpi = map_dbl(`80%`, ~.$lower), upi = map_dbl(`80%`, ~.$upper) ) palette_dholt &lt;- c(`Holt&#39;s Linear` = &quot;goldenrod&quot;, `Damped Holt&#39;s Linear` = &quot;seagreen&quot;) dholt_fit %&gt;% augment() %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = Population)) + geom_line(aes(y = .fitted, color = .model)) + geom_line(data = dholt_fc, aes(y = .mean, color = .model)) + geom_ribbon(data = dholt_fc, aes(ymin = lpi, ymax = upi, color = .model, fill = .model), alpha = 0.2) + scale_fill_manual(values = palette_dholt) + scale_color_manual(values = palette_dholt) + theme(legend.position = &quot;top&quot;) + labs(color = NULL, fill = NULL, title = &quot;Holt&#39;s Linear Method, ETS(A, A, N), and Additive Damped, ETS(A, Ad, N)&quot;) 4.4 Holt-Winters The Holt-Winters method extends Holt’s method with a seasonality component, \\(s_t\\), for \\(m\\) seasons per period. There are two versions of this model, the additive and the multiplicative. The additive method assumes the error variance is constant, and the seasonal component sums to approximately zero over the course of the year. The multiplicative version assumes the error variance scales with the level, and the seasonal component sums to approximately \\(m\\) over the course of the year. Additive Holt-Winters introduces the seasonality component as an additive element. \\[ \\begin{align} \\hat{y}_{t+h|t} &amp;= l_t + hb_t + s_{t+h-m(k+1)} \\\\ l_t &amp;= \\alpha(y_t - s_{t-m}) + (1 - \\alpha)(l_{t-1} + b_{t-1}) \\\\ b_t &amp;= \\beta^*(l_t - l_{t-1}) + (1 - \\beta^*)b_{t-1} \\\\ s_t &amp;= \\gamma(y_t - l_{t-1} - b_{t-1}) + (1 - \\gamma)s_{t-m} \\end{align} \\] \\(k\\) is the modulus of \\((h - 1) / m\\), so \\(s_{t+h-m(k+1)}\\) is always based on the prior seasonal period. \\(l_t\\) is a weighted average (\\(\\alpha\\) weighting) between the seasonally adjusted observation and the non-seasonal forecast. The trend component is unchanged. The seasonal component is a weighted average (\\(\\gamma\\) weighting) between the current seasonal index and the same season of the prior season period. The seasonality averages to one in multiplicative Holt-Winters. Use the multiplicative method if the seasonal variation increases with the level. \\[ \\begin{align} \\hat{y}_{t+h|t} &amp;= (l_t + hb_t) s_{t+h-m(k+1)} \\\\ l_t &amp;= \\alpha\\frac{y_t}{s_{t-m}} + (1 - \\alpha)(l_{t-1} + b_{t-1}) \\\\ b_t &amp;= \\beta^*(l_t - l_{t-1}) + (1-\\beta*)b_{t-1} \\\\ s_t &amp;= \\gamma\\frac{y_t}{(l_{t-1} - b_{t-1})} + (1 - \\gamma)s_{t-m} \\\\ \\end{align} \\] Now there are five smoothing parameters to estimate: \\(\\alpha\\), \\(l_0\\), \\(\\beta^*\\), \\(b_0\\), and \\(\\gamma\\), plus an initial value for each season of the seasonal period. Example Data set tsibble::tourism contains quarterly domestic tourist visit-nights in Australia. It’s not clear whether the error variance increases with the series level, so either the additive or the multiplicative method may be appropriate. tsibble::tourism %&gt;% filter(Purpose == &quot;Holiday&quot;) %&gt;% summarize(Trips = sum(Trips) / 1000) %&gt;% ggplot(aes(x = Quarter, y = Trips)) + geom_line() + labs(title = &quot;Australian Domestic Tourist Visit-Nights&quot;) Fit the model with ETS() specifying additive and multiplicative seasonality and error, ETS(A, A, A) and ETS(M, A, M). hw_fit &lt;- tsibble::tourism %&gt;% filter(Purpose == &quot;Holiday&quot;) %&gt;% summarize(Trips = sum(Trips) / 1000) %&gt;% model( `Additive Holt-Winters` = ETS(Trips ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;)), `Multiplicative Holt-Winters` = ETS(Trips ~ error(&quot;M&quot;) + trend(&quot;A&quot;) + season(&quot;M&quot;)) ) hw_fit %&gt;% tidy() %&gt;% pivot_wider(names_from = .model, values_from = estimate) ## # A tibble: 9 × 3 ## term `Additive Holt-Winters` `Multiplicative Holt-Winters` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alpha 0.262 0.224 ## 2 beta 0.0431 0.0304 ## 3 gamma 0.000100 0.000100 ## 4 l[0] 9.79 10.0 ## 5 b[0] 0.0211 -0.0114 ## 6 s[0] -0.534 0.943 ## 7 s[-1] -0.670 0.927 ## 8 s[-2] -0.294 0.969 ## 9 s[-3] 1.50 1.16 Notice that the seasonal component estimates, s[0] to s[-3], sum to ~0 for the additive model and ~4 for the multiplicative model, the number of seasons in the seasonal period. The small \\(\\hat{\\gamma}\\) values mean the seasonal component hardly changes over time. The small \\(\\hat{\\beta}\\) values mean the slope component hardly changes over time. You can see this more clearly in the fit decomposition (note the differing vertical scales for the slope and level components). hw_fit %&gt;% components() %&gt;% pivot_longer(cols = Trips:remainder) %&gt;% mutate(name = factor(name, levels = c(&quot;Trips&quot;, &quot;level&quot;, &quot;slope&quot;, &quot;season&quot;, &quot;remainder&quot;))) %&gt;% ggplot(aes(x = Quarter, y = value)) + geom_line() + facet_grid(rows = vars(name), cols = vars(.model), scales = &quot;free_y&quot;) + labs(x = NULL, y = NULL, title = &quot;Holt-Winters Fit Decomposition&quot;) The model fit metrics suggest Multiplicative Holt-Winters is the better model. It has lower AIC and MSE. hw_fit %&gt;% glance() ## # A tibble: 2 × 9 ## .model sigma2 log_lik AIC AICc BIC MSE AMSE MAE ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Additive Holt-Winters 0.193 -105. 229. 231. 250. 0.174 0.184 0.321 ## 2 Multiplicative Holt-Wint… 0.00212 -104. 227. 229. 248. 0.170 0.183 0.0328 Check the model assumptions of the multiplicative model. The residuals plot shows some heteroscedasticity in the middle periods, and higher variance the latter years. The histogram shows a normal distribution. The autocorrelation function (ACF) plot shows a single significant spike at t14. The Ljung-Box test fails to reject the null hypothesis of no autocorrelation of the residuals (p = 0.504). # Can&#39;t use an object containing multiple fits, so re-fit just the multiplicative model. hwm_fit &lt;- tsibble::tourism %&gt;% filter(Purpose == &quot;Holiday&quot;) %&gt;% summarize(Trips = sum(Trips) / 1000) %&gt;% model(ETS(Trips ~ error(&quot;M&quot;) + trend(&quot;A&quot;) + season(&quot;M&quot;))) hwm_fit %&gt;% gg_tsresiduals() hwm_fit %&gt;% augment() %&gt;% features(.var = .innov, features = ljung_box, lag = 14) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;ETS(Trips ~ error(\\&quot;M\\&quot;) + trend(\\&quot;A\\&quot;) + season(\\&quot;M\\&quot;))&quot; 13.3 0.504 Use the fitted model to forecast the response variable for ten periods. hw_fc &lt;- hw_fit %&gt;% forecast(h = 10) %&gt;% hilo(80) %&gt;% mutate( lpi = map_dbl(`80%`, ~.$lower), upi = map_dbl(`80%`, ~.$upper) ) palette_hw &lt;- c(`Additive Holt-Winters` = &quot;goldenrod&quot;, `Multiplicative Holt-Winters` = &quot;seagreen&quot;) hw_fit %&gt;% augment() %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Trips)) + geom_line(aes(y = .fitted, color = .model)) + geom_line(data = hw_fc, aes(y = .mean, color = .model)) + geom_ribbon(data = hw_fc, aes(ymin = lpi, ymax = upi, color = .model, fill = .model), alpha = 0.2) + scale_fill_manual(values = palette_hw) + scale_color_manual(values = palette_hw) + theme(legend.position = &quot;top&quot;) + labs(color = NULL, fill = NULL, title = &quot;Holt Winters Additive, ETS(A, A, A), and Multiplicative, ETS(M, A, M)&quot;) 4.5 Auto-fitting If you specify an ETS model with no parameters, it will use maximum likelihood to select the model with the minimum AICc. auto_fit &lt;- tsibble::tourism %&gt;% filter(Purpose == &quot;Holiday&quot;) %&gt;% summarize(Trips = sum(Trips) / 1000) %&gt;% model(ETS(Trips)) auto_fit %&gt;% report() ## Series: Trips ## Model: ETS(M,N,A) ## Smoothing parameters: ## alpha = 0.3484054 ## gamma = 0.0001000018 ## ## Initial states: ## l[0] s[0] s[-1] s[-2] s[-3] ## 9.727072 -0.5376106 -0.6884343 -0.2933663 1.519411 ## ## sigma^2: 0.0022 ## ## AIC AICc BIC ## 226.2289 227.7845 242.9031 ETS chose a multiplicative error, non-trended, additive error model. Compare this model fit with the two Holt-Winters fits from the previous section. The autofit had the largest MSE, but lowest AICc. bind_rows( hw_fit %&gt;% glance(), auto_fit %&gt;% glance() ) ## # A tibble: 3 × 9 ## .model sigma2 log_lik AIC AICc BIC MSE AMSE MAE ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Additive Holt-Winters 0.193 -105. 229. 231. 250. 0.174 0.184 0.321 ## 2 Multiplicative Holt-Wint… 0.00212 -104. 227. 229. 248. 0.170 0.183 0.0328 ## 3 ETS(Trips) 0.00215 -106. 226. 228. 243. 0.183 0.197 0.0347 The trend can also be multiplicative (M) or multiplicative damped (Md), but Hyndman explains that they do not produce good forecasts.↩︎ Innovation residuals are residuals on the transformed scale if the outcome variable was transformed.↩︎ "],["arima.html", "Chapter 5 ARIMA 5.1 Stationary Time Series 5.2 Autoregressive: AR(p) 5.3 Moving Average: MA(q) 5.4 Non-Seasonal: ARIMA(p, d, q) 5.5 Seasonal: ARIMA(p, d, q)(P, D, Q)m 5.6 Fitting an ARIMA Model", " Chapter 5 ARIMA Whereas exponential smoothing models describe trend and seasonality, ARIMA models describe autocorrelations. An autoregressive, AR(p), model is a multiple regression with p lagged observations as predictors. \\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t\\] A moving average, MA(q), model is a multiple regression with q lagged errors as predictors. \\[y_t = c + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_p \\epsilon_{t-q}\\] An autoregressive moving average, ARMA(p, q), model is a multiple regression with p lagged observations and q lagged errors as predictors. \\[y_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\theta_1 y_{t-1} + \\dots + \\theta_p y_{t-q} + \\epsilon_t\\] An ARMA model with differencing, ARIMA(p,d,q), model is an ARMA model with d levels of differencing. Whereas exponential smoothing models can handle non-constant variance with multiplicative errors and seasonality, autoregressive models require that you explicitly transform the data into a stationary time series. A stationary time series is one in whose statistical properties do not depend on the time at which the series is observed: it has no trend, is homoscedastic, and has no seasonality. Stationary time series can have cyclical patterns, but no regular long term patterns. ARIMA analyses consists of making the data stationary, fitting the model(s), reversing the stationarity, then generating forecasts. 5.1 Stationary Time Series The first step is to create a stationary time series. This typically involves a transformation of the response variable to remove heteroscedasticity, seasonal differencing to remove seasonality, and one-period differencing to remove trend. Let’s work with the fpp::usmelec data set of us monthly electricity generation. Fig. 5.1 shows that this data has trend and non-constant variance. elec &lt;- fpp::usmelec %&gt;% as_tsibble() elec %&gt;% as_tsibble() %&gt;% ggplot(aes(x = index, y = value)) + geom_line() + labs(title = &quot;US Electricity Generation&quot;, y = &quot;billion kWh&quot;, x = &quot;Month&quot;) Figure 5.1: Data series with trend and non-constant variance. There are an infinite number of transformations, but the common ones (in increasing strength) are: square root, cube root, log, and inverse. Fig. 5.2 shows several transformations of the elec data set. Hyndman seems to stick with the log and that seems to work here. elec %&gt;% as_tsibble() %&gt;% mutate(sqrt = value^.5, cubert = value^(1/3), log = log(value), inv = 1/value) %&gt;% rename(y = value) %&gt;% pivot_longer(-index) %&gt;% ggplot(aes(x = index, y = value)) + geom_line() + facet_wrap(vars(name), scales = &quot;free_y&quot;) + labs(title = &quot;Common Transformations to Remove Heterscedasticity&quot;, x = &quot;Month&quot;, y = &quot;billion kWh transformed&quot;) Figure 5.2: Standard transformations may under- or over-shoot constant variance. The Box-Cox transformation can find the optimal transformation (Fig. 5.3). lambda &lt;- features(elec, value, features = guerrero) %&gt;% pull(lambda_guerrero) elec2 &lt;- elec %&gt;% mutate(value = box_cox(value, lambda)) elec2 %&gt;% ggplot(aes(x = index, y = value)) + geom_line() + theme_light() + labs(title = glue(&quot;Box-Cox Transformation, lambda = {number(lambda, .001)}.&quot;), x = &quot;Month&quot;, y = &quot;billion kWh transformed&quot;) Figure 5.3: Box-Cox transformation produces constant variance. After transforming the response variable, use differencing of successive observations to stabilize the level. In the case of seasonal data, take a seasonal difference (12 observation difference for monthly data), then possibly take a second one-observation difference (Fig. 5.4). x1 &lt;- elec2 %&gt;% rename(`Non-Transformed` = value) %&gt;% mutate( `Seasonal Difference` = difference(`Non-Transformed`, 12), `1-period Difference` = difference(`Seasonal Difference`, 1) ) x2 &lt;- bind_rows( `Non-Transformed` = x1 %&gt;% feasts::ACF(`Non-Transformed`) %&gt;% as_tibble(), `Seasonal Difference` = x1 %&gt;% feasts::ACF(`Seasonal Difference`) %&gt;% as_tibble(), `1-period Difference` = x1 %&gt;% feasts::ACF(`1-period Difference`) %&gt;% as_tibble(), .id = &quot;transformation&quot; ) %&gt;% mutate(transformation = fct_inorder(transformation)) series_n &lt;- x1 %&gt;% as_tibble() %&gt;% pivot_longer(-index, names_to = &quot;transformation&quot;) %&gt;% filter(!is.na(value)) %&gt;% mutate(transformation = factor(transformation, levels = levels(x2$transformation))) %&gt;% summarize(.by = transformation, n = n()) x2 %&gt;% ggplot() + geom_segment(aes(x = lag, xend = lag, y = 0, yend = acf), color = &quot;steelblue&quot;) + geom_point(aes(x = lag, y = acf), size = 2, color = &quot;steelblue&quot;) + geom_hline(yintercept = 0, color = &quot;gray20&quot;) + geom_hline(data = series_n, aes(yintercept = qnorm(.975) / sqrt(n)), linetype = 2, color = &quot;goldenrod&quot;) + geom_hline(data = series_n, aes(yintercept = qnorm(.025) / sqrt(n)), linetype = 2, color = &quot;goldenrod&quot;) + scale_y_continuous(limits = c(-.2, 1.0)) + theme(panel.grid = element_blank()) + labs(y = &quot;ACF&quot;, title = &quot;ACF before and after differencing.&quot;) + facet_wrap(facets = vars(transformation), nrow = 1) Figure 5.4: ACF before and after differencing. perform a Ljung-Box test on the transformed data. elec3 &lt;- elec2 %&gt;% mutate( value = difference(value, 12), value = difference(value, 1) ) (elec_jung &lt;- elec3 %&gt;% features(value, ljung_box, lag = 10)) ## # A tibble: 1 × 2 ## lb_stat lb_pvalue ## &lt;dbl&gt; &lt;dbl&gt; ## 1 72.9 1.20e-11 The ACF of the twice-differenced values is white noise (Ljung-Box Q = 72.94, p = 0.000, Fig. 5.5). elec3 %&gt;% ggplot(aes(x = index, y = value)) + geom_line() + theme_light() + labs(title = glue(&quot;US Electricity Generation, Stationary.&quot;), x = &quot;Month&quot;, y = &quot;billion kWh&quot;) Figure 5.5: Box-Cox transformation plus double-differencing produces stationary time series with constant variance. 5.2 Autoregressive: AR(p) Autoregressive models, AR(p), forecast the response variable as a linear combination of past observations. \\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t\\] where p is the order of regression and \\(\\epsilon_t\\) is white noise. \\(\\phi_k\\) is the correlation between \\(y_t\\) and \\(y_{t-k}\\) after removing the effects of lags \\(1, \\ldots, k-1\\). If \\(\\phi_1\\) = 0 and c = 0, \\(y_t\\) is white noise. If \\(\\phi_1\\) = 1 and c = 0, \\(y_t\\) is a random walk. If \\(\\phi_1\\) = 1 and c != 0, \\(y_t\\) is a random walk with drift. If \\(\\phi_1\\) &lt; 0, \\(y_t\\) oscillates around the mean. 5.3 Moving Average: MA(q) Moving average models, MA(q), forecast the response variable as a linear combination of q lagged errors. \\[y_t = c + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_p \\epsilon_{t-q}\\] where \\(\\epsilon_t\\) is white noise and q is the order of the regression. The model is denoted AR(q). 5.4 Non-Seasonal: ARIMA(p, d, q) An autoregressive integrated moving average (ARIMA) model combines differencing with the AR and MA model. In this context integration is the opposite of differencing. \\[y&#39;_t = c + \\phi_1 y&#39;_{t-1} + \\dots + \\phi_p y&#39;_{t-p} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_p \\epsilon_{t-q} + \\epsilon_t\\] where \\(y&#39;_t\\) is the differenced series (possibly multiple differences) and d is the order of differencing. The model is denoted ARIMA(p, d, q). 5.5 Seasonal: ARIMA(p, d, q)(P, D, Q)m A seasonal ARIMA model is formed by including additional seasonal components. 5.6 Fitting an ARIMA Model Fit an ARIMA model to non-seasonal time series data with the following steps: Plot the data. Identify unusual observations, seasonality, and/or heteroscedasticity. Transform the data with a Box-Cox transformation to stabilize the variance (if necessary). Take one (or at most two) first differences to make the data stationary (if necessary). Plot the ACF/PACF to decide whether an ARIMA(p, d, 0) (autoregressive) or ARIMA(0, d, q) (moving average) model is appropriate. Use the AICc to search for the best model. Verify the residuals are white noise by plotting the ACF of the residuals and performing a portmanteau test of the residuals. Forecast future values. ARIMA() does steps 3-5 automatically. Use the model with the minimum Akaike’s Information Criterion (AIC or AICc) or Bayesian Information Criterion (BIC) to choose the order of an ARIMA model (p and q). \\[AIC = −2 \\log(L) + 2(p + q + k + 1)\\] where \\(L\\) is the likelihood of the data, \\(k = 1\\) if \\(c \\ne 0\\) and 0 if \\(c = 0\\). \\[AICc = AIC + \\frac{2(p + q + k + 1)(p + q + k + 2)}{T - p - q - k - 2}\\] \\[BIC = AIC + (\\log(T) - 2)(p + q + k + 1)\\] fable uses maximum likelihood estimation (MLE) to find model parameter values which maximize the probability of obtaining the observed data. In practice, fable maximizes the log likelihood. Returning to the fpp::usmelec data set, recall from Fig. 5.1 that the data was non-stationary: it had heteroscedasticity, seasonality, and trending. A Box-Cox transformation corrected the heteroscedasticity. A seasonal difference and one-period difference corrected the seasonality and trending. We can assess the differencing portion with a diagnostic plot. I’ll repeat the Box-Cox part below for completeness. lambda &lt;- features(elec, value, features = guerrero) %&gt;% pull(lambda_guerrero) elec2 &lt;- elec %&gt;% mutate(value = box_cox(value, lambda)) elec2 %&gt;% gg_tsdisplay(difference(value, 12) %&gt;% difference(), plot_type = &quot;partial&quot;, lag = 36) + labs(title = &quot;Double-Differenced&quot;, y = NULL) Figure 5.6: Double-differenced fpp::usmelec data with ACF and PACF plots. The seasonal part of an AR or MA model is revealed in the seasonal lags of the PACF and ACF. There is a spike in the ACF at lag 12. The PACF has decaying spikes at 12, 24, and 36. The significant spike at lag 2 in the ACF suggests a non-seasonal MA(2) component. The significant spike at lag 12 suggests a seasonal MA(1). Model this as ARIMA(0,1,2)(0,1,1)12. The significant spike at lag 2 in the PACF suggests a non-seasonal AR(2) component. Model this as ARIMA(2,1,0)(0,1,1)12. Let’s try them both. By not specifying p, d, and q, ARIMA() will automatically perform steps 3-5 above to find the best values. If you want a more complete models space, specify stepwise = FALSE. arima_fit &lt;- elec2 %&gt;% model( arima_012_011 = ARIMA(value ~ pdq(0,1,2) + PDQ(0,1,1)), arima_210_011 = ARIMA(value ~ pdq(2,1,0) + PDQ(0,1,1)), arima_stepwise = ARIMA(value), arima_gridsearch = ARIMA(value, stepwise = FALSE) ) arima_fit %&gt;% pivot_longer(everything()) ## # A mable: 4 x 2 ## # Key: name [4] ## name value ## &lt;chr&gt; &lt;model&gt; ## 1 arima_012_011 &lt;ARIMA(0,1,2)(0,1,1)[12]&gt; ## 2 arima_210_011 &lt;ARIMA(2,1,0)(0,1,1)[12]&gt; ## 3 arima_stepwise &lt;ARIMA(1,1,1)(2,1,1)[12]&gt; ## 4 arima_gridsearch &lt;ARIMA(1,1,1)(2,1,1)[12]&gt; ARIMA() optimized to ARIMA(1,1,1)(2,1,1)12. The AICc measures are close, but the stepwise/gridsearch model have lower values. arima_fit %&gt;% glance() ## # A tibble: 4 × 8 ## .model sigma2 log_lik AIC AICc BIC ar_roots ma_roots ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 arima_012_011 0.00000369 2133. -4257. -4257. -4241. &lt;cpl [0]&gt; &lt;cpl [14]&gt; ## 2 arima_210_011 0.00000394 2118. -4228. -4227. -4211. &lt;cpl [2]&gt; &lt;cpl [12]&gt; ## 3 arima_stepwise 0.00000365 2135. -4259. -4259. -4234. &lt;cpl [25]&gt; &lt;cpl [13]&gt; ## 4 arima_gridsearch 0.00000365 2135. -4259. -4259. -4234. &lt;cpl [25]&gt; &lt;cpl [13]&gt; The ACF plot for the stepwise/gridsearch model shows that all autocorrelations up to lag 12 are within the threshold limits. arima_fit %&gt;% select(arima_stepwise) %&gt;% gg_tsresiduals() A portmanteau test suggests the residuals are indeed white noise. arima_fit %&gt;% augment() %&gt;% filter(.model == &quot;arima_stepwise&quot;) %&gt;% features(.innov, ljung_box, lag = 36, dof = 6) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 arima_stepwise 35.8 0.213 arima_111_211_fc &lt;- arima_fit %&gt;% select(arima_stepwise) %&gt;% forecast(h = 12) %&gt;% hilo(80) %&gt;% mutate( lpi = map_dbl(`80%`, ~.$lower), upi = map_dbl(`80%`, ~.$upper) ) arima_fit %&gt;% augment() %&gt;% filter(.model == &quot;arima_stepwise&quot;) %&gt;% ggplot(aes(x = index)) + geom_line(aes(y = value)) + geom_line(data = arima_111_211_fc, aes(y = .mean)) + geom_ribbon(data = arima_111_211_fc, aes(ymin = lpi, ymax = upi), color = &quot;goldenrod&quot;, fill = &quot;lightgoldenrod&quot;) + labs(color = NULL, fill = NULL, title = &quot;ARIMA(1,1,1)(2,1,1)_12&quot;) R function auto.arima() from the forecast package chooses the optimal ARIMA model parameters using the Akaike criterion.6 Use a unit root test to verify stationarity. (elec3_unitroot &lt;- elec3 %&gt;% features(value, unitroot_kpss)) ## # A tibble: 1 × 2 ## kpss_stat kpss_pvalue ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0159 0.1 The p-value is reported as .1 if it is greater than .1. In this case the test statistic of 0.0159 is smaller than the 1% critical value so the p-value is greater than .1. The null hypothesis is not rejected, meaning the data is assumed stationary. You can use the Akaike criterion to compare models of the same class, but not different models, so do not use it to compare an ARIMA model to an ETS model. You cannot use the Akaike criterion for models of different levels of differencing.↩︎ "],["dynamic.html", "Chapter 6 Dynamic Harmonic Regression 6.1 TBATS Model", " Chapter 6 Dynamic Harmonic Regression Dynamic harmonic regression is based on the principal that a combination of sine and cosine funtions can approximate any periodic function. \\[y_t = \\beta_0 + \\sum_{k=1}^{K}[\\alpha_k s_k(t) + \\gamma_k c_k(t)] + \\epsilon_t\\] where \\(s_k(t) = sin(\\frac{2\\pi k t}{m})\\) and \\(c_k(t) = cos(\\frac{2\\pi k t}{m})\\), \\(m\\) is the seasonal period, \\(\\alpha_k\\) and \\(\\gamma_k\\) are regression coefficients, and \\(\\epsilon_t\\) is modeled as a non-seasonal ARIMA process. The optimal model has the lowest AICc, so start with K=1 and increase until the AICc is no longer decreasing. K cannot be greater than \\(m/2\\). With weekly data, it is difficult to handle seasonality using ETS or ARIMA models as the seasonal length is too large (approximately 52). Instead, you can use harmonic regression which uses sines and cosines to model the seasonality. The fourier() function makes it easy to generate the required harmonics. The higher the order (K), the more “wiggly” the seasonal pattern is allowed to be. With K=1, it is a simple sine curve. You can select the value of K by minimizing the AICc value. Function fourier() takes in a required time series, required number of Fourier terms to generate, and optional number of rows it needs to forecast. # # Set up harmonic regressors of order 13 # harmonics &lt;- fourier(gasoline, K = 13) # # # Fit a dynamic regression model to fit. Set xreg equal to harmonics and seasonal to FALSE because seasonality is handled by the regressors. # fit &lt;- auto.arima(gasoline, xreg = harmonics, seasonal = FALSE) # # # Forecasts next 3 years # newharmonics &lt;- fourier(gasoline, K = 13, h = 3*52) # fc &lt;- forecast(fit, xreg = newharmonics) # # # Plot forecasts fc # autoplot(fc) Harmonic regressions are also useful when time series have multiple seasonal patterns. For example, taylor contains half-hourly electricity demand in England and Wales over a few months in the year 2000. The seasonal periods are 48 (daily seasonality) and 7 x 48 = 336 (weekly seasonality). There is not enough data to consider annual seasonality. # # Fit a harmonic regression using order 10 for each type of seasonality # fit &lt;- tslm(taylor ~ fourier(taylor, K = c(10, 10))) # # # Forecast 20 working days ahead # fc &lt;- forecast(fit, newdata = data.frame(fourier(taylor, K = c(10, 10), h = 20*48))) # # # Plot the forecasts # autoplot(fc) # # # Check the residuals of fit # checkresiduals(fit) Another time series with multiple seasonal periods is calls, which contains 20 consecutive days of 5-minute call volume data for a large North American bank. There are 169 5-minute periods in a working day, and so the weekly seasonal frequency is 5 x 169 = 845. The weekly seasonality is relatively weak, so here you will just model daily seasonality. The residuals in this case still fail the white noise tests, but their autocorrelations are tiny, even though they are significant. This is because the series is so long. It is often unrealistic to have residuals that pass the tests for such long series. The effect of the remaining correlations on the forecasts will be negligible. # # Plot the calls data # autoplot(calls) # # # Set up the xreg matrix # xreg &lt;- fourier(calls, K = c(10, 0)) # # # Fit a dynamic regression model # fit &lt;- auto.arima(calls, xreg = xreg, seasonal = FALSE, stationary = TRUE) # # # Check the residuals # checkresiduals(fit) # # # Plot forecasts for 10 working days ahead # fc &lt;- forecast(fit, xreg = fourier(calls, c(10, 0), h = 169*8)) # autoplot(fc) 6.1 TBATS Model Thte TBATS model (Trigonometric terms for seasonality, Box-Cox transformations for hetergeneity, ARMA errors for short-term dynamics, Trend (possibly damped), and Seasonal (including multiple and non-integer periods)). # gasoline %&gt;% tbats() %&gt;% forecast() %&gt;% autoplot() TBATS is easy to use, but the prediction intervals are often too wide, and it can be quite slow with large time series. TBATS returns output similar to this: TBATS(1, {0,0}, -, {&lt;51.18,14&gt;}), meaning 1=Box-Cox parameter, {0,0} = ARMA error, - = damping parameter, {&lt;51.18,14&gt;} = seasonal period and Fourier terms. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
