[["index.html", "Time Series Analysis Preface", " Time Series Analysis Michael Foley 2023-08-28 Preface These notes are based on the Time Series with R skill track at DataCamp and Rob Hyndman’s Forecasting: Principles and Practice (Rob J Hyndman 2021). I organized them into a section on working with a tsibble (time series tibble) (chapter 1), a section on data exploration (chapter 2), and then four sections on models. Forecasts aren’t necessarily based on time series models - you can perform a cross-sectional regression analysis of features, possibly including time-related features such as month of year (chapter 3). Time series forecasts are a specific type of forecast based, at least in part, on the assumption that future outcomes are functionally dependent upon prior outcomes. In most cases the objective of a time series forecast is to project a time series. In these cases, the forecast either decomposes a time series into trend and seasonality components (exponential smoothing models, chapter 4) or describes the autocorrelation within the data (ARIMA models, chapter 5). There may also be cases where you include other predictor variables (dynamic models, chapter 6). In addition to the standard packages, these notes use the tsibble, feasts, fable, and tsibbledata packages. library(tidyverse) library(lubridate) library(patchwork) # arranging plots library(glue) library(tsibble) library(feasts) # feature extraction and statistics library(fable) # forecasting library(tsibbledata) References "],["toolbox.html", "Chapter 1 Toolbox 1.1 R Structures 1.2 Fitting Models 1.3 Evaluating Models 1.4 Evaluating Accuracy", " Chapter 1 Toolbox This section deals with fundamental concepts in time series analysis. Time series analysis typically uses different data structures than the standard data.frame and tibble of other analyses. The fable package provides a framework for piped “tidy” modeling that improves on the conventional lm(formula, data) framework. The subsequent modeling chapters make use of standard benchmarking methods, prediction intervals, and accuracy evaluation methods presented here. 1.1 R Structures Use a tsibble object to work with time series data. A tsibble, from the package of the same name, is a time-series tibble. Unlike the older, more common ts, zoo, and xts objects, a tsibble preserves the time index, making heterogeneous data structures possible. For example, you can re-index a tsibble from monthly to yearly analysis, or include one or more features per time element. Since the tsibble is relatively new, you will encounter the other frameworks and should at least be familiar with them. Let’s work with the prison_population.csv file accompanying Hyndman’s text to create time series structures with each framework. The data set is quarterly prison population counts grouped by several features. In essence, it is several time series (State x Gender x Legal x Indigenous) within one file. prison &lt;- readr::read_csv(&quot;https://OTexts.com/fpp3/extrafiles/prison_population.csv&quot;) head(prison) ## # A tibble: 6 × 6 ## Date State Gender Legal Indigenous Count ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2005-03-01 ACT Female Remanded ATSI 0 ## 2 2005-03-01 ACT Female Remanded Non-ATSI 2 ## 3 2005-03-01 ACT Female Sentenced ATSI 0 ## 4 2005-03-01 ACT Female Sentenced Non-ATSI 5 ## 5 2005-03-01 ACT Male Remanded ATSI 7 ## 6 2005-03-01 ACT Male Remanded Non-ATSI 58 dim(prison) ## [1] 3072 6 ts, zoo, and xts ts is the base R time series package. The ts object is essentially a matrix of observations indexed by a chronological identifier. Because it is a matrix, any descriptive attributes need to enter as numeric, perhaps by one-hot encoding, or pivoting the data (yuck). But since there is only one row per time observation, the descriptive attributes cannot really act as grouping variables. Another limitation of a ts is that it does not recognize irregularly spaced time series. Define a ts object with ts(x, start, frequency) where frequency is the number of observations in the seasonal pattern: 7 for daily observations with a week cycle; 5 for weekday observations in a week cycle; 24 for hourly observations in a day cycle, 24x7 for hourly observations in a week cycle, etc. prison is quarterly observations starting with 2005 Q1. Had the series started at 2005 Q2, you’d specify start = c(2005, 2). I’ll pull a single time series from the file with a filter() statement. prison_ts &lt;- prison %&gt;% filter(State == &quot;ACT&quot; &amp; Gender == &quot;Male&quot; &amp; Legal == &quot;Remanded&quot; &amp; Indigenous == &quot;ATSI&quot;) %&gt;% arrange(Date) %&gt;% select(Count) %&gt;% ts(start = 2005, frequency = 4) str(prison_ts) ## Time-Series [1:48, 1] from 2005 to 2017: 7 7 9 9 12 9 8 7 6 11 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;Count&quot; zoo (Zeileis’s ordered observations) provides methods similar to those in ts, but also supports irregular time series. A zoo object contains an array of data values and an index attribute to provide information about the data ordering. zoo was introduced in 2014. xts (extensible time series) extends zoo. xts objects are more flexible than ts objects while imposing reasonable constraints to make them truly time-based. An xts object is essentially a matrix of observations indexed by a time object. Create an xts object with xts(x, order.by) where order.by is a vector of dates/times to index the data. You can also add metadata to the xts object by declaring name-value pairs such as born = as.POSIXct(\"1899-05-08\"). library(xts) ## Warning: package &#39;xts&#39; was built under R version 4.3.1 ## Warning: package &#39;zoo&#39; was built under R version 4.3.1 x &lt;- prison %&gt;% filter(State == &quot;ACT&quot; &amp; Gender == &quot;Male&quot; &amp; Legal == &quot;Remanded&quot; &amp; Indigenous == &quot;ATSI&quot;) %&gt;% arrange(Date) prison_xts &lt;- xts(x$Count, order.by = x$Date, State = &quot;ACT&quot;, Gender = &quot;Male&quot;, Legal = &quot;Remanded&quot;, Indigenous = &quot;ATSI&quot;) str(prison_xts) ## An xts object on 2005-03-01 / 2016-12-01 containing: ## Data: double [48, 1] ## Index: Date [48] (TZ: &quot;UTC&quot;) ## xts Attributes: ## $ State : chr &quot;ACT&quot; ## $ Gender : chr &quot;Male&quot; ## $ Legal : chr &quot;Remanded&quot; ## $ Indigenous: chr &quot;ATSI&quot; tsibble A tsibble object is a tibble uniquely defined by key columns plus a date index column. This structure accommodates multiple series, and descriptive attribute columns. The date index can be a Date, period, etc. (see tsibble() help file). Express weekly time series with yearweek(), monthly time series with yearmonth(), or quarterly (like here) with yearquarter(). prison_tsibble &lt;- prison %&gt;% mutate(Date = yearquarter(Date)) %&gt;% rename(Qtr = Date) %&gt;% tsibble(key = c(State, Gender, Legal, Indigenous), index = Qtr) head(prison_tsibble) ## # A tsibble: 6 x 6 [1Q] ## # Key: State, Gender, Legal, Indigenous [1] ## Qtr State Gender Legal Indigenous Count ## &lt;qtr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2005 Q1 ACT Female Remanded ATSI 0 ## 2 2005 Q2 ACT Female Remanded ATSI 1 ## 3 2005 Q3 ACT Female Remanded ATSI 0 ## 4 2005 Q4 ACT Female Remanded ATSI 0 ## 5 2006 Q1 ACT Female Remanded ATSI 1 ## 6 2006 Q2 ACT Female Remanded ATSI 1 A tsibble behaves like a tibble, so you can use *tidyverse** verbs. The only thing that will trip you up is that tsibble objects are grouped by the index - group_by() operations only group non-index columns while retaining the index. Use index_by() if you need to summarize at a new time level (e.g., year). # Group by State retains the Qtr index column. prison_tsibble %&gt;% group_by(State) %&gt;% summarize(sum_Count = sum(Count)) ## # A tsibble: 384 x 3 [1Q] ## # Key: State [8] ## State Qtr sum_Count ## &lt;chr&gt; &lt;qtr&gt; &lt;dbl&gt; ## 1 ACT 2005 Q1 178 ## 2 ACT 2005 Q2 183 ## 3 ACT 2005 Q3 187 ## 4 ACT 2005 Q4 204 ## 5 ACT 2006 Q1 190 ## 6 ACT 2006 Q2 190 ## 7 ACT 2006 Q3 165 ## 8 ACT 2006 Q4 179 ## 9 ACT 2007 Q1 172 ## 10 ACT 2007 Q2 160 ## # ℹ 374 more rows # But you can change the Qtr index aggregation level with index_by() prison_tsibble %&gt;% index_by(Year = ~ year(.)) %&gt;% group_by(State) %&gt;% summarise(sum_Count = sum(Count)) ## # A tsibble: 96 x 3 [1Y] ## # Key: State [8] ## State Year sum_Count ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ACT 2005 752 ## 2 ACT 2006 724 ## 3 ACT 2007 673 ## 4 ACT 2008 697 ## 5 ACT 2009 668 ## 6 ACT 2010 842 ## 7 ACT 2011 946 ## 8 ACT 2012 1085 ## 9 ACT 2013 1146 ## 10 ACT 2014 1336 ## # ℹ 86 more rows # Add group_by_key() to retain the key prison_tsibble %&gt;% group_by_key() %&gt;% index_by(Year = ~ year(.)) %&gt;% summarize(sum_Count = sum(Count)) ## # A tsibble: 768 x 6 [1Y] ## # Key: State, Gender, Legal, Indigenous [64] ## # Groups: State, Gender, Legal [32] ## State Gender Legal Indigenous Year sum_Count ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ACT Female Remanded ATSI 2005 1 ## 2 ACT Female Remanded ATSI 2006 3 ## 3 ACT Female Remanded ATSI 2007 3 ## 4 ACT Female Remanded ATSI 2008 9 ## 5 ACT Female Remanded ATSI 2009 6 ## 6 ACT Female Remanded ATSI 2010 5 ## 7 ACT Female Remanded ATSI 2011 6 ## 8 ACT Female Remanded ATSI 2012 2 ## 9 ACT Female Remanded ATSI 2013 4 ## 10 ACT Female Remanded ATSI 2014 4 ## # ℹ 758 more rows # If you don&#39;t care about the time index, convert the tsibble back to a tibble. prison_tsibble %&gt;% as_tibble() %&gt;% group_by(State) %&gt;% summarize(sum_Count = sum(Count)) ## # A tibble: 8 × 2 ## State sum_Count ## &lt;chr&gt; &lt;dbl&gt; ## 1 ACT 12003 ## 2 NSW 491924 ## 3 NT 58193 ## 4 QLD 290794 ## 5 SA 101606 ## 6 TAS 24367 ## 7 VIC 234734 ## 8 WA 220256 1.2 Fitting Models Consider whether you are fitting a model for explanatory variable inference or for predictive purposes. If explanation is your goal, your workflow will be fitting a model, verifying the model assumptions related to inference, then summarizing the model parameters. If prediction is your goal, your workflow will be comparing multiple models by cross-validating the results against a hold-out data set, then making predictions. Fit a model using fabletools::model().1 You can even fit multiple models at once. Let’s fit a simple model using the GAFA stock prices data set in tsibbledata. GAFA is daily stock prices from 2014-2018 for several companies. We’ll work with Google. The data is indexed by date, but we’ll re-index to “trading day” since the trading days are irregularly spaced. goog &lt;- tsibbledata::gafa_stock %&gt;% filter(Symbol == &quot;GOOG&quot;, year(Date) &gt;= 2015) %&gt;% # re-index on trading day since markets not open on weekends, holidays arrange(Date) %&gt;% mutate(trading_day = row_number()) %&gt;% update_tsibble(index = trading_day, regular = TRUE) head(goog) ## # A tsibble: 6 x 9 [1] ## # Key: Symbol [1] ## Symbol Date Open High Low Close Adj_Close Volume trading_day ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 GOOG 2015-01-02 526. 528. 521. 522. 522. 1447600 1 ## 2 GOOG 2015-01-05 520. 521. 510. 511. 511. 2059800 2 ## 3 GOOG 2015-01-06 512. 513. 498. 499. 499. 2899900 3 ## 4 GOOG 2015-01-07 504. 504. 497. 498. 498. 2065100 4 ## 5 GOOG 2015-01-08 495. 501. 488. 500. 500. 3353600 5 ## 6 GOOG 2015-01-09 502. 502. 492. 493. 493. 2069400 6 dim(goog) ## [1] 1006 9 Let’s fit a naive model (projection of last value) to Google’s Close from 2015, then predict values from Jan 2016. We’ll create an 80:20 train-test split (test should be ~20% or at least as long as the anticipated forecast.). # Segment with `filter()`, or `group_by()` + `slice()`. goog_train &lt;- goog %&gt;% filter(year(Date) == 2015) goog_test &lt;- goog %&gt;% filter(yearmonth(Date) == yearmonth(&quot;2016 Jan&quot;)) # Train model goog_mdl &lt;- goog_train %&gt;% model(mdl_naive = NAIVE(Close)) # Generate predictions (forecast) goog_fc &lt;- goog_mdl %&gt;% forecast(new_data = goog_test) The autoplot() and autolayer() functions take a lot of the headache out of plotting the results, especially since forecast() tucks away the confidence intervals in a distribution list object. # Consider using autoplot() + autolayer()... # goog_fc %&gt;% # autoplot(color = &quot;goldenrod&quot;) + # autolayer(goog_train, Close, color = &quot;goldenrod&quot;) goog_fc_2 &lt;- goog_fc %&gt;% mutate(mu = map_dbl(Close, ~unlist(.) %&gt;% .[&quot;mu&quot;]), sigma = map_dbl(Close, ~unlist(.) %&gt;% .[&quot;sigma&quot;]), ci_025 = qnorm(.025, mu, sigma), ci_100 = qnorm(.100, mu, sigma), ci_900 = qnorm(.900, mu, sigma), ci_975 = qnorm(.975, mu, sigma)) %&gt;% select(trading_day, Date, Close, mu, sigma, ci_025:ci_975) bind_rows(goog_train, goog_test) %&gt;% ggplot(aes(x = Date)) + geom_line(aes(y = Close), color = &quot;goldenrod&quot;) + geom_line(data = goog_fc_2, aes(y = mu), color = &quot;goldenrod&quot;, size = 1) + geom_ribbon(data = goog_fc_2, aes(ymin = ci_025, ymax = ci_975), fill = &quot;goldenrod&quot;, alpha = .2) + geom_ribbon(data = goog_fc_2, aes(ymin = ci_100, ymax = ci_900), fill = &quot;goldenrod&quot;, alpha = .2) + theme_light() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;20d naive forecast from model fit to CY-2015&quot;, caption = &quot;Shaded area is 80%- and 95% confidence interval.&quot;, x = &quot;Trading Day&quot;, y = &quot;Closing Price&quot;) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. 1.3 Evaluating Models Evaluate the model fit with residuals diagnostics.2 broom::augment() adds three columns to the model cols: .fitted, .resid, and .innov. .innov is the residual from the transformed data (if no transformation, it just equals .resid). goog_mdl_aug &lt;- goog_mdl %&gt;% broom::augment() Innovation residuals should be independent random variables normally distributed with mean zero and constant variance (the normality and variance conditions are only required for inference and prediction intervals). Happily, feasts has just what you need. goog_mdl %&gt;% gg_tsresiduals() + labs(title = &quot;Residuals Analysis&quot;) The autocorrelation plot above supports the independence assumption. The histogram plot tests normality (it is pretty normal, but the right tail is long). The residuals plot tests mean zero and constant variance. You can carry out a portmanteau test test on the autocorrelation assumption. Two common tests are the Box-Pierce and the Ljung-Box. These tests check the likelihood of a combination of autocorrelations at once, without testing any one correlation - kind of like an ANOVA test. The Ljung-Box test statistic is a sum of squared \\(k\\)-lagged autocorrelations, \\(r_k^2\\), \\[Q^* = T(T+2) \\sum_{k=1}^l(T-k)^{-1}r_k^2.\\] The test statistic has a \\(\\chi^2\\) distribution with \\(l - K\\) degrees of freedom (where \\(K\\) is the number of parameters in the model). Use \\(l = 10\\) for non-seasonal data and \\(l = 2m\\) for seasonal data. If your model has no explanatory variables, \\(K = 0.\\) goog_mdl_aug %&gt;% features(.var = .innov, features = ljung_box, lag = 10, dof = 0) ## # A tibble: 1 × 4 ## Symbol .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GOOG mdl_naive 7.91 0.637 The p-value is not under .05, so do not reject the assumption of no autocorrelation - i.e., the assumption of white noise. 1.4 Evaluating Accuracy Some forecasting methods are extremely simple and surprisingly effective. The mean method projects the historical average, \\(\\hat{y}_{T+h|T} = \\bar{y}.\\) The naive method projects the last observation, \\(\\hat{y}_{T+h|T} = y_T.\\) The seasonal naive method projects the last seasonal observation, \\(\\hat{y}_{T+h|T} = y_{T+h-m(k+1)}.\\) The drift method projects the straight line from the first and last observation, \\(\\hat{y}_{T+h|T} = y_T + h\\left(\\frac{y_T - y_1}{T-1}\\right).\\) The plot below of tsibbledata::aus_production shows the four forecast benchmarks. tsibbledata::aus_production %&gt;% # same thing as # filter(Quarter &gt;= yearquarter(&quot;1970 Q1&quot;) &amp; Quarter &lt;= yearquarter(&quot;2004 Q4&quot;)) %&gt;% filter_index(&quot;1995 Q1&quot; ~ &quot;2007 Q4&quot;) %&gt;% model(Mean = MEAN(Beer), Naive = NAIVE(Beer), SNaive = SNAIVE(Beer), Drift = RW(Beer ~ drift())) %&gt;% forecast(h = 8) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = .mean, color = .model), size = 1) + geom_line(data = tsibbledata::aus_production %&gt;% filter_index(&quot;1995 Q1&quot; ~ &quot;2009 Q4&quot;), aes(y = Beer), color = &quot;darkgrey&quot;, size = 1) + theme_light() + guides(color = guide_legend(title = &quot;Forecast&quot;)) + labs(title = &quot;Simple forecast methods are useful benchmarks.&quot;, x = NULL, y = NULL, caption = &quot;Source: Quarterly beer production (ML) from tsibbledata::aus_production.&quot;) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. Evaluate the forecast accuracy with the test data (aka, “hold-out set”, and “out-of-sample data”). The forecast error is the difference between the observed and forecast value, \\(e_{T+h} = y_{T+h} - \\hat{y}_{t+h|T}.\\) Forecast errors differ from model residuals in the data (train vs test) and because forecast values are (usually) multi-step forecasts which include prior forecast values as inputs. There are a few benchmark metrics to evaluate a fit based on the errors. MAE. Mean absolute error, \\(mean(|e_t|)\\) RMSE. Root mean squared error, \\(\\sqrt{mean(e_t^2)}\\) MAPE. Mean absolute percentage error, \\(mean(|e_t / y_t|) \\times 100\\) MASE. Mean absolute scaled error, \\(MAE/Q\\) where \\(Q\\) is a scaling constant calculated as the average one-period change in the outcome variable (error from a one-step naive forecast). The MAE and RMSE are on the same scale as the data, so they are only useful for comparing models fitted to the same series. MAPE is unitless, but does not work for \\(y_t = 0\\), and it assumes a meaningful zero (ratio data). The MASE is most useful for comparing data sets of different units. Use accuracy() to evaluate a model. Comparing the naive, drift, and mean methods for forecasting the Google stock price, the naive model wins on all measures. goog_mdl &lt;- goog_train %&gt;% model(Naive = NAIVE(Close), Drift = RW(Close ~ drift()), Mean = MEAN(Close)) goog_fc &lt;- goog_mdl %&gt;% forecast(new_data = goog_test) ggplot() + geom_line(data = goog_fc, aes(x = trading_day, y = .mean, color = .model)) + geom_line(data = bind_rows(goog_train, goog_test), aes(x = trading_day, y = Close)) + theme_light() + scale_y_continuous(labels = scales::dollar_format()) + labs(title = &quot;Comparison of three models.&quot;, subtitle = &quot;Google stock price predicted over 19 days.&quot;, x = &quot;Trading Day&quot;, y = &quot;Closing Price&quot;, color = &quot;Forecast&quot;) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. # Or just this. # autoplot(goog_fc, bind_rows(goog_train, goog_test), level = NULL) accuracy(goog_fc, goog) %&gt;% select(.model, RMSE, MAE, MAPE, MASE) ## # A tibble: 3 × 5 ## .model RMSE MAE MAPE MASE ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Drift 53.1 49.8 6.99 6.99 ## 2 Mean 118. 117. 16.2 16.4 ## 3 Naive 43.4 40.4 5.67 5.67 There is a better way of evaluating a model than with a single test set. Time series cross-validation breaks the dataset into multiple training sets by setting the cutoff at varying points and then setting the test set to be a single steps ahead of the horizon. Function stretch_tsibble() creates a tsibble of initial size .init then appends additional data sets of increasing size .step. goog_train_cv &lt;- goog %&gt;% filter(year(Date) == 2015) %&gt;% stretch_tsibble(.init = 3, .step = 1) %&gt;% # move .id next to the other key col relocate(Date, Symbol, .id) # 250 keys! goog_train_cv goog_fc &lt;- goog_train_cv %&gt;% # Fit a model for each key model(RW(Close ~ drift())) %&gt;% # 8 forecast rows per model = 250 * 8 = 2,000 rows forecast(h = 8) %&gt;% # Capture the forecast period for comparison group_by(.id) %&gt;% mutate(h = row_number()) %&gt;% ungroup() goog_fc %&gt;% accuracy(data = goog %&gt;% filter(year(Date) == 2015), by = c(&quot;h&quot;, &quot;.model&quot;)) %&gt;% select(h, RMSE) Loading the fable package automatically loads fabletools as well.↩︎ Residuals and errors are not the same thing. The residual is the difference between the observed and fitted value in the training data set. The error is the difference between the observed and fitted value in the test data set.↩︎ "],["exporation.html", "Chapter 2 Exploratory Analysis 2.1 Graphical Analysis 2.2 Transformations 2.3 Decomposition", " Chapter 2 Exploratory Analysis Start an analysis by viewing the data values and structure, then take some summary statistics. fabletools::features() is great for this. tsibbledata::aus_production %&gt;% features(Beer, list(mean = mean, quantile = quantile)) ## # A tibble: 1 × 6 ## mean `quantile_0%` `quantile_25%` `quantile_50%` `quantile_75%` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 415. 213 379. 422 466. ## # ℹ 1 more variable: `quantile_100%` &lt;dbl&gt; There are many autocorrelation features you might want to review. I don’t understand why you’d want to know all of these, but feat_acf has them. tsibbledata::aus_production %&gt;% features(Beer, feat_acf) ## # A tibble: 1 × 7 ## acf1 acf10 diff1_acf1 diff1_acf10 diff2_acf1 diff2_acf10 season_acf1 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.684 4.37 -0.221 2.72 -0.361 2.26 0.940 2.1 Graphical Analysis The next task is to plot the data to identify patterns, unusual observations, changes over time, and relationships between variables. Look for trend, cycles, and seasonality in your exploratory plots. These features inform the subsequent forecasting process. ggplot2::autoplot() does a great job picking out the right plot, but I feel more comfortable staying old-school for now. tsibbledata::ansett %&gt;% filter(Airports == &quot;MEL-SYD&quot;, Class == &quot;Economy&quot;) %&gt;% ggplot(aes(x = Week, y = Passengers)) + geom_line(color = &quot;goldenrod&quot;) + theme_light() + labs(title = &quot;Start with a simple time series plot.&quot;, subtitle = &quot;Weekly passenger volume.&quot;, x = NULL, y = NULL) What does this one reveal? There was a period in 1989 of zero passengers (strike). There was a period in 1992 where passenger load dropped (planes temporarily reconfigured). Volume increased during the second half of 1992. Several large post-holiday dips in volume. Some larger trends of increasing and decreasing volume (the data is cyclic). Also appears to be some missing observations. (common practice with missing observations is to impute values with the time series mean.) If a data series has trend and seasonality, highlight it with feasts::gg_season(). a10 &lt;- tsibbledata::PBS %&gt;% filter(ATC2 == &quot;A10&quot;) %&gt;% select(Month, Cost) %&gt;% summarize(Cost = sum(Cost)) p1 &lt;- a10 %&gt;% filter(year(Month) %in% c(1991, 1995, 2000, 2005)) %&gt;% ggplot(aes(x = month(Month, label = TRUE, abbr = TRUE), y = Cost, group = factor(year(Month)), color = factor(year(Month)), label = if_else(month(Month) %in% c(1, 12), year(Month), NA_real_))) + geom_line(show.legend = FALSE) + geom_text(show.legend = FALSE) + theme_light() + theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) + labs(subtitle = &quot;using gglot&quot;, x = NULL, y = NULL) p2 &lt;- a10 %&gt;% filter(year(Month) %in% c(1991, 1995, 2000, 2005)) %&gt;% fill_gaps() %&gt;% # otherwise, gg_season() barks. gg_season(Cost, labels = &quot;both&quot;) + theme_light() + theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) + labs(subtitle = &quot;using feasts&quot;, x = NULL, y = NULL) p1 + p2 + plot_annotation(title = &quot;Medicare script costs increase from Feb - following Jan.&quot;, subtitle = &quot;12-month seasonality plot, selected years.&quot;) Emphasize the seasonality further by faceting on the sub-series with feasts::gg_subseries(). yint &lt;- a10 %&gt;% as.tibble() %&gt;% group_by(month(Month, label = TRUE, abbr = TRUE)) %&gt;% mutate(mean_cost = mean(Cost) / 1e6) p1 &lt;- a10 %&gt;% ggplot(aes(x = year(Month), y = Cost / 1e6)) + geom_line(show.legend = FALSE, color = &quot;goldenrod&quot;) + geom_hline(data = yint, aes(yintercept = mean_cost), linetype = 2) + theme_light() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), panel.grid = element_blank()) + scale_y_continuous(labels = scales::dollar) + facet_wrap(vars(month(Month, label = TRUE, abbr = TRUE)), nrow = 1) + labs(subtitle = &quot;using gglot&quot;, x = NULL, y = NULL) p2 &lt;- a10 %&gt;% mutate(Cost = Cost / 1e6) %&gt;% fill_gaps() %&gt;% # otherwise, gg_subseries() barks. gg_subseries(Cost) + theme_light() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), panel.grid = element_blank()) + scale_y_continuous(labels = scales::dollar) + labs(subtitle = &quot;using feasts&quot;, x = NULL, y = NULL) p1 / p2 + plot_annotation(title = &quot;Medicare script costs follow monthly seasonality, inreasing annually.&quot;, subtitle = &quot;Monthly subseries plot of cost $MM, 1991 - 2009.&quot;) Explore the correlation between two quantitative variables with the Pearson correlation coefficient. Recall that the covariance between series \\(X\\) and \\(Y\\) is defined \\(Cov(X, Y) = E[(X - \\mu_X) (Y - \\mu_Y)]\\) which simplifies to \\(Cov(X, Y) = E[XY] - \\mu_X \\mu_Y\\). The covariance of \\(X\\) and \\(Y\\) is positive if \\(X\\) and \\(Y\\) increase together, negative if they move in opposite directions, and if \\(X\\) and \\(Y\\) are independent, \\(E[XY] = E[X]E[Y] = \\mu_X \\mu_Y\\), so \\(Cov(X, Y) = 0\\). Covariance is usually inconvenient because its values depend on the units of the series. Dividing \\(Cov(X, Y)\\) by the standard deviations \\(\\sigma_X \\sigma_Y\\) creates a unit-less variable with range [-1, 1], also known as the Pearson correlation.3 \\[\\rho = \\frac{\\sigma_{XY}} {\\sigma_X \\sigma_Y}.\\] PBS %&gt;% group_by(ATC1) %&gt;% summarize(.groups = &quot;drop&quot;, Cost = sum(Cost)) %&gt;% pivot_wider(names_from = ATC1, values_from = Cost) %&gt;% as_tibble() %&gt;% select(-Month) %&gt;% cor() %&gt;% ggcorrplot::ggcorrplot() + theme_light() + labs(title = &quot;Z and P indexes negatively correlated with others.&quot;, subtitle = &quot;Correlation plot of Medicare ATC1 indexes.&quot;, x = NULL, y = NULL) Autocorrelation is correlation with lagging observations. Lag plots of current period vs lags are a particular kind of correlation scatterplot useful for identifying seasonality. feasts::ACF() extends the base R acf() function to tsibbles. aus_production contains quarterly production levels. The 4-period lag is a year-over-year correlation and is strong because of seasonality in production. The 8-period lag is less strong. The 1-, 2-, and 3-period lags are not positively correlated. In fact, the lag-2 is negatively correlated. prod2k &lt;- tsibbledata::aus_production %&gt;% filter(year(Quarter) &gt;= 2000) prod2k %&gt;% gg_lag(Beer, geom = &quot;point&quot;) + theme_light() + labs(title = &quot;Production is seasonal with Q4 peak, Q1 second.&quot;, subtitle = &quot;Quarterly lag plot of beer production&quot;, x = NULL, y = NULL) p1 &lt;- prod2k %&gt;% ACF(Beer, lag_max = 16) %&gt;% ggplot(aes(x = lag)) + geom_linerange(aes(ymin = 0, ymax = acf), color = &quot;goldenrod&quot;) + geom_point(aes(y = acf), color = &quot;goldenrod&quot;) + geom_hline(yintercept = 0) + geom_hline(yintercept = -2 / sqrt(nrow(prod2k)), linetype = 2) + geom_hline(yintercept = +2 / sqrt(nrow(prod2k)), linetype = 2) + theme_light() + theme(panel.grid = element_blank()) + labs(subtitle = &quot;using ggplot&quot;, y = NULL) p2 &lt;- prod2k %&gt;% ACF(Beer, lag_max = 16) %&gt;% autoplot() + theme_light() + labs(subtitle = &quot;using autoplot&quot;, y = NULL) p1 + p2 + plot_annotation(title = &quot;Strongest correlation is 4-period lag, negative 2-period lag.&quot;, subtitle = &quot;Quarterly autocorrelation of beer production.&quot;, caption = &quot;Dashed lines are +/- square root of series length, the white noise bound.&quot;) Autocorrelation for trending data tends to be large and positive because observations nearby in time are also nearby in size. The ACF tends to have positive values that slowly decrease as the lags increase. Autocorrelation for seasonal data tends to be larger for the seasonal lags (at multiples of the seasonal frequency). The Quarterly Australian Beer Production ACF above shows seasonality. Autocorrelation for both trended and seasonal data has a combination of these effects. Time series that show no autocorrelation are called white noise. White noise series have near-zero autocorrelation. Stock price changes often exhibit white noise. Almost all lags have autocorrelation insignificantly different from zero. Expect 95% of spikes to lie withing \\(\\pm 2 / \\sqrt{T}\\) where \\(T\\) is the length of the data series. stock &lt;- tsibbledata::gafa_stock %&gt;% filter(Symbol == &quot;FB&quot;) %&gt;% mutate(l1 = Close - lag(Close)) stock %&gt;% ACF(l1, lag_max = 16) %&gt;% ggplot(aes(x = lag)) + geom_linerange(aes(ymin = 0, ymax = acf), color = &quot;goldenrod&quot;) + geom_point(aes(y = acf), color = &quot;goldenrod&quot;) + geom_hline(yintercept = 0) + geom_hline(yintercept = -2 / sqrt(nrow(stock)), linetype = 2) + geom_hline(yintercept = +2 / sqrt(nrow(stock)), linetype = 2) + theme_light() + theme(panel.grid = element_blank()) + labs(title = &quot;Stock price changes tend to exhibit white noise.&quot;, subtitle = &quot;Daily autocorrelation of stock price changes.&quot;, caption = &quot;Dashed lines are +/- square root of series length, the white noise bound.&quot;) The Ljung-Box test tests the randomness of a series; a p-value under 0.05 rejects the null hypothesis of white noise. The test reject white noise for the beer production, but not for stock price changes. Box.test(prod2k$Beer, lag = 16, fitdf = 0, type = &quot;Ljung&quot;) ## ## Box-Ljung test ## ## data: prod2k$Beer ## X-squared = 187.44, df = 16, p-value &lt; 2.2e-16 Box.test(stock$l1, lag = 16, fitdf = 0, type = &quot;Ljung&quot;) ## ## Box-Ljung test ## ## data: stock$l1 ## X-squared = 16.745, df = 16, p-value = 0.4023 2.2 Transformations Remove known sources of variation (e.g., days per month, population growth, inflation). E.g., monthly totals may vary due to differing month lengths. milk &lt;- fma::milk %&gt;% as_tsibble() %&gt;% # milk is a `ts` object mutate(daily_avg = value / lubridate::days_in_month(index)) p1 &lt;- milk %&gt;% ggplot(aes(x = index, y = value)) + geom_line(color = &quot;goldenrod&quot;) + theme_light() + theme(axis.text.x = element_blank()) + labs(subtitle = &quot;Original&quot;, x = NULL, y = NULL) p2 &lt;- milk %&gt;% ggplot(aes(x = index, y = daily_avg)) + geom_line(color = &quot;goldenrod&quot;) + theme_light() + labs(subtitle = &quot;Daily Average&quot;, x = NULL, y = NULL) p1 / p2 + plot_annotation(title = &quot;Convert monthly sums into daily averages per month.&quot;, subtitle = &quot;Average daily milk production by month.&quot;) Make patterns more consistent across the data set. Simpler patterns usually lead to more accurate forecasts. A Box-Cox transformation can equalize seasonal variation. \\[w_t = \\begin{cases} \\mathrm{log}(y_t), &amp; \\mbox{if } \\lambda\\mbox{ = 0} \\\\ \\left(\\mathrm{sign}(y_t) |y_t|^\\lambda - 1 \\right) / \\lambda, &amp; \\mbox{otherwise} \\end{cases}\\] \\(\\lambda\\) can take any value, but values near the following yield familiar transformations. \\(\\lambda = 1\\): no substantive transformation. \\(\\lambda = 0.5\\): square root plus linear transformation. \\(\\lambda = 0.333\\): cube root plus linear transformation. \\(\\lambda = 0\\): natural log. \\(\\lambda = -1\\): inverse. A good value of \\(\\lambda\\) is one which makes the size of the seasonal variation about the same across the whole series. fabletools::features() and BoxCox.lambda() optimize \\(\\lambda\\), but try to choose a simple value to make interpretation clearer. Note that while forecasts are not sensitive to \\(\\lambda\\), prediction intervals are. lambda &lt;- tsibbledata::aus_production %&gt;% # guerrero() applies Guerrero&#39;s method to select lambda that minimizes the # coefficient of variation: .12. features(Gas, features = guerrero) %&gt;% pull(lambda_guerrero) # supposedly the same method, but returns .10 instead. lambda_v2 &lt;- forecast::BoxCox.lambda(aus_production$Gas, method = &quot;guerrero&quot;) p1 &lt;- aus_production %&gt;% mutate(gas_xform = box_cox(Gas, lambda)) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Gas), color = &quot;goldenrod&quot;) + theme_light() + theme(axis.text.x = element_blank()) + labs(subtitle = &quot;Original&quot;, x = NULL) p2 &lt;- aus_production %&gt;% mutate(gas_xform = box_cox(Gas, lambda)) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = gas_xform), color = &quot;goldenrod&quot;) + theme_light() + labs(subtitle = glue(&quot;Box-Cox, lambda = {scales::comma(lambda, accuracy = 0.01)}&quot;), y = glue(&quot;Gas^{scales::comma(lambda, accuracy = 0.01)}&quot;)) p1 / p2 + plot_annotation(title = &quot;Box-Cox transformation equalizes the seasonal component.&quot;, subtitle = &quot;Box-Cox transformation (Guerrero&#39;s method) of quarterly gas production.&quot;) 2.3 Decomposition Time series data often has trending, seasonality, and cycles. It’s usually useful to lump trending and cycles into a trend-cycle components, or simply “trend”, and treat time series data as consisting of seasonality \\(S_t\\), trend \\(T_t\\), and a remainder \\(R_t\\). If the magnitude of the seasonal fluctuations and the variation in the trend cycle are constant, then these components are additive, \\(y_t = S_t + T_t + R_t\\); if they are proportional to the level, then these components are multiplicative, \\(y_t = S_t \\times T_t \\times R_t\\). 2.3.1 Classical Decomposition Classical decomposition was commonly used until the 1950s. It is still the basis of other methods, so it is good to understand. Classical decomposition is based on moving averages. An m-MA moving average of order \\(m = 2k + 1\\) averages the \\(k\\) observations before \\(t\\) through the \\(k\\) observations after \\(t\\). The slider package is great for this. \\(m\\) is usually an odd number so that the number of periods before and after are equal. \\[\\hat{T}_t = \\frac{1}{m}\\sum_{j = -k}^k y_{t+j}\\] library(slider) # sliding window functions ## Warning: package &#39;slider&#39; was built under R version 4.3.1 tsibbledata::global_economy %&gt;% filter(Country == &quot;Australia&quot;) %&gt;% mutate( `3-MA` = slide_dbl(Exports, mean, .before = 1, .after = 1, .complete = TRUE), `5-MA` = slide_dbl(Exports, mean, .before = 2, .after = 2, .complete = TRUE), `7-MA` = slide_dbl(Exports, mean, .before = 3, .after = 3, .complete = TRUE), `9-MA` = slide_dbl(Exports, mean, .before = 4, .after = 4, .complete = TRUE) ) %&gt;% pivot_longer(cols = ends_with(&quot;-MA&quot;), names_to = &quot;MA_name&quot;, values_to = &quot;MA&quot;) %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = MA, color = MA_name), na.rm = TRUE) + geom_line(aes(y = Exports), size = 1.25, color = &quot;#000000&quot;, alpha = 0.4) + theme_light() + # guides(color = guide_legend(title = &quot;series&quot;)) + labs(title = &quot;m-MA simple moving averages smooth a time series.&quot;, subtitle = &quot;m-MA of annual export proportion of GDP for m = 3, 5, 7, 9.&quot;, x = NULL, y = &quot;Pct of GDP&quot;, color = NULL) But what if you have known seasonal periods in the data? For example, with quarterly seasonality it makes sense to take a moving average of the 4 periods at once. Do this with a moving average of a four-period moving average. tsibbledata::aus_production %&gt;% filter(year(Quarter) &gt;= 1992) %&gt;% mutate( `4-MA` = slide_dbl(Beer, mean, .before = 1, .after = 2, .complete = TRUE), `2x4-MA` = slide_dbl(`4-MA`, mean, .before = 1, .after = 0, .complete = TRUE) ) %&gt;% pivot_longer(cols = ends_with(&quot;-MA&quot;), names_to = &quot;MA_type&quot;, values_to = &quot;MA&quot;) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Beer), color = &quot;gray&quot;, size = 1) + geom_line(aes(y = MA, color = MA_type), na.rm = TRUE) + theme_light() + ggthemes::scale_color_few() + guides(color = guide_legend(title = &quot;series&quot;)) + labs(title = &quot;MAs of MAs smooth seasonal periods.&quot;, subtitle = &quot;2-MA of a 4-MA for quarterly data.&quot;, y = NULL) Classical decomposition calculates a trend-cycle component \\(\\hat{T}_t\\) with an m-MA (odd order) or \\(2 \\times m\\)-MA. The de-trended series is the difference, \\(y_t - \\hat{T}_t\\). The seasonal component \\(\\hat{S}_t\\) is the seasonal average (e.g., for monthly seasons, then \\(\\hat{S}_1\\) would be the January average). The remainder is the residual, \\(\\hat{R}_t = y_t - \\hat{T}_t - \\hat{S}_t.\\) If the cycle variation and seasonal magnitude increases with the observation level, then the same principles apply except the subtrations are replaced with divisions, \\(y_t / \\hat{T}_t\\) and remainder \\(\\hat{R}_t = y_t / (\\hat{T}_t \\hat{S}_t)\\). fpp3::us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(classical = classical_decomposition(Employed, type = &quot;additive&quot;)) %&gt;% components() %&gt;% pivot_longer(cols = Employed:random, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;seasonal&quot;, &quot;random&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;Classical additive decomposition of US retail employment&quot;, subtitle = &quot;Employed = trend + seasonal + random&quot;, y = NULL) Classical decomposition has weaknesses: the trend-cycle is unavailable for the first few and and last few periods; it assumes the seasonal component is stable over time; and it also tends to over-smooth the data. 2.3.2 X-11 and SEATS X-11 and Seasonal Extraction in ARIMA Time Series (SEATS) are commonly used by governmental agencies. X-11 overcomes some of classical decomposition’s drawbacks by adding extra steps. It creates trend-cycle estimates for all periods, and accommodates a slowly varying seasonal component. fpp3::us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(x11 = X_13ARIMA_SEATS(Employed ~ x11())) %&gt;% components() %&gt;% pivot_longer(cols = Employed:irregular, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;seasonal&quot;, &quot;irregular&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;X11 decomposition of US retail employment&quot;, subtitle = &quot;Employed = trend + seasonal + irregular&quot;, y = NULL) SEATS is another one (too complicated to discuss evidently!). fpp3::us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(x11 = X_13ARIMA_SEATS(Employed ~ seats())) %&gt;% components() %&gt;% pivot_longer(cols = Employed:irregular, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;seasonal&quot;, &quot;irregular&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;SEATS decomposition of US retail employment&quot;, subtitle = &quot;Employed = f(trend, seasonal, irregular)&quot;, y = NULL) 2.3.3 STL Seasonal and Trend decomposition using Loess (STL) has several advantages over classical decomposition, and the SEATS and X-11 methods. It handles any type of seasonality (not just monthly and quarterly); the seasonality component can change; the smoothness of the trend-cycle can be changed by the modeler; and it is robust to outliers. The widow settings inside model() control how rapidly the components change. fpp3::us_employment %&gt;% filter(year(Month) &gt;= 1990 &amp; Title == &quot;Retail Trade&quot;) %&gt;% model(STL = STL(Employed ~ trend(window = 7) + season(window = &quot;periodic&quot;), robust = TRUE)) %&gt;% components() %&gt;% pivot_longer(cols = Employed:remainder, names_to = &quot;component&quot;, values_to = &quot;employed&quot;) %&gt;% mutate(component = factor(component, levels = c(&quot;Employed&quot;, &quot;trend&quot;, &quot;season_year&quot;, &quot;remainder&quot;))) %&gt;% ggplot(aes(x = Month, y = employed)) + geom_line(na.rm = TRUE, color = &quot;goldenrod&quot;) + theme_light() + facet_grid(vars(component), scales = &quot;free_y&quot;) + labs(title = &quot;STL decomposition of US retail employment&quot;, subtitle = &quot;Employed = f(trend, seasonal, irregular)&quot;, y = NULL) STL decomposition is the basis for other insights into the data series. You can measure the relative strength of trend and seasonality by the relative size of their variance: \\(F_T = 1 - \\mathrm{Var}(R_T) / \\mathrm{Var}(R_T + T_T)\\) and \\(F_S = 1 - \\mathrm{Var}(S_T) / \\mathrm{Var}(S_T + T_T)\\). us_employment_featues &lt;- fpp3::us_employment %&gt;% features(Employed, feat_stl) %&gt;% inner_join(fpp3::us_employment %&gt;% as_tibble() %&gt;% select(Series_ID, Title) %&gt;% unique(), by = &quot;Series_ID&quot;) us_1 &lt;- us_employment_featues %&gt;% filter(trend_strength &gt;= 0.995 &amp; seasonal_strength_year &gt; 0.9) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_2 &lt;- us_employment_featues %&gt;% filter(trend_strength &gt;= 0.995 &amp; seasonal_strength_year &lt; 0.5) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_3 &lt;- us_employment_featues %&gt;% filter(trend_strength &lt;= 0.985 &amp; seasonal_strength_year &lt; 0.5) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_4 &lt;- us_employment_featues %&gt;% filter(trend_strength &lt;= 0.985 &amp; seasonal_strength_year &gt; 0.9) %&gt;% slice_sample(n = 1) %&gt;% pull(Series_ID) us_employment_ids &lt;- c(us_1, us_2, us_3, us_4) us_employment_featues %&gt;% mutate(Series_lbl = if_else(Series_ID %in% us_employment_ids, Title, NA_character_)) %&gt;% ggplot(aes(x = trend_strength, y = seasonal_strength_year)) + geom_point(color = &quot;goldenrod&quot;) + geom_text(aes(label = Series_lbl), color = &quot;goldenrod4&quot;) + theme_light() + labs(title = &quot;Some sectors have seasonal employment, othes trend, others both.&quot;) Incidentally, \\(\\rho\\) is related to the slope of the linear regression line: \\(\\beta_1 = \\frac{\\sigma_{XY}}{\\sigma_X^2} = \\rho \\frac{\\sigma_Y}{\\sigma_X}\\).↩︎ "],["regression.html", "Chapter 3 Time Series Regression 3.1 Exploratory Analysis 3.2 Fit Model 3.3 Model Evaluation 3.4 Variable Selection 3.5 Predicting Values 3.6 Nonlinear Regression", " Chapter 3 Time Series Regression A time series regression forecasts a time series as a linear relationship with the independent variables. \\[y_t = X_t \\beta + \\epsilon_t\\] The linear regression model assumes there is a linear relationship between the forecast variable and the predictor variables. This implies that the errors must have mean zero, otherwise the forecasts are biased: \\(E(\\epsilon | X_j) = 0\\). The least squares method guarantees this condition is met. The residuals must not be autocorrelated, otherwise the forecasts are inefficient because there is more information in the data that can be exploited. To produce reliable inferences and prediction intervals, the residuals must be independent normal random variables with constant variance. Let’s learn by example. Data set tsibbledata::aus_production contains quarterly estimates of selected indicators of manufacturing production in Australia: Beer, Gas, Electricity, and Cement. (Tobacco and Bricks too, but they are incomplete data series). We’ll look at Beer production. 3.1 Exploratory Analysis The correlation matrix shows Beer is negatively correlated with Gas, Electricity, and Cement. tsibbledata::aus_production %&gt;% filter(year(Quarter) &gt;= 1992) %&gt;% as_tibble() %&gt;% select(-Quarter) %&gt;% cor() %&gt;% ggcorrplot::ggcorrplot(type = &quot;upper&quot;, lab = TRUE, lab_size = 3) + theme_light() + labs(title = &quot;Consumption is correlated with predictors&quot;, subtitle = &quot;Correlation plot of US economic indicators.&quot;, caption = &quot;Source: tsibbledata::aus_production.&quot;, x = NULL, y = NULL) You would probably want to explore the data further, but this is a minimal example. 3.2 Fit Model Consider this kitchen-sink model: \\[\\mathrm{Beer}_t = \\beta_0 + \\beta_1 \\mathrm{Gas}_t + \\beta_2 \\mathrm{Electricity}_t + \\beta_3 \\mathrm{Cement}_t + \\epsilon_t\\] Use fable::TSLM() to fit a linear regression model to tsibble time series data. TSLM() is similar to lm() with additional facilities for handling time series. TSLM() %&gt;% report() is identical to lm() %&gt;% summary(). fable is part of the tidyverts group of time series packages. One nice feature of the tidyverts is piping. ausprod_fmla &lt;- formula(Beer ~ Gas + Electricity + Cement) ausprod_gas &lt;- tsibbledata::aus_production %&gt;% filter(year(Quarter) &gt;= 1992) ausprod_lm &lt;- ausprod_gas %&gt;% model(TSLM(ausprod_fmla)) report(ausprod_lm) ## Series: Beer ## Model: TSLM ## ## Residuals: ## Min 1Q Median 3Q Max ## -61.47 -25.47 -11.00 21.05 76.03 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.748e+02 3.583e+01 13.250 &lt; 2e-16 *** ## Gas -1.216e+00 2.501e-01 -4.860 6.93e-06 *** ## Electricity 7.544e-04 1.469e-03 0.514 0.60919 ## Cement 7.404e-02 2.609e-02 2.838 0.00594 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 36.67 on 70 degrees of freedom ## Multiple R-squared: 0.3097, Adjusted R-squared: 0.2801 ## F-statistic: 10.47 on 3 and 70 DF, p-value: 9.003e-06 The modeled \\(R^2\\) is 0.310, the adjusted \\(R^2\\) is 0.280, and the standard error of the regression4, \\(\\hat{\\sigma}_\\epsilon,\\) is 36.7. The fitted values follow the observations okay. The fitted to actuals plot has a moderate linear relationship. plot_lm &lt;- function() { p1 &lt;- augment(ausprod_lm) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Beer), color = &quot;dark gray&quot;, size = 1) + geom_line(aes(y = .fitted), color = &quot;goldenrod&quot;, size = 1) + theme_light() + labs(subtitle = &quot;Time series&quot;) p2 &lt;- augment(ausprod_lm) %&gt;% ggplot(aes(x = Beer, y = .fitted)) + geom_point(color = &quot;goldenrod&quot;, size = 1) + geom_abline(intercept = 0, slope = 1, linetype = 2, size = 1, color = &quot;dark gray&quot;) + theme_light() + labs(subtitle = &quot;Fitted vs actuals&quot;) p3 &lt;- p1 + p2 + patchwork::plot_annotation(title = &quot;Fitted values plots&quot;, subtitle = ausprod_fmla) p3 } plot_lm() Time series regressions usually suffer from autocorrelation, so you need to control for trend and seasonality by adding time and seasonal dummy variables. Do this in TSLM() by including the trend() and season() helper functions in the formula. These are the tidyverts equivalent to introducing a row sequence and seasonal dummies as predictors in base R. ausprod_fmla &lt;- update(ausprod_fmla, . ~ . + trend() + season()) ausprod_lm &lt;- ausprod_gas %&gt;% model(TSLM(ausprod_fmla)) plot_lm() Much better. The modeled \\(R^2\\) is 0.926, the adjusted \\(R^2\\) is 0.919, and the standard error of the regression, \\(\\hat{\\sigma}_\\epsilon,\\) is 12.3. The fitted to actuals plot has a moderate linear relationship. Special Predictors We used two special predictor functions in our model, trend() and season(). Let’s look more closely at them. Adding the trend() special function is the same thing as adding a row number predictor. It captures the slope of the response variable associated with its sequence in the regression. ausprod_gas %&gt;% model(TSLM(Beer ~ trend())) %&gt;% augment() %&gt;% mutate(rownum = row_number()) %&gt;% ggplot(aes(x = rownum)) + geom_point(aes(y = .fitted), color = &quot;steelblue&quot;, size = 2) + geom_point(aes(y = Beer), color = &quot;goldenrod&quot;, size = 1) + geom_smooth(aes(y = Beer), method = &quot;lm&quot;, formula = &quot;y ~ x&quot;, size = .5) + theme_light() + labs(title = &quot;trend() is equivalent to adding time sequence to model.&quot;, subtitle = &quot;Beer ~ trend()&quot;) season() is the equivalent to adding seasonal dummy vars to the regression. tsibbledata::aus_production is defined with a quarterly index, so there are three associated dummies. x &lt;- ausprod_gas %&gt;% model(TSLM(Beer ~ season())) # Gets the number of dummies right, but why call them &quot;year&quot;? coef(x) ## # A tibble: 4 × 6 ## .model term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TSLM(Beer ~ season()) (Intercept) 429. 3.27 131. 1.71e-85 ## 2 TSLM(Beer ~ season()) season()year2 -35.0 4.63 -7.57 1.14e-10 ## 3 TSLM(Beer ~ season()) season()year3 -17.8 4.69 -3.80 3.05e- 4 ## 4 TSLM(Beer ~ season()) season()year4 72.5 4.69 15.5 2.97e-24 augment(x) %&gt;% mutate(season = quarter(Quarter)) %&gt;% ggplot(aes(x = season)) + geom_point(aes(y = .fitted), color = &quot;steelblue&quot;, size = 2) + geom_point(aes(y = Beer), color = &quot;goldenrod&quot;, size = 1) + theme_light() + labs(title = &quot;season() is equivalent to adding seasonal dummy variables to model.&quot;, subtitle = &quot;Beer ~ season()&quot;) There are other special variables you might consider. If an exogenous event has a one period effect or level effect, model it with an intervention dummy. If the intervention has a trend effect, use a piecewise linear trend. If you are modeling monthly totals, you might want to control for the number of days in the month (trading days, business days, etc.). You can do this by transforming your response variable, or by introducing a predictor with the normalizing factor. Some predictor variables may exert a lagged effect. A good example is advertising; its effect is most pronounced in the first month with a diminishing effect in subsequent months. If you expect a lag effect for a variable, include its lagged value in the model formula. \\[y = \\beta_0 + \\beta_1x_{l0} + \\beta_2x_{l1} + \\ldots\\] Fourier terms are an alternative to dummy variables. They are especially useful for long seasonal periods. Fourier showed that a series of alternating sine and cosine terms of the right frequencies can approximate any periodic function. A quarterly seasonal regression would have terms \\(\\sin\\left(\\frac{2\\pi}{m}\\right)\\), \\(\\cos\\left(\\frac{2\\pi}{m}\\right)\\), \\(\\sin\\left(\\frac{4\\pi}{m}\\right)\\). Use special function fourier(K) where K equals the number of sin and cos pairs to include, usually equal to half the number of seasonal periods. 3.3 Model Evaluation Evaluate the regression model with diagnostic plots. Use feasts::gg_tsresiduals() from the tidyverts. gg_tsresiduals(ausprod_lm) Time series observations are usually related to prior observations. That shows up in diagnostic plots as autocorrelation in the residuals. Autocorrelation in the residuals increases the prediction intervals, making forecasts less efficient (although still unbiased). The autocorrelation function plot (ACF) finds a significant negative spike at lag 1 and a positive spike at lag 12. Another test of autocorrelation in the residuals is the Breusch-Godfrey test for serial correlation up to a specified order. A small p-value indicates there is significant autocorrelation remaining in the residuals. The Breusch-Godfrey test is similar to the Ljung-Box test, but it is specifically designed for use with regression models. I cannot find support for the Breusch-Godfrey test. (ausprod_lm_lb &lt;- ausprod_lm %&gt;% augment() %&gt;% features(.innov, ljung_box, lag = 12, dof = 4)) ## # A tibble: 1 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TSLM(ausprod_fmla) 22.0 0.00497 Using Ljung-Box, the spike at lag 1 is significant (p = 0.0050). The autocorrelation is not particularly large and is unlikely to have a noticeable impact on the forecasts or the prediction intervals. The residuals vs time diagnostic plot revealed no heteroscedasticity (although there might be an outlier). Heteroscedasticity can make prediction intervals inaccurate. The histogram shows that the residuals are slightly left-skewed. Non-normality of the residuals can also make the prediction intervals inaccurate. The residuals should be independent of each of the explanatory variables and independent of candidate variables not used in the model. In this case, the residuals have a random pattern in each of the plots. ausprod_gas %&gt;% left_join(residuals(ausprod_lm), by = &quot;Quarter&quot;) %&gt;% pivot_longer(Cement:Gas, names_to = &quot;regressor&quot;, values_to = &quot;x&quot;) %&gt;% ggplot(aes(x = x, y = .resid, color = regressor)) + geom_point(show.legend = FALSE) + facet_wrap(vars(regressor), scales = &quot;free_x&quot;) + labs(title = &quot;There is no relationship between residuals and individual regressors.&quot;, subtitle = &quot;otherwise the relationship may be nonlinear.&quot;, x = NULL) + theme_light() + ggthemes::scale_color_few() A second check on the homoscedastity assumption is a plot of the residuals against the fitted values. Again, there should be no pattern. augment(ausprod_lm) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + labs(title = &quot;There is no relationship between residuals and fitted values.&quot;, subtitle = &quot;otherwise the response variable may require transformation.&quot;, y = &quot;Residuals&quot;, x = &quot;Fitted&quot;) + theme_light() Outliers, Leverage Points, and Influential Points Check for outliers, leverage points, and influential points. An outlier is a point far from the others (in either the x or y direction); a leverage point is far from the others in the x direction, potentially affecting the measured slope; an influential point is a leverage point that does affect the slope. For multiple linear regression models there is no straight-forward visual diagnostic like the simple linear regression scatter plot. The “hat matrix” \\(H\\) identifies leverage points. Recall that in the linear regression model, \\(\\hat{y} = X \\hat{\\beta}\\), the slope coefficients are estimated by \\(\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y\\). Substituting, \\(\\hat{y} = X(X&#39;X)^{-1}X&#39;y\\), or \\(\\hat{y} = Hy\\), where \\[H = X(X&#39;X)^{-1}X&#39;.\\] \\(H\\) is called the hat matrix because \\(H\\) puts the hat on \\(y\\). \\(H\\) is an \\(n \\times n\\) matrix. The diagonal elements \\(H_{ii}\\) are a measure of the distances between each observation \\(i\\)’s predictor variables \\(X_i\\) and the average of the entire data set predictor variables \\(\\bar{X}\\). \\(H_{ii}\\) are the leverage that the observed responses \\(y_i\\) exert on the predicted responses \\(\\hat{y}_i\\). Each \\(H_{ii}\\) is in the unit interval [0, 1] and the values sum to the number of regression parameters (including the intercept) \\(\\sum{H_{ii}} = k + 1\\). A common rule is to research any observation whose leverage value is more than 3 times larger than the mean leverage value, which since the sum of the leverage values is \\(k + 1\\), equals \\[H_{ii} &gt; 3 \\frac{k + 1}{n}.\\] Identify influential points by their Cook’s distance. Cook’s distance for observation \\(i\\) is defined \\[D_i = \\frac{(y_i - \\hat{y}_i)^2}{p \\times MSE} \\frac{H_{ii}}{(1 - H_{ii})^2}.\\] \\(D_i\\) directly summarizes how much all of the fitted values change when the ith observation is deleted. A data point with \\(D_i &gt; 1\\) is probably influential. \\(D_i &gt; 0.5\\) is at least worth investigating. I cannot find any support for Cook’s distance in fable. 3.4 Variable Selection There are five common measures of predictive accuracy: \\(\\bar{R}^2\\), CV, AIC, AICc, and BIC. glance(ausprod_lm) %&gt;% t() ## [,1] ## .model &quot;TSLM(ausprod_fmla)&quot; ## r_squared &quot;0.9263632&quot; ## adj_r_squared &quot;0.9185533&quot; ## sigma2 &quot;152.123&quot; ## statistic &quot;118.613&quot; ## p_value &quot;7.315323e-35&quot; ## df &quot;8&quot; ## log_lik &quot;-286.6818&quot; ## AIC &quot;381.3606&quot; ## AICc &quot;384.1731&quot; ## BIC &quot;402.0972&quot; ## CV &quot;170.4503&quot; ## deviance &quot;10040.12&quot; ## df.residual &quot;66&quot; ## rank &quot;8&quot; \\(\\bar{R}^2\\) is common and well-established, but tends to select too many predictor variables, making it less suitable for forecasting. BIC has the feature that if there is a true underlying model, the BIC will select it given enough data. However, there is rarely a true underlying model, and even if there was one, that model would not necessarily produce the best forecasts because the parameter estimates may not be accurate. The AICc, AIC, and CV statistics are usually best because forecasting is their objective. If the value of time series size \\(T\\) is large enough, they all lead to the same model. \\(R^2 = 1 - \\frac{SSE}{SST}\\) and \\(\\bar{R}^2 = 1 - (1 - R^2) \\frac{T - 1}{T - k - 1}\\). Maximizing \\(\\bar{R}^2\\) is equivalent to minimizing the regression standard error \\(\\hat{\\sigma}\\). Classical leave-one-out cross-validation (CV) measures the predictive ability of a model. In concept, CV is calculated by fitting the model without observation \\(t\\) and measuring the predictive error on observation \\(t\\). Repeat for all \\(T\\) observations. CV is the mean squared error, and the model with the minimum CV is the best model for forecasting. In practice, you use the hat matrix instead of fitting the model repeatedly. \\[CV = MSE = \\frac{1}{T} \\sum_{t=1}^T \\left[\\frac{e_t}{1 - h_t}\\right]^2\\] where \\(h_t\\) are the diagonal values of the hat-matrix \\(H\\) from \\(\\hat{y} = X\\beta = X(X&#39;X)^{-1}X&#39;y = Hy\\) and \\(e_t\\) is the residual obtained from fitting the model to all \\(T\\) observations. Closely related to CV is Akaike’s Information Criterion (AIC), defined as \\[AIC = T \\log\\left(\\frac{SSE}{T}\\right) + 2(k + 2)\\] The measure penalizes the model by the number of parameters that need to be estimated. The model with the minimum AIC is the best model for forecasting. For large values of \\(T\\), minimizing the AIC is equivalent to minimizing the CV. For small values of \\(T\\), the AIC tends to select too many predictors, and so a bias-corrected version of the AIC has been developed, AICc. \\[AIC_c = AIC + \\frac{2(k+2)(k + 3)}{T - k - 3}\\] BIC is similar to AIC, but penalizes the number of parameters more heavily than the AIC. For large values of \\(T\\), minimizing BIC is similar to leave-v-out cross-validation when \\(v = T[1 − 1/\\log(T) - 1]\\). \\[BIC = T \\log\\left(\\frac{SSE}{T}\\right) + (k + 2)\\log(T)\\] There are many strategies to choose regression model predictors when there are many to choose from. Two common methods for using these measures are best subsets regression and stepwise regression. In best subsets regression, you fit all possible models then choose the one with the best metric value (e.g., lowest AIC). If there are too many candidate models (40 predictors would yield \\(2^{40}\\) models!), use stepwise regression. In backwards stepwise regression, include all candidate predictors initially, then check whether leaving any one predictor out improves the evaluation metric. If any leave-one-out model is better, then choose the best leave-one-out model. Repeat until no leave-one-out model is better. Let’s use the best subsets method to evaluate the possible models for the ausprod_gas data set. The 3 candidate predictors yield \\(2^3 = 8\\) possible models. I don’t know how to do this in caret, so I’ll just do it manually. x &lt;- formula(Beer ~ trend() + season()) fmla &lt;- tribble( ~Gas, ~Electricity, ~Cement, ~fmla, 1, 1, 1, update(x, ~ . + Gas + Electricity + Cement), 1, 1, 0, update(x, ~ . + Gas + Electricity), 1, 0, 1, update(x, ~ . + Gas + Cement), 0, 1, 1, update(x, ~ . + Electricity + Cement), 1, 0, 0, update(x, ~ . + Gas), 0, 1, 0, update(x, ~ . + Electricity), 0, 0, 1, update(x, ~ . + Cement), 0, 0, 0, x ) ausprod_best_subsets &lt;- fmla %&gt;% mutate(mdl = map(fmla, function(x) model(ausprod_gas, TSLM(x))), mdl_glance = map(mdl, glance)) %&gt;% unnest(mdl_glance) %&gt;% select(Gas, Electricity, Cement, AdjR2 = adj_r_squared, CV, AIC, AICc, BIC) ausprod_best_subsets %&gt;% flextable() %&gt;% flextable::theme_zebra() %&gt;% flextable::border(j = 3, border.right = officer::fp_border()) %&gt;% flextable::colformat_double(j = 4, digits = 3) %&gt;% flextable::colformat_double(j = 5:8, digits = 0) %&gt;% flextable::set_caption(&quot;Best subsets model is Beer ~ Gas + Cement&quot;) .cl-6a444eb8{}.cl-6a397b00{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-6a397b14{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-6a3dcfe8{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-6a3dea50{width:0.75in;background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a3dea64{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a3dea65{width:0.75in;background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a3dea66{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a3dea6e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 3.1: Best subsets model is Beer ~ Gas + Cement GasElectricityCementAdjR2CVAICAICcBIC1110.9191703813844021100.9201673803823981010.9191653803823980110.9181693813833991000.9201623783803940100.9191653793813950010.9191643793813950000.920160377379391 All of the models produce about the same \\(\\bar{R}^2\\). The CV, AIC, AICc, and BIC metrics shrink somewhat with less predictors, optimizing on the intercept-only model. It seems that while beer production is correlated with other commodities, it is fully explained by the trend and seasonal variables. 3.5 Predicting Values Use the forecast() method to predict future periods. If there are no predictors in the model other than trend() and season(), forecast() is all you need. If there are predictors, construct “new data” using scenarios() and new_data(). # scenarios() creates data sets extending the key-index in `.data` `n` periods. future_dat &lt;- scenarios( `High Gas` = new_data(ausprod_gas, n = 4) %&gt;% mutate(Gas = max(ausprod_gas$Gas), Electricity = mean(ausprod_gas$Electricity), Cement = mean(ausprod_gas$Cement)), `Low Gas` = new_data(ausprod_gas, n = 4) %&gt;% mutate(Gas = min(ausprod_gas$Gas), Electricity = mean(ausprod_gas$Electricity), Cement = mean(ausprod_gas$Cement)) ) future_dat ## $`High Gas` ## # A tsibble: 4 x 4 [1Q] ## Quarter Gas Electricity Cement ## &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010 Q3 252 50230. 1948. ## 2 2010 Q4 252 50230. 1948. ## 3 2011 Q1 252 50230. 1948. ## 4 2011 Q2 252 50230. 1948. ## ## $`Low Gas` ## # A tsibble: 4 x 4 [1Q] ## Quarter Gas Electricity Cement ## &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010 Q3 116 50230. 1948. ## 2 2010 Q4 116 50230. 1948. ## 3 2011 Q1 116 50230. 1948. ## 4 2011 Q2 116 50230. 1948. ## ## attr(,&quot;names_to&quot;) ## [1] &quot;.scenario&quot; ausprod_fc &lt;- forecast(ausprod_lm, new_data = future_dat) ausprod_fc_2 &lt;- ausprod_fc %&gt;% mutate(mu = map_dbl(Beer, ~unlist(.) %&gt;% .[&quot;mu&quot;]), sigma = map_dbl(Beer, ~unlist(.) %&gt;% .[&quot;sigma&quot;]), ci_025 = qnorm(.025, mu, sigma), ci_100 = qnorm(.100, mu, sigma), ci_900 = qnorm(.900, mu, sigma), ci_975 = qnorm(.975, mu, sigma)) %&gt;% select(.scenario, Quarter, Beer, mu, sigma, ci_025:ci_975) ausprod_gas %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Beer), color = &quot;goldenrod&quot;) + geom_line(data = ausprod_fc_2, aes(y = mu, color = .scenario), size = 1) + geom_ribbon(data = ausprod_fc_2, aes(ymin = ci_100, ymax = ci_900, fill = .scenario), alpha = .2) + geom_ribbon(data = ausprod_fc_2, aes(ymin = ci_025, ymax = ci_975, fill = .scenario), alpha = .2) + scale_color_manual(values = c(&quot;High Gas&quot; = &quot;brown&quot;, &quot;Low Gas&quot; = &quot;darkolivegreen&quot;)) + scale_fill_manual(values = c(&quot;High Gas&quot; = &quot;brown&quot;, &quot;Low Gas&quot; = &quot;darkolivegreen&quot;)) + theme_light() + theme(legend.position = &quot;right&quot;) + labs(title = &quot;Four period linear model, high- and low-gas scenarios.&quot;, caption = &quot;Shaded area is 80%- and 95% confidence interval.&quot;, x = NULL, y = &quot;Beer&quot;, color = NULL, fill = NULL) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## Warning: Removed 8 rows containing missing values (`geom_line()`). ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf 3.6 Nonlinear Regression If there are kinks in the response variable trend, you can use piecewise linear regression by defining the knots in the series. boston_dat &lt;- fpp3::boston_marathon %&gt;% filter(Year &gt;= 1924) %&gt;% filter(Event == &quot;Men&#39;s open division&quot;) %&gt;% mutate(Minutes = as.numeric(Time)/60) boston_lm &lt;- boston_dat %&gt;% model(piecewise = TSLM(Minutes ~trend(knots = c(1950, 1980)))) boston_fc &lt;- boston_lm %&gt;% forecast(h = 10) %&gt;% mutate(mu = map_dbl(Minutes, ~unlist(.) %&gt;% .[&quot;mu&quot;]), sigma = map_dbl(Minutes, ~unlist(.) %&gt;% .[&quot;sigma&quot;]), ci_025 = qnorm(.025, mu, sigma), ci_100 = qnorm(.100, mu, sigma), ci_900 = qnorm(.900, mu, sigma), ci_975 = qnorm(.975, mu, sigma)) %&gt;% select(Year, Minutes, mu, sigma, ci_025:ci_975) augment(boston_lm) %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = Minutes), color = &quot;darkgray&quot;) + geom_line(aes(y = .fitted), color = &quot;goldenrod&quot;) + geom_line(aes(y = mu), data = boston_fc, color = &quot;goldenrod&quot;) + geom_ribbon(aes(ymin = ci_100, ymax = ci_900, fill = .scenario), data = boston_fc, fill = &quot;goldenrod&quot;, alpha = .2) + geom_ribbon(aes(ymin = ci_025, ymax = ci_975, fill = .scenario), data = boston_fc, fill = &quot;goldenrod&quot;, alpha = .2) + theme_light() + theme(legend.position = &quot;right&quot;) + labs(title = &quot;Piecewise linear regression with knots at 1950 and 1980.&quot;, subtitle = &quot;Boston Marathon winning times, Minutes ~trend(knots = c(1950, 1980))&quot;, caption = &quot;Shaded area is 80%- and 95% confidence interval.&quot;, x = NULL, y = &quot;Minutes&quot;, color = NULL, fill = NULL) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## Warning: Removed 10 rows containing missing values (`geom_line()`). ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf ## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning ## -Inf The residual standard error, \\(\\hat{\\sigma}\\), is actually the standard deviation of the residuals. See discussion on StackExchange. Is the name related to the fact that \\(\\hat{\\sigma}\\) is part of the formula for the standard error of the coefficient estimates, \\(SE\\left(\\hat{\\beta}\\right) = \\sqrt{\\hat{\\sigma}^2(X&#39;X)^{-1}}\\)? See my regression notes.↩︎ "],["exponential.html", "Chapter 4 Exponential Smoothing 4.1 Simple Exponential Smoothing (SES) 4.2 Holt’s Linear Method 4.3 Additive Damped Trend Method 4.4 Holt-Winters 4.5 Model Selection with CV 4.6 Model Selection via Maximum Likelihood", " Chapter 4 Exponential Smoothing Exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get more remote. Exponential smoothing is a family of methods that vary by their trend and seasonal components. Table 4.1: Exponential smoothing taxonomy. Seasonal Component Trend Component None (N) Additive (A) Multiplicative (M) None (N) (N, N)Simple Exponential Smoothing (N, A) (N, M) Additive (A) (A, N)Holt’s linear method (A, A)Additive Holt-Winters’ method (A, M)Multiplicative Holt-Winters’ method Additive Damped (Ad) (Ad, N)Additive damped trend method (Ad, A) (Ad, M)Holt-Winters’ damped method There can be no trend (N), an additive (A) linear trend from the forecast horizon, or a damped additive (Ad) trend leveling off from the forecast horizon. The trend could also be multiplicative (M) or multiplicative damped (Md), but Hyndman explains that they do not produced good forecasts. There can be no seasonality (N), or it can be additive (A) change, or multiplicative (M) (proportional) change. Apparently seasonality does not have an additive damped version. The trend and seasonal combinations produce 3 x 3 = 9 possible exponential smoothing methods. ETS (Error, Trend, and Seasonality) models double the number of possible state space models to 18 by treating the error variances as either additive (A) or multiplicative (M). ETS models do not just extend the exponential smoothing models; they also estimate their parameters differently, using maximum likelihood estimation. For models with additive errors, this is equivalent to minimizing the sum of squared errors (SSE). The great advantage of using ETS models is that you can optimize the parameter settings by minimizing the Akaike Information Criterion (AICc). The fable::ETS() function fits ETS models. The combinations are specified through the formula: ETS(y ~ error(c(\"A\", \"M\")) + trend(c(\"N\", \"A\", \"Ad\")) + season(c(\"N\", \"A\", \"M\")) The following sections review just the named models above. 4.1 Simple Exponential Smoothing (SES) Simple exponential smoothing models have no seasonal or trend components. Simple exponential smoothing models are of the form \\(\\hat{y}_{T+h|T} = \\alpha y_T + \\alpha(1-\\alpha)y_{T-1} + \\alpha(1-\\alpha)^2y_{T-2} \\dots\\) where \\(0 &lt; \\alpha &lt; 1\\) is a weighting parameter. On the one extreme, \\(\\alpha\\) = 1 is the same as a naive model. On the other extreme \\(\\alpha \\approx\\) 0 is the average model. Exponential smoothing models are commonly expressed in a component form as a recursive model. \\[ \\begin{align} \\hat{y}_{t+h|t} &amp;= l_t \\\\ l_t &amp;= \\alpha y_t + (1 - \\alpha)l_{t-1} \\end{align} \\] The first component, \\(\\hat{y}_{t+h|t}\\), is the forecast. It equals the last value of the estimated level, kind of like a y-intercept. The second component, \\(l_t\\) is the level (or smoothed value) of the series at time \\(t\\). It describes how the level changes over time, kind of like a slope. There are two parameters to estimate: \\(\\alpha\\) and \\(l_0\\) - the level at base of the recursion. Estimate the parameters by minimizing the SSE with a nonlinear optimization method (black box for me). Example Data set tsibbledata::global_economy contains annual country-level economic indicators, including Exports. This time series has no trend or seasonality, so it is a good candidate for a simple exponential smoothing model. tsibbledata::global_economy %&gt;% filter(Country == &quot;Algeria&quot;) %&gt;% ggplot(aes(x = Year, y = Exports)) + geom_line() + theme_light() + labs(title = &quot;Algerian exports (% of GDP) show no trend or seasonality.&quot;) ETS() is fable’s exponential smoothing function. Using additive errors, this is an ETS(A, N, N) model. mdl_ses &lt;- tsibbledata::global_economy %&gt;% filter(Country == &quot;Algeria&quot;) %&gt;% model(ETS(Exports ~ error(&quot;A&quot;) + trend(&quot;N&quot;) + season(&quot;N&quot;))) mdl_ses %&gt;% report() ## Series: Exports ## Model: ETS(A,N,N) ## Smoothing parameters: ## alpha = 0.8399875 ## ## Initial states: ## l[0] ## 39.539 ## ## sigma^2: 35.6301 ## ## AIC AICc BIC ## 446.7154 447.1599 452.8968 ETS() estimates \\(\\hat{l}_0\\) = 39.539 and \\(\\hat{\\alpha}\\) = 0.840, a high weighting on the prior value. Check the model assumptions with residuals plots. gg_tsresiduals(mdl_ses) Autocorrelation in the residuals increases the prediction intervals. The autocorrelation function plot finds a barely significant negative spike at lag 12 (years). Heteroscedasticity can make prediction intervals inaccurate. The residuals vs time diagnostic plot finds no heteroscedasticity, although there might be an outlier around 1962. Non-normality of the residuals can also make the prediction intervals inaccurate. The histogram shows that the residuals are slightly left-skewed. Use the fitted model to forecast the response variable for five periods. mdl_ses_fc &lt;- mdl_ses %&gt;% forecast(h = 5) %&gt;% mutate(sigma = map_dbl(Exports, ~unlist(.) %&gt;% .[&quot;sigma&quot;]), ci_025 = qnorm(.025, .mean, sigma), ci_975 = qnorm(.975, .mean, sigma)) mdl_ses %&gt;% augment() %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = Exports)) + geom_line(aes(y = .fitted), color = &quot;goldenrod&quot;) + geom_line(data = mdl_ses_fc, aes(y = .mean), color = &quot;goldenrod&quot;) + geom_ribbon(data = mdl_ses_fc, aes(ymin = ci_025, ymax = ci_975), alpha = 0.2, fill = &quot;goldenrod&quot;) + theme_light() + labs(title = &quot;Simple Exponential Smoothing ETS(A, N, N)&quot;, subtitle = &quot;Algerian exports (% of GDP).&quot;) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. 4.2 Holt’s Linear Method Holt’s linear method extends SES with a trend component. \\[ \\begin{align} \\hat{y}_{t+h|t} &amp;= l_t + hb_t \\\\ l_t &amp;= \\alpha y_t + (1 - \\alpha)(l_{t-1} + hb_{t-1}) \\\\ b_t &amp;= \\beta^*(l_t - l_{t-1}) + (1 - \\beta^*)b_{t-1} \\end{align} \\] The level equation, \\(l_t\\) is the same except for a trend adjustment. A third equation, \\(b_t\\), the trend, describes how the slope changes over time. The parameter \\(\\beta^*\\) describes how quickly the slope can change. Now there are four parameter to estimate, \\(\\alpha\\), \\(l_0\\), \\(\\beta^*\\), and \\(b_0\\). Example Data set tsibbledata::global_economy contains annual country-level economic indicators, including Population size. This time series has a trend, so it is a good candidate for Holt’s linear trend method. tsibbledata::global_economy %&gt;% filter(Country == &quot;Australia&quot;) %&gt;% ggplot(aes(x = Year, y = Population)) + geom_line() + theme_light() + labs(title = &quot;Australian population has trend, but no seasonality.&quot;) Fit the model with ETS() again. This time specify an “additive” trend, ETS(A, A, N). mdl_holt &lt;- tsibbledata::global_economy %&gt;% filter(Country == &quot;Australia&quot;) %&gt;% model(ETS(Population ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;N&quot;))) mdl_holt %&gt;% report() ## Series: Population ## Model: ETS(A,A,N) ## Smoothing parameters: ## alpha = 0.9998992 ## beta = 0.3257153 ## ## Initial states: ## l[0] b[0] ## 10067191 228012.5 ## ## sigma^2: 4139605871 ## ## AIC AICc BIC ## 1525.705 1526.859 1536.008 ETS() estimates an \\(\\hat{l}_0\\) of 10,067,191 people at period 0 (1960) with a very high weighting on recent values \\(\\hat{\\alpha}\\) of 0.9999. \\(\\alpha\\) is high when the trend increases rapidly. \\(\\beta_0\\) is 228,013 with a \\(\\hat{\\beta}\\) of 0.326. This is a fairly large \\(\\beta\\), meaning the trend changes often. Check the model assumptions with residuals plots. I’ll skip that step this time and move on to forecasting. mdl_holt_fc &lt;- mdl_holt %&gt;% forecast(h = 10) %&gt;% mutate(sigma = map_dbl(Population, ~unlist(.) %&gt;% .[&quot;mu&quot;]), ci_025 = qnorm(.025, .mean, sigma), ci_975 = qnorm(.975, .mean, sigma)) mdl_holt %&gt;% augment() %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = Population)) + geom_line(aes(y = .fitted), color = &quot;goldenrod&quot;) + geom_line(data = mdl_holt_fc, aes(y = .mean), color = &quot;goldenrod&quot;) + geom_ribbon(data = mdl_holt_fc, aes(ymin = ci_025, ymax = ci_975), alpha = 0.2, fill = &quot;goldenrod&quot;) + theme_light() + labs(title = &quot;Holt&#39;s Linear Method&quot;, subtitle = &quot;Australian Population.&quot;) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. 4.3 Additive Damped Trend Method Holt’s linear trend produces a sloped, but straight line. Research has shown that the assumption of a constant trend in the forecast tends to overshoot. Gardner and McKenzie added a damping parameter \\(\\phi\\) to reduce the forecasted trend to a flat line over time. The forecast equation replaces the \\(h\\) with the series \\(\\phi^1 + \\phi^2 + \\cdots + \\phi^h\\) and the level equation replaces \\(h\\) with \\(\\phi\\). The trend equation adds \\(\\phi\\) as a multiplier to the second term. \\[ \\begin{align} \\hat{y}_{t+h|t} &amp;= l_t + (\\phi^1 + \\phi^2 + \\cdots + \\phi^h)b_t \\\\ l_t &amp;= \\alpha y_t + (1 - \\alpha)(l_{t-1} + \\phi b_{t-1}) \\\\ b_t &amp;= \\beta^*(l_t - l_{t-1}) + (1 - \\beta^*) \\phi b_{t-1} \\end{align} \\] There are now five parameters to estimate, \\(\\alpha\\), \\(\\beta^*\\), \\(l_0\\), \\(b_0\\), and \\(\\phi\\) (although you can supply a \\(\\phi\\) value to the trend() equation. Expect a \\(\\phi\\) between .8 and .998. This is an ETS(A, Ad, N) model. Example Return to the Australian population data. You can fit the original model and the additive damped trend model in a single statement. mdl_holt_d &lt;- tsibbledata::global_economy %&gt;% filter(Country == &quot;Australia&quot;) %&gt;% model( `Holt&#39;s method` = ETS(Population ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;N&quot;)), `Damped Holt&#39;s method` = ETS(Population ~ error(&quot;A&quot;) + trend(&quot;Ad&quot;) + season(&quot;N&quot;)) ) mdl_holt_d %&gt;% select(`Damped Holt&#39;s method`) %&gt;% report() ## Series: Population ## Model: ETS(A,Ad,N) ## Smoothing parameters: ## alpha = 0.9998988 ## beta = 0.4392868 ## phi = 0.98 ## ## Initial states: ## l[0] b[0] ## 10067191 277729.2 ## ## sigma^2: 4584582964 ## ## AIC AICc BIC ## 1532.543 1534.190 1544.906 ETS() estimates \\(\\hat{\\phi}\\) = 0.980 - just a small amount of damping. mdl_holt_d_fc &lt;- mdl_holt_d %&gt;% forecast(h = 10) %&gt;% mutate(sigma = map_dbl(Population, ~unlist(.) %&gt;% .[&quot;mu&quot;]), ci_025 = qnorm(.025, .mean, sigma), ci_975 = qnorm(.975, .mean, sigma)) mdl_holt %&gt;% augment() %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = Population)) + geom_line(aes(y = .fitted), color = &quot;goldenrod&quot;) + geom_line(data = mdl_holt_d_fc, aes(y = .mean, color = .model)) + geom_ribbon(data = mdl_holt_d_fc, aes(ymin = ci_025, ymax = ci_975, fill = .model), alpha = 0.2) + scale_fill_manual(values = c(`Holt&#39;s method` = &quot;goldenrod&quot;, `Damped Holt&#39;s method` = &quot;seagreen&quot;)) + scale_color_manual(values = c(`Holt&#39;s method` = &quot;goldenrod&quot;, `Damped Holt&#39;s method` = &quot;seagreen&quot;)) + theme_light() + labs(fill = &quot;Model&quot;, color = &quot;Model&quot;, title = &quot;Additive Damped Trend Method&quot;, subtitle = &quot;Australian Population&quot;) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. 4.4 Holt-Winters The Holt-Winters method extends Holt’s method with a seasonality component, \\(s_t\\), for \\(m\\) seasons per period. There are two versions of this model, the additive and the multiplicative. The additive method assumes the error variance is constant, and the seasonal component sums to approximately zero over the course of the year. The multiplicative version assumes the error variance scales with the level, and the seasonal component sums to approximately \\(m\\) over the course of the year. 4.4.1 Additive Holt-Winters Method The additive method introduces the seasonality component as an additive element. \\[ \\begin{align} \\hat{y}_{t+h|t} &amp;= l_t + hb_t + s_{t+h-m(k+1)} \\\\ l_t &amp;= \\alpha(y_t - s_{t-m}) + (1 - \\alpha)(l_{t-1} + b_{t-1}) \\\\ b_t &amp;= \\beta^*(l_t - l_{t-1}) + (1 - \\beta^*)b_{t-1} \\\\ s_t &amp;= \\gamma(y_t - l_{t-1} - b_{t-1}) + (1 - \\gamma)s_{t-m} \\end{align} \\] \\(k\\) is the modulus of \\((h - 1) / m\\), so \\(s_{t+h-m(k+1)}\\) is always based on the prior seasonal period. \\(l_t\\) is a weighted average (\\(alpha\\) weighting) between the seasonally adjusted observation and the non-seasonal forecast. The trend component is unchanged. The seasonal component is a weighted average (\\(\\gamma\\) weighting) between the current seasonal index and the same season of the prior season period. Now there are five smoothing parameters to estimate: \\(\\alpha\\), \\(l_0\\), \\(\\beta^*\\), \\(b_0\\), and \\(\\gamma\\), plus an initial value for each season of the seasonal period. 4.4.2 Multiplicative Holt-Winters Method In the multiplicative version, the seasonality averages to one. Use the multiplicative method if the seasonal variation increases with the level. \\[ \\begin{align} \\hat{y}_{t+h|t} &amp;= (l_t + hb_t) s_{t+h-m(k+1)} \\\\ l_t &amp;= \\alpha\\frac{y_t}{s_{t-m}} + (1 - \\alpha)(l_{t-1} + b_{t-1}) \\\\ b_t &amp;= \\beta^*(l_t - l_{t-1}) + (1-\\beta*)b_{t-1} \\\\ s_t &amp;= \\gamma\\frac{y_t}{(l_{t-1} - b_{t-1})} + (1 - \\gamma)s_{t-m} \\\\ \\end{align} \\] Example Data set tsibble::tourism contains quarterly domestic tourist visit-nights in Australia. The plot below is extended with forecasts using the Holt-Winters additive method and the Holt-Winters seasonal method. It’s not obvious whether the error variance increases with the series level, so either the additive or the multiplicative method may be appropriate. The RMSE from the multiplicative model tour &lt;- tsibble::tourism %&gt;% filter(Purpose == &quot;Holiday&quot;) %&gt;% summarize(Trips = sum(Trips) / 1000) tour_fit &lt;- tour %&gt;% model( Additive = ETS(Trips ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;)), Multiplicative = ETS(Trips ~ error(&quot;M&quot;) + trend(&quot;A&quot;) + season(&quot;M&quot;)) ) tour_fit %&gt;% report() ## Warning in report.mdl_df(.): Model reporting is only supported for individual ## models, so a glance will be shown. To see the report for a specific model, use ## `select()` and `filter()` to identify a single model. ## # A tibble: 2 × 9 ## .model sigma2 log_lik AIC AICc BIC MSE AMSE MAE ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Additive 0.193 -105. 229. 231. 250. 0.174 0.184 0.321 ## 2 Multiplicative 0.00212 -104. 227. 229. 248. 0.170 0.183 0.0328 tour_fit_fc &lt;- tour_fit %&gt;% forecast(h = 10) %&gt;% mutate(sigma = map_dbl(Trips, ~unlist(.) %&gt;% .[&quot;mu&quot;]), ci_025 = qnorm(.025, .mean, sigma), ci_975 = qnorm(.975, .mean, sigma)) tour_fit_aug &lt;- tour_fit %&gt;% augment() tour_fit_aug %&gt;% # filter(.model == &quot;Additive&quot;) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Trips)) + geom_line(aes(y = .fitted, color = .model)) + geom_line(data = tour_fit_fc, aes(y = .mean, color = .model)) + geom_ribbon(data = tour_fit_fc, aes(ymin = ci_025, ymax = ci_975, fill = .model), alpha = 0.2) + scale_color_manual(values = c(&quot;Additive&quot; = &quot;goldenrod&quot;, &quot;Multiplicative&quot; = &quot;slategray&quot;)) + scale_fill_manual(values = c(&quot;Additive&quot; = &quot;goldenrod&quot;, &quot;Multiplicative&quot; = &quot;slategray&quot;)) + theme_light() + labs(title = &quot;Australian Domestic Tourism with Holt-Winters Models.&quot;, y = &quot;Trips (millions)&quot;, fill = &quot;Model&quot;, color = &quot;Model&quot;) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. Notice below that the additive seasonal component estimates (s[0] to s[-3]) sum to approximately zero. tour_fit %&gt;% select(Additive) %&gt;% report() ## Series: Trips ## Model: ETS(A,A,A) ## Smoothing parameters: ## alpha = 0.2620382 ## beta = 0.04314266 ## gamma = 0.0001000312 ## ## Initial states: ## l[0] b[0] s[0] s[-1] s[-2] s[-3] ## 9.791341 0.02106875 -0.534408 -0.6697662 -0.2937802 1.497954 ## ## sigma^2: 0.1931 ## ## AIC AICc BIC ## 228.5676 231.1390 250.0058 And the multiplicative seasonal component estimates (s[0] to s[-3]) sum to approximately 4 - the number of seasons in the seasonal period. tour_fit %&gt;% select(Multiplicative) %&gt;% report() ## Series: Trips ## Model: ETS(M,A,M) ## Smoothing parameters: ## alpha = 0.2236926 ## beta = 0.03042124 ## gamma = 0.0001000009 ## ## Initial states: ## l[0] b[0] s[0] s[-1] s[-2] s[-3] ## 10.01351 -0.01141645 0.9430572 0.9270043 0.9692079 1.160731 ## ## sigma^2: 0.0021 ## ## AIC AICc BIC ## 226.7196 229.2910 248.1578 Check the model assumptions with checkresiduals. The residuals plot shows some long-term autocorrelation (a long hump), and the variance increases in the latter years. The histogram shows a normal distribution. The autocorrelation function (ACF) plot shows many spikes outside the insignificance band, and the Ljung-Box test rejects the null hypothesis of no autocorrelation of the residuals (p &lt; 0.0001). # checkresiduals(a10.hw) Here is a four-week Holt-Winters forecast of the hyndsight dataset of daily pageviews on the Hyndsight blog for one year starting April 30, 2014. Create a training dataset consisting of all obserations minus the last four weeks. Then forecast those four weeks with Holt-Winters. Use the additive method because the variance is not scaling with page volume. Creae a second forecast with the seasonal naive method as a benchmark. Notice that the Ljung-Box test rejects the null hypothesis of no autocorrelation of the residuals. The forecast might still provide useful information even with residuals that fail the white noise test. # hyndsight.train &lt;- subset(hyndsight, end = length(hyndsight) - 4*7) # # hyndsight.hw &lt;- hw(hyndsight.train, seasonal = &quot;additive&quot;, h = 4*7) # # hyndsight.sn &lt;- snaive(hyndsight.train, h = 4*7) # # checkresiduals(hyndsight.hw) Compare Holt-Winters to the seasonal naive forecast. The RMSE of Holt-Winters (201.7656) is smaller than the RMSE of seasonal naive (202.7610), so it is the more accurate forecast. # accuracy(hyndsight.hw, hyndsight) # accuracy(hyndsight.sn, hyndsight) Here finally is a plot of the forecasted page views. # autoplot(hyndsight) + # autolayer(hyndsight.hw$mean) 4.5 Model Selection with CV Let’s compare the performance of a few candidate models for the base R datasets::WWWusage data set of internet usage. datasets::WWWusage %&gt;% as_tsibble() %&gt;% ggplot(aes(x = index, y = value)) + geom_line() + theme_light() + labs(title = &quot;Internet usage by minute&quot;, x = NULL, y = &quot;Users&quot;) We will use time-series cross-validation. The data set has 100 rows. Function stretch_tsibble(.init, .step) takes a tsibble and creates a new tsibble for cross validation. First stretch_tsibble() takes the first .init rows from the tsibble and adds a new column .id with value 1. Then it takes the first .init + .step rows from the tsibble and assigns .id value 2. It continues like this, creating longer and longer tsibbles until it cannot create a longer one from the original tsibble. Finally, it appends these together into one long tsibble with .id added to the index. Nofmal cross-validation repeatedly fits a model to data set with one of the rows left out. Since model() fits a separate model per index value, creating this long tsibble effectively accomplishes the same thing. Note the fundamental difference here though: time seris CV does not leave out single values from points along in the time series. It leaves out all points after a particular point along the time series - each sub-data set starts at the beginning and is uninterrupted until reaching the varying end points. Let’s take a look at the CV data set before using it to fit the models. www_cv &lt;- datasets::WWWusage %&gt;% as_tsibble() %&gt;% stretch_tsibble(.init = 10, .step = 1) # 10 rows + 11 rows + ... + 100 rows = 5,005 rows: nrow(www_cv) ## [1] 5005 # .id added to index head(www_cv) ## # A tsibble: 6 x 3 [1] ## # Key: .id [1] ## index value .id ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 88 1 ## 2 2 84 1 ## 3 3 85 1 ## 4 4 85 1 ## 5 5 84 1 ## 6 6 85 1 # 91 index values summary(www_cv$.id) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 42.00 62.00 58.55 78.00 91.00 Fit four models to the 91 data sets to compare the accuracy. Here’s the full code. datasets::WWWusage %&gt;% as_tsibble() %&gt;% stretch_tsibble(.init = 10, .step = 1) %&gt;% model( OLS = TSLM(value ~ index), `Simple Exponential Smoothing` = ETS(value ~ error(&quot;A&quot;) + trend(&quot;N&quot;) + season(&quot;N&quot;)), `Holt&#39;s method` = ETS(value ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;N&quot;)), `Holt&#39;s method (damped)` = ETS(value ~ error(&quot;A&quot;) + trend(&quot;Ad&quot;) + season(&quot;N&quot;)) ) %&gt;% forecast(h = 1) %&gt;% accuracy(data = as_tsibble(datasets::WWWusage)) ## Warning: The future dataset is incomplete, incomplete out-of-sample data will be treated as missing. ## 1 observation is missing at 101 ## # A tibble: 4 × 10 ## .model .type ME RMSE MAE MPE MAPE MASE RMSSE ACF1 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Holt&#39;s method Test 0.0610 3.87 3.17 0.244 2.38 0.701 0.668 0.296 ## 2 Holt&#39;s method (damp… Test 0.288 3.69 3.00 0.347 2.26 0.663 0.636 0.336 ## 3 OLS Test -6.01 38.7 31.4 -10.8 24.8 6.94 6.67 0.974 ## 4 Simple Exponential … Test 1.46 6.05 4.81 0.904 3.55 1.06 1.04 0.803 The best model as measured by RMSE was Holt’s method with damping. OLS was pretty bad. Let’s fit it to the whole data set and forecast future periods. www_fit &lt;- datasets::WWWusage %&gt;% as_tsibble() %&gt;% model(holt_d = ETS(value ~ error(&quot;A&quot;) + trend(&quot;Ad&quot;) + season(&quot;N&quot;))) www_fit %&gt;% report() ## Series: value ## Model: ETS(A,Ad,N) ## Smoothing parameters: ## alpha = 0.9999 ## beta = 0.9966439 ## phi = 0.814958 ## ## Initial states: ## l[0] b[0] ## 90.35177 -0.01728234 ## ## sigma^2: 12.2244 ## ## AIC AICc BIC ## 717.7310 718.6342 733.3620 This time the damping parameter is very small (0.815), resulting in a quick return to the horizontal. www_fc &lt;- www_fit %&gt;% forecast(h = 10) %&gt;% mutate(sigma = map_dbl(value, ~unlist(.) %&gt;% .[&quot;mu&quot;]), ci_025 = qnorm(.025, .mean, sigma), ci_975 = qnorm(.975, .mean, sigma)) www_fit %&gt;% augment() %&gt;% ggplot(aes(x = index)) + geom_line(aes(y = value)) + geom_line(aes(y = .fitted), color = &quot;goldenrod&quot;) + geom_line(data = www_fc, aes(y = .mean), color = &quot;goldenrod&quot;) + geom_ribbon(data = www_fc, aes(ymin = ci_025, ymax = ci_975), alpha = 0.2, fill = &quot;goldenrod&quot;) + theme_light() + labs(title = &quot;Internet usage by Minute.&quot;, subtitle = &quot;Holt&#39;s method with damping. Shaded are is 95% prediction interval.&quot;, x = NULL) ## Warning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. ## The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package. ## If you&#39;re using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values. 4.6 Model Selection via Maximum Likelihood If you specify an ETS model with no parameters, it will use maximum likelihood to select the model with the minimum AICc. Try it with the internet usage data set. ets_mdl &lt;- datasets::WWWusage %&gt;% as_tsibble() %&gt;% model(ETS(value)) ets_mdl %&gt;% report() ## Series: value ## Model: ETS(A,Ad,N) ## Smoothing parameters: ## alpha = 0.9999 ## beta = 0.9966439 ## phi = 0.814958 ## ## Initial states: ## l[0] b[0] ## 90.35177 -0.01728234 ## ## sigma^2: 12.2244 ## ## AIC AICc BIC ## 717.7310 718.6342 733.3620 ETS chose an additive damped trend model, the same as we found using cross validation. "],["arima.html", "Chapter 5 ARIMA 5.1 Tranformations and Differencing 5.2 Autoregressive Models 5.3 Moving Average Models 5.4 Non-Seasonal ARIMA", " Chapter 5 ARIMA Whereas exponential smoothing models describe the trend and seasonality, ARIMA models describe the autocorrelations. An autoregressive (AR(p)) model is a multiple regression with p lagged observations as predictors. \\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t\\] A moving average (MA(q)) model is a multiple regression with q lagged errors as predictors. \\[y_t = c + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_p \\epsilon_{t-q}\\] An autoregressive moving average (ARMA(p, q)) model is a multiple regression with p lagged observations and q lagged errors as predictors. \\[y_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\theta_1 y_{t-1} + \\dots + \\theta_p y_{t-q} + \\epsilon_t\\] An ARMA model with differencing (ARIMA(p,d,q)) model is an ARMA model with d levels of differencing. Whereas exponential smoothing models can handle non-constant variance with multiplicative errors and seasonality, autoregressive models require that you explicitly transform the data into a stationary time series with constant variance. A stationary time series is one in whose statistical properties do not depend on the time at which the series is observed. Stationary time series can have patterns, but no regular long term patterns. 5.1 Tranformations and Differencing The first step in any modeling exercise is plotting the data to identify unusual observations and non-constant variance. Let’s work through the process with the fpp::usmelec data set of us monthly electricity generation. You can see in Figure 5.1 that this data has non-constant variance. elec &lt;- fpp::usmelec %&gt;% as_tsibble() Figure 5.1: This data series has non-constant variance. There are an infinite number of transformations, but the common ones (in increasing strength) are: square root, cube root, log, and inverse. In Figure 5.2, the square root, cube root, and log transformations are not quite strong enough to even out the error variance, but the inverse transformation is a little too large. We need a transformation somewhere in between. elec %&gt;% mutate(sqrt = sqrt(value), cubert = value^(1/3), log = log(value), inverse = 1/value) %&gt;% pivot_longer(-index) %&gt;% mutate( name = factor( name, levels = c(&quot;value&quot;, &quot;sqrt&quot;, &quot;cubert&quot;, &quot;log&quot;, &quot;inverse&quot;), labels = c(&quot;y&quot;, &quot;sqrt(y)&quot;, &quot;cubrt(y)&quot;, &quot;log(y)&quot;, &quot;1 / y&quot;) ) ) %&gt;% ggplot(aes(x = index, y = value)) + geom_line() + facet_wrap(vars(name), scales = &quot;free_y&quot;) + theme_light() + labs( title = &quot;US monthly electricity generation with standard transformations.&quot;, x = NULL) Figure 5.2: Standard transformations under- or over-shoot constant variance. The Box-Cox transformation can find the optimal transformation (Figure 5.3). (lambda &lt;- elec %&gt;% features(value, features = guerrero) %&gt;% pull(lambda_guerrero)) ## [1] -0.4772531 elec2 &lt;- elec %&gt;% mutate(value = box_cox(value, lambda)) elec2 %&gt;% ggplot(aes(x = index, y = value)) + geom_line() + theme_light() + labs(title = glue(&quot;US monthly electricity generation with lambda = {scales::number(lambda, accuracy = .001)} Box-Cox transformation.&quot;), x = NULL) Figure 5.3: Box-Cox transformation produces constant variance. After transforming the response variable, use differencing of successive observations to stabilize the level. In the case of seasonal data, take a seasonal difference (for monthly data, that means take the 12 observation difference), then possibly take a second one-observation difference (Figure ??). elec3 &lt;- elec2 %&gt;% mutate(value = difference(value, 12)) %&gt;% mutate(value = difference(value, 1)) (elec_jung &lt;- elec3 %&gt;% features(value, ljung_box, lag = 10)) ## # A tibble: 1 × 2 ## lb_stat lb_pvalue ## &lt;dbl&gt; &lt;dbl&gt; ## 1 72.9 1.20e-11 The ACF of the twice-differenced values is white noise (Ljung-Box Q = 72.94, p = 0.000). elec3 %&gt;% ggplot(aes(x = index, y = value)) + geom_line() + theme_light() + labs(title = glue(&quot;US monthly electricity generation with Box-Cox and differencing.&quot;), x = NULL) ## Warning: Removed 13 rows containing missing values (`geom_line()`). (#fig:xform_and_diff)Box-Cox transformation plus double-differencing produces stationary time series with constant variance. R function auto.arima() from the forecast package chooses the optimal ARIMA model parameters using the Akaike criterion.5 Use a unit root test to verify stationarity. (elec3_unitroot &lt;- elec3 %&gt;% features(value, unitroot_kpss)) ## # A tibble: 1 × 2 ## kpss_stat kpss_pvalue ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0159 0.1 The p-value is reported as .1 if it is greater than .1. In this case the test statistic of 0.0159 is smaller than the 1% critical value so the p-value is greater than .1. The null hypothesis is not rejected, meaning the data is assumed stationary. 5.2 Autoregressive Models Autoregressive models forecast the response variable as a function of past observations. \\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t\\] where p is the order of regression. The model is denoted AR(p). For an AR(1) model, a \\(\\phi_1\\) = 0 and c = 0 would be the equivalent to white noise. If \\(\\phi_1\\) was 1, then the model would be a random walk. If \\(phi_1\\) was 1 and c was non-zero then the model would be random walk with drift. 5.3 Moving Average Models A moving average (MA(q)) model is a multiple regression with q lagged errors as predictors. \\[y_t = c + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_p \\epsilon_{t-q}\\] where \\(\\epsilon_t\\) is white noise and q is the order of the regression. The model is denoted AR(q). 5.4 Non-Seasonal ARIMA If you combine differencing with the AR and MA model, you get a non-seasonal ARIMA model (autoregressive integrated moving average) (in this context integration is the opposite of differencing). \\[y&#39;_t = c + \\phi_1 y&#39;_{t-1} + \\dots + \\phi_p y&#39;_{t-p} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_p \\epsilon_{t-q} + \\epsilon_t\\] where \\(y&#39;_t\\) is the differenced series (possibly multiple differences) and d is the order of differencing. The model is denoted ARIMA(p, d, q). You can use the Akaike criterion to compare models of the same class, but not different models, so do not use it to compare an ARIMA model to an ETS model. You cannot use the Akaike criterion for models of different levels of differencing.↩︎ "],["dynamic.html", "Chapter 6 Dynamic Harmonic Regression 6.1 TBATS Model", " Chapter 6 Dynamic Harmonic Regression Dynamic harmonic regression is based on the principal that a combination of sine and cosine funtions can approximate any periodic function. \\[y_t = \\beta_0 + \\sum_{k=1}^{K}[\\alpha_k s_k(t) + \\gamma_k c_k(t)] + \\epsilon_t\\] where \\(s_k(t) = sin(\\frac{2\\pi k t}{m})\\) and \\(c_k(t) = cos(\\frac{2\\pi k t}{m})\\), \\(m\\) is the seasonal period, \\(\\alpha_k\\) and \\(\\gamma_k\\) are regression coefficients, and \\(\\epsilon_t\\) is modeled as a non-seasonal ARIMA process. The optimal model has the lowest AICc, so start with K=1 and increase until the AICc is no longer decreasing. K cannot be greater than \\(m/2\\). With weekly data, it is difficult to handle seasonality using ETS or ARIMA models as the seasonal length is too large (approximately 52). Instead, you can use harmonic regression which uses sines and cosines to model the seasonality. The fourier() function makes it easy to generate the required harmonics. The higher the order (K), the more “wiggly” the seasonal pattern is allowed to be. With K=1, it is a simple sine curve. You can select the value of K by minimizing the AICc value. Function fourier() takes in a required time series, required number of Fourier terms to generate, and optional number of rows it needs to forecast. # # Set up harmonic regressors of order 13 # harmonics &lt;- fourier(gasoline, K = 13) # # # Fit a dynamic regression model to fit. Set xreg equal to harmonics and seasonal to FALSE because seasonality is handled by the regressors. # fit &lt;- auto.arima(gasoline, xreg = harmonics, seasonal = FALSE) # # # Forecasts next 3 years # newharmonics &lt;- fourier(gasoline, K = 13, h = 3*52) # fc &lt;- forecast(fit, xreg = newharmonics) # # # Plot forecasts fc # autoplot(fc) Harmonic regressions are also useful when time series have multiple seasonal patterns. For example, taylor contains half-hourly electricity demand in England and Wales over a few months in the year 2000. The seasonal periods are 48 (daily seasonality) and 7 x 48 = 336 (weekly seasonality). There is not enough data to consider annual seasonality. # # Fit a harmonic regression using order 10 for each type of seasonality # fit &lt;- tslm(taylor ~ fourier(taylor, K = c(10, 10))) # # # Forecast 20 working days ahead # fc &lt;- forecast(fit, newdata = data.frame(fourier(taylor, K = c(10, 10), h = 20*48))) # # # Plot the forecasts # autoplot(fc) # # # Check the residuals of fit # checkresiduals(fit) Another time series with multiple seasonal periods is calls, which contains 20 consecutive days of 5-minute call volume data for a large North American bank. There are 169 5-minute periods in a working day, and so the weekly seasonal frequency is 5 x 169 = 845. The weekly seasonality is relatively weak, so here you will just model daily seasonality. The residuals in this case still fail the white noise tests, but their autocorrelations are tiny, even though they are significant. This is because the series is so long. It is often unrealistic to have residuals that pass the tests for such long series. The effect of the remaining correlations on the forecasts will be negligible. # # Plot the calls data # autoplot(calls) # # # Set up the xreg matrix # xreg &lt;- fourier(calls, K = c(10, 0)) # # # Fit a dynamic regression model # fit &lt;- auto.arima(calls, xreg = xreg, seasonal = FALSE, stationary = TRUE) # # # Check the residuals # checkresiduals(fit) # # # Plot forecasts for 10 working days ahead # fc &lt;- forecast(fit, xreg = fourier(calls, c(10, 0), h = 169*8)) # autoplot(fc) 6.1 TBATS Model Thte TBATS model (Trigonometric terms for seasonality, Box-Cox transformations for hetergeneity, ARMA errors for short-term dynamics, Trend (possibly damped), and Seasonal (including multiple and non-integer periods)). # gasoline %&gt;% tbats() %&gt;% forecast() %&gt;% autoplot() TBATS is easy to use, but the prediction intervals are often too wide, and it can be quite slow with large time series. TBATS returns output similar to this: TBATS(1, {0,0}, -, {&lt;51.18,14&gt;}), meaning 1=Box-Cox parameter, {0,0} = ARMA error, - = damping parameter, {&lt;51.18,14&gt;} = seasonal period and Fourier terms. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
